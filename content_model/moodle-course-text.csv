course_id;component;instance_url_id; instance_id; instance_section_num; text; 
2; mod_longpage;120; 1; 0; Kurstext Kurseinheit 1 Rolf Klein Jörg M. Haake Christian Icking Lihong Ma 1 Geräte und Prozesse  
2; mod_longpage;120; 1; 1; 1.1 Einführung Diese Einführung soll Ihnen einen Überblick darüber geben worum es im vorliegenden Kurs geht und an welchem Platz die Kursinhalte in der Informatik als Wissenschaft angesiedelt sind. Seit den 1980er Jahren hat der Computer nicht nur die Berufswelt von Grund auf verändert er hat auch in den Bereichen Bildung und Freizeit Einzug gehalten. Diese Entwicklung wurde durch verschiedene Faktoren ermöglicht und begünstigt: Zum einen ist die Hardware der Computer - dazu gehören unter anderem der Prozessor die Hauptspeicherbausteine und die Festspeicher - immer leistungsfähiger und preiswerter geworden. Erst dadurch wurden Computer für kleine Betriebe und für private Benutzer überhaupt erschwinglich. Zum anderen ist der Umgang mit dem Computer immer einfacher geworden: Wer gelegentlich ein Textverarbeitungssystem oder ein Spielprogramm benutzen möchte kommt mit den komplexen technischen Details der Hardware überhaupt nicht in Berührung und braucht sie deshalb nicht zu kennen. Hier hat sich eine ähnliche Entwicklung vollzogen wie z.B. bei den Kraftfahrzeugen: Um einen Oldtimer zu starten musste man Zündzeitpunkt und Gemischanreicherung sorgfältig einstellen und die technischen Zusammenhänge kennen - heute wird dem Fahrer diese Aufgabe von elektronischen Motormanagementsystemen abgenommen. Beim modernen Computer übernimmt das Betriebssystem eine solche Funktion. Es bildet eine Schicht zwischen der Rechnerhardware und den Anwendungsprogrammen. Das Betriebssystem trennt die Anwendungen von der Hardware ermöglicht es ihnen aber auch in kontrollierter Weise mit der Hardware zu interagieren. Dieses Konzept der Unabhängigkeit zwischen Software und Hardware bildete eine wesentliche Voraussetzung für die Entstehung des Personalcomputers (PC)1 der von Jedermann bedient werden kann und für die Entwicklung kostengünstiger Anwendungsprogramme. Abbildung 1.1: Einfaches Schichtenmodell eines Computersystems: Das Betriebssystem als Vermittler zwischen der Rechnerhardware (unten) und der Anwendungssoftware und den Benutzern (oben). Ein Computersystem besteht aus mehreren Schichten. Ein vereinfachtes Schichtenmodell ist in Abbildung 1.1 dargestellt: Die oberste Schicht ist eine Menge von Anwendungsprogrammen2 wie z.B. E-Mail-Programme und Web-Browser die von einem normalen Benutzer gestartet und ausgeführt werden können. Ein Benutzer sieht einen Computer als eine Menge von Anwendungsprogrammen. Die Schicht Betriebssystem ist eine Menge von besonderen Programmen die für Benutzer nicht direkt sichtbar sind und deren Aufgabe es ist die Hardware wie z.B. CPU Speicher und Ein-/Ausgabegeräte so zu steuern dass die Ausführung von Anwendungsprogrammen möglich wird. Die unterste Schicht ist die Hardware die aus Prozessoren Speichern und Ein-/Ausgabegeräten besteht. Zwischen zwei Schichten ist in Abbildung 1.1 jeweils eine sogenannte Schnittstelle dargestellt dies ist eine Menge von Operationen. Jede Schicht bietet der direkt darüber liegenden Schicht eine Menge von Services (Diensten) an ein Service besteht aus einer Menge von Programmen. Eine Schicht kann einen Service der direkt darunter liegenden Schicht nur über diese Schnittstelle erhalten. D.h. eine Schnittstelle definiert wie eine Schicht auf Dienste der direkt darunter liegenden Schicht zugreifen kann sie ermöglicht die Kommunikation zwischen zwei benachbarten Schichten. Zwei Schnittstellen werden dafür vom Betriebssystem zur Verfügung gestellt: eine Programmierschnittstelle für die Kommunikation der Programme mit dem Betriebssystem und eine Benutzerschnittstelle3 für die Kommunikation der Benutzer mit dem Betriebssystem in Form von Systemprogrammen wie z.B. Shell Compiler und Editor. Beispielsweise legt ein Anwendungsprogrammierer fest welche Operationen aus der Programmierschnittstelle ein Anwendungsprogramm nutzen soll. Zum Editieren von Programmtexten kann ein Text-Editor verwendet werden und dieser kann mit Maus und Tastatur als Benutzerschnittstelle gesteuert werden. Die Hardware bietet dem Betriebssystem eine Menge von Maschinenbefehlen an die Instruction set architecture (ISA) das Betriebssystem verwendet wiederum diese Instruktionen um mit der Hardware zu kommunizieren. Ein dritter Grund für die rasante Verbreitung von Computern liegt darin dass man sie zu Netzwerken zusammenschalten kann die neue Möglichkeiten eröffnen. So kann etwa ein Netz aus einfachen PCs den Mitarbeitern einer Abteilung den Zugriff auf gemeinsame Datenbestände erlauben und dabei viel preisgünstiger sein als ein Abteilungsrechner mit gleicher Gesamtleistung und einer entsprechenden Anzahl von Terminals. Ein weiteres Beispiel ist das World Wide Web das den für den Einzelnen zugänglichen Informationsraum in einer Weise erweitert die bis zum Ende der 1980er Jahre noch nicht vorstellbar war. Dazu kamen ab ca. 2000 die mobilen Geräte die mit drahtloser Kommunikation ganz neue Felder der Flexibilität und Interoperabilität erschließen. In diesem Kurs beschäftigen wir uns in den ersten beiden Kurseinheiten mit Betriebssystemen die Kurseinheiten drei und vier befassen sich mit Rechnernetzen. Natürlich kann keines der beiden Themen in diesem kleinen Rahmen umfassend behandelt werden unser Ziel ist es Ihnen einige wesentliche Prinzipien und Grundtatsachen nahezubringen die in der Praxis immer wieder auftreten und deswegen von allgemeinem Interesse sind. Wer diesen Stoff im späteren Studium vertiefen möchte sei z.B. auf die Kurse Betriebssysteme Verteilte Systeme Kommunikations- und Rechnernetze und Sicherheit im Internet hingewiesen. An welchem Platz steht nun der Inhalt dieses Kurses im Gesamtkontext der Informatik? Stark vereinfacht kann man sagen die Informatik beschäftigt sich damit wie man von einem Problem der realen Welt zu einer Computerlösung kommt. Am Anfang wird im Dialog mit dem Anwender durch Abstraktion eine zunächst noch unscharfe Beschreibung des realen Problems gewonnen sie muss dann weiter präzisiert werden bis eine formale Problemspezifikation vorliegt auf der dann das Pflichtenheft aufbauen kann. Dieser schwierige Prozess ist in der Informatik Gegenstand des Software Engineering. Anschließend wird ein Algorithmus zur Lösung des Problems entwickelt und auf Korrektheit und Effizienz geprüft. Ein wenig mathematisches Rüstzeug ist dabei unentbehrlich. Es gibt wichtige Problembereiche für die im Laufe der Zeit eigene algorithmische Techniken entstanden sind Beispiele sind die Bereiche Datenbanken und Informationssysteme Künstliche Intelligenz Computergraphik und kombinatorische Algorithmen. Zu allen diesen Bereichen können Sie Wahlkurse der Informatik in Ihrem Studiengang belegen. Aus dem Algorithmus entsteht dann ein Programm zum Beispiel in Pascal wie es im Kurs Einführung in die imperative Programmierung gelehrt wird oder in Java wie es der Kurs Einführung in die Objektorientierte Programmierung vermittelt. Wie aus einem für Menschen lesbaren Programmtext ein Programm in Maschinensprache (Instruktionen der ISA) wird zeigt der Kurs ÜbersetzerbauÜbersetzerbau. In den Kursen der Technischen Informatik wird erklärt wie die Maschine also die Hardware-Plattform aufgebaut ist. Zu guter Letzt kommt dieser Kurs zum Tragen: Wir untersuchen hier was geschieht während Maschinenprogramme von Computern ausgeführt werden. Wer hat mit Betriebssystemen und Rechnernetzen zu tun? Zwei Gruppen hatten wir bereits identifiziert: Wer als reiner Benutzer auf seinem Computer ausschließlich fertige Anwendungsprogramme laufen lässt arbeitet gelegentlich mit der Benutzerschnittstelle des Betriebssystems etwa um ein Programm zu starten ein neues Verzeichnis anzulegen oder um nach einer Datei zu suchen. Wer als Anwendungsprogrammierer selbst Software entwickelt muss die Programmierschnittstelle des Betriebssystems kennen. Bei allen netzbasierten Anwendungen sind außerdem solide Kenntnisse über Rechnernetze vonnöten zum Beispiel über Protokolle und den Client-Server-Betrieb. Es lassen sich zumindest zwei weitere Berufsgruppen ausmachen für die der Inhalt dieses Kurses ebenfalls interessant ist: Wer als Systemadministrator einzelne Computer einrichtet und an ein vorhandenes Netzwerk anschließt oder selbst ein kleines oder größeres Netzwerk einrichtet betreibt und wartet muss neben den Benutzer- und Programmierschnittstellen der Betriebssysteme der beteiligten Computer auch deren spezielle Kommandos für den Systemadministrator (Super-User) kennen mit denen die Konfiguration des Rechners und des Netzwerks festgelegt wird. Benötigt werden auch Kenntnisse der internen Abläufe um Performanzprobleme beheben zu können. Schließlich sind für die Wartung auch grundlegende Hardwarekenntnisse erforderlich. Wer schließlich selbst an der Weiterentwicklung von Betriebssystemen oder anderer Systemsoftware4 mitarbeitet muss detaillierte Kenntnisse über die interne Struktur des Betriebssystems und der Rechnerhardware besitzen. Natürlich lassen sich diese Rollen nicht scharf gegeneinander abgrenzen wer etwa privat intensiv mit seinem PC arbeitet ist oft Benutzer Programmierer und Administrator in einer Person. Bevor wir nun mit den Betriebssystemen beginnen noch ein paar Vorbemerkungen. Ein Kurs über Betriebssysteme will in allgemeiner Form beschreiben welche Aufgaben moderne Betriebssysteme erfüllen müssen und Prinzipien zur Lösung dieser Aufgaben vorstellen. Er will nicht die Dokumentation eines konkreten Betriebssystems ersetzen5 . Trotzdem wird in diesem Kurs gelegentlich ein reales Betriebssystem als Beispiel auftreten und die Beispiele sollten für Sie zu Hause am Rechner nachvollziehbar sein deshalb haben wir uns für Linux entschieden. Informationen zu Linux und Installationshinweise stehen zum Beispiel in [1][2][3][4][5][6]. Die Fachsprache der Informatik ist Englisch. Hilfetexte und Handbücher sind oft nur auf Englisch verfügbar. Wer Informatik studiert sollte deshalb nach besten Kräften Englisch lernen.6 Wir ergänzen ihn hier durch einschlägige Fachvokabeln aus den Bereichen Betriebssysteme und Rechnernetze die meist in Klammern hinter den deutschen Begriffen aufgeführt werden. In den nächsten Kapiteln finden Sie an verschiedenen Stellen Links und QR-Codes die sie zu computerunterstützten Übungsaufgaben Übungsaufgaben führen. Sie sind mit nebenstehendem Zeichen gekennzeichnet das auch den Schwierigkeitsgrad angibt (\(1\) = leicht \(5\) = schwierig). Diese Übungsaufgaben geben Ihnen die Möglichkeit den im Kurstext behandelten Stoff zu vertiefen und ihren Lernerfolg zu kontrollieren. Dazu geben Ihnen die computerunterstützten Übungsaufgaben Feedback zu Ihrer Lösung und Hinweise auf relevante Stellen im Kurstext. Wir empfehlen Ihnen nachdrücklich die Übungsaufgaben aktiv zu bearbeiten. Sie sollten dazu auf der Grundlage des Kurstextes ohne weitere Hilfsmittel benutzen zu müssen in der Lage sein. In allen Kapiteln finden Sie eine Reihe von Übungsaufgaben Übungsaufgaben und jeweils am Kapitelende ihre Lösungen. Sie dienen zur Selbstkontrolle beim Lesen zur Einübung des Stoffs und zu einem geringen Teil auch zur Ergänzung. Alle Leser werden nachdrücklich ermutigt sich mit diesen Übungsaufgaben zu beschäftigen. Sie sind mit nebenstehendem Zeichen gekennzeichnet das auch den Schwierigkeitsgrad angibt (\(1 =\) leicht \(5=\) schwierig).  
2; mod_longpage;120; 1; 2; 1.2 Die Hardwarekomponenten eines Computers In der Einführung hatten wir festgestellt dass das Betriebssystem eine Zwischenschicht ist die die Benutzer und ihre Programme von der nackten Hardware des Rechners trennt. Um die Funktionen des Betriebssystems richtig verstehen zu können sollten wir uns deshalb eine ungefähre Vorstellung davon machen wie die Hardware aufgebaut ist. Dazu dient dieser Abschnitt. Wir beschränken uns dabei auf die klassische Architektur des von-Neumann-Rechners 7 wie sie auch im Kurs Einführung in die imperative Programmierung beschrieben wird. Um diese Architektur richtig würdigen zu können muss man wissen dass in der Anfangszeit die Rechner durch physische Veränderung der Hardware programmiert wurden der legendäre ENIAC8 etwa durch das Umstöpseln von Steckverbindungen. Im Jahr 1945 hatte dann von Neumann eine bahnbrechende Idee:9 Das auszuführende Programm ist nicht mehr ein fester Bestandteil des Rechners es wird vielmehr - genau wie die benötigten Daten - vor dem Programmlauf in den Speicher des Rechners geladen und hinterher wieder entfernt. Fest in den Rechner einbauen muss man also nur noch die Fähigkeit ein beliebiges im Speicher befindliches Programm ausführen zu können - und hieran braucht man dann nie wieder etwas zu verändern! Diese Vorstellung vom Computer als einer universellen Maschine zur Ausführung von Programmen wird Ihnen in der Theoretischen Informatik wiederbegegnen. Die wesentlichen Bestandteile der Hardware eines modernen von-Neumann-Rechners sind daher der Prozessor (CPU = central processing unit) der Hauptspeicher (main memory) und die Ein-/AusgabegeräteEin-/Ausgabegerät (I/O devices) dazu zählen typischerweise Bildschirm (monitor) Maus (mouse) und Tastatur (keyboard) zusammen auch Terminal genannt Kommunikationsgeräte zum Anschluss an Rechnernetze zum Beispiel Ethernet-Controller oder WLAN-Controller Festspeicher wie SSD (solid state drive) und Magnetplattenlaufwerk (HD hard disk) Wechselspeicher wie SD-Karte (secure digital memory card) USB-Stick optische Medien wie CD und DVD oder früher Diskettenlaufwerk (floppy disk drive) und Bandlaufwerk (tape drive) Drucker (printer) Mikrofon Kamera und diverse Spielecontroller (game controller). In den nächsten beiden Abschnitten gehen wir auf einige dieser Hardwarebestandteile etwas näher ein. Prozessor und Hauptspeicher Alle eigentlichen Berechnungen eines Computers finden in seinem Prozessor statt genauer gesagt im Rechenwerk das manchmal auch als ALU (arithmetic-logic unit) bezeichnet wird. Der Prozessor greift auf die Programme und die Daten zu die sich zur Laufzeit im Hauptspeicher befinden. Dieser ist als eine lange Folge von gleich großen Speicherzellen organisiert die einzeln adressiert werden können. Zugreifen heißt hier dass das Datum aus einer Speicherzelle gelesen oder in sie geschrieben wird. Wegen dieses wahlfreien Zugriffs hat sich für den Hauptspeicher auch die Bezeichnung RAM (random access memory) eingebürgert.10 In Pascal-Notation ist also der Hauptspeicher ein sehr langes array [0 .. \(n-1\)] of word wobei ein Wort gerade der Inhalt einer Speicherzelle ist. Es besteht aus einem oder mehreren Bytes. Ein Byte enthält acht Bit und ein Bit hat den Wert null oder eins. Die Speicherkapazität des Hauptspeichers beträgt daher \(n\) mal die Wortlänge. Sie wird in Kilobyte (KByte) Megabyte (MByte) oder Gigabyte (GByte) angegeben wobei mit Zweierpotenzen gerechnet wird:11 \(\rm 1 GB = 2^{10} MB = 2^{20} KB = 2^{30} Byte. \) Bei der von-Neumann-Architektur enthält der Hauptspeicher Daten und Programme. Entsprechend kann ein Wort die Binärdarstellung einer Zahl sein der Code für einen Prozessorbefehl oder die Adresse einer anderen Speicherzelle. Einem Wort selbst kann man nicht ansehen was es darstellt. Trotzdem sind Verwechselungen ausgeschlossen denn der Prozessor weiß bei jedem Zugriff auf eine Speicherzelle ob darin eine Zahl oder ein Befehl steht. Um ein Wort aus einer Speicherzelle zu holen hat die CPU einige wichtige Register die als Zwischenspeicher verwendet werden: Der Befehlszähler (Program Counter PC) enthält die Adresse derjenigen Speicherzelle in der der als nächstes auszuführende Befehl steht. Das Befehlsregister (Instruction Register IR) speichert den aktuellen Befehl der gerade verarbeitet bzw.ausgeführt wird. Das Speicheradressregister (Memory Address Register MAR) enthält die Adresse derjenigen Speicherzelle die als nächstes gelesen oder beschrieben werden soll. Das Programmstatuswortregister enthält verschiedene Bits die zeigen ob das gerade ausgeführte Programm privilegierte Befehle benutzen darf oder ob die CPU eine Unterbrechung bearbeiten will. Der Akkumulator ist eines von vielen Datenregistern und speichert ein Zwischenergebnis einer Berechnung. Die CPU arbeitet Befehle in Zyklen ab dem sogenannten Instruktionszyklus. Eine Befehlsausführung kann in zwei Phasen aufgeteilt werden: In der Holphase (fetch stage) wird das Speicheradressregister mit dem Wert des Befehlszählers belegt und die Adresse an den Adressbus weitergegeben. Der Inhalt der Speicherzelle mit dieser Adresse wird in das Befehlsregister geladen. Zum Abschluss wird in dieser Phase der Befehlszähler um Eins erhöht. In der Ausführungsphase (execution stage) wird der in das Befehlsregister geladene Befehl ausgeführt. In dieser Phase können auch weitere Daten oder Adressen von Speicherzellen geholt werden. Übungsaufgabe 1.1 Identifizieren Sie den korrekten Ablauf des Instruktionszyklus ... Aufgabe jetzt bearbeiten Schauen wir dem Prozessor einmal bei der Arbeit zu! Zur Illustration dient Abbildung 1.2. Sie zeigt einen Hauptspeicher der Kapazität 64 KByte der aus \(2^{16}\) Speicherzellen der Wortlänge 1 Byte besteht und einige typische Bestandteile einer CPU wie sie zum Beispiel beim Intel 8080 anzutreffen waren. Das Befehlszählregister in der CPU enthält die Adresse - also die Binärdarstellung12 des Index - von derjenigen Speicherzelle in der der Code des nächsten auszuführenden Befehls steht. In Abbildung 1.2 ist das die Zelle mit dem Index 4 die Adresse als 16-Bit-Zahl lautet also 0000000000000100. Abbildung 1.2: Der Prozessor greift über Daten- und Adressbus auf den Hauptspeicher zu. Um auf diese Speicherzelle zugreifen zu können wird die Adresse in das Speicheradressregister übertragen und gelangt auf den Adressbus. Hierdurch wird die Zelle des Hauptspeichers mit Index 4 angesprochen und ihr Inhalt wird über den Datenbus in das Speicherinhaltsregister des Prozessors übertragen. Weil durch die vorderen Bits des Wortes 01100111 der Prozessor erkennt dass das Wort einen Befehl darstellt wird es in das Befehlsregister kopiert dort wird der Befehl decodiert und interpretiert. Die Holphase ist fertig. Angenommen der auszuführende Befehl lautet A := A + (HL). Das bedeutet: Der Inhalt des Akkumulators eines speziellen 8-Bit-Registers der CPU für die Speicherung eines Zwischenergebnisses soll um die Zahl erhöht werden die in der Speicherzelle steht deren Adresse sich ergibt wenn man die Worte in den Registern H (high) und L (low) hintereinanderschreibt man spricht hierbei von indirekter Adressierung. Im Beispiel ergibt sich hierdurch die Adresse 1111111111111110 sie wird in das Speicheradressregister übertragen und ein weiterer Speicherzugriff wird ausgelöst der das Wort 00000011 in das Speicherinhaltsregister bringt. Aus dem Zusammenhang ist klar dass es sich hierbei um die Binärdarstellung einer Zahl handeln muss denn das Wort tritt ja als Operand einer Addition auf. Diese Addition kann nun ausgeführt werden im Akkumulator steht danach die Binärdarstellung der Zahl \(13 + 3 = 16\). Nun könnte zum Beispiel anschließend das Ergebnis der Addition in eine Hauptspeicherzelle zurückgeschrieben werden. Die Ausführungsphase ist zu Ende und ein Befehlsausführungszyklus ist fertig. Der Inhalt des Befehlszählers wird am Ende der Holphase immer um Eins erhöht. Durch dieses Hochzählen um Eins erklärt sich auch der Name man findet auch die Bezeichnung Programmzähler (program counter). Ein Sprungbefehl hingegen setzt den Befehlszähler auf einen bestimmten Wert dies entspricht der Aktion verzweige im Programmablauf zu einer bestimmten Adresse . Übungsaufgabe 1.2 Die Binärdarstellung einer natürlichen Zahl \(n\geq 1\) ist die Folge \(a_ka_k-1 ... a_1a_0\) von Nullen und Einsen \(a_i\) für welche \( n = a_k 2^k + a_{k-1} 2^{k-1} + ... + a_1 2^{1} + a_0 2^{0} \) gilt und \(a_k = 1\) führende Nullen lässt man also fort. Welche Zahl \(n\) wird durch 10011101 dargestellt? Und bestimmen Sie die Binärdarstellung von 160. Aufgabe jetzt bearbeiten Übungsaufgabe 1.3 Beim Prozessor 68000 von Motorola beträgt die Wortlänge 2 Byte. Wie breit muss der Adressbus sein - das heißt: aus wievielen Bits bestehen die Adressen - bei einer Hauptspeicherkapazität von 16 MByte? Aufgabe jetzt bearbeiten Die CPU arbeitet getaktet. Ein Maschinenbefehl erfordert bei der Ausführung ein oder mehrere Takte. Einfache Maschinenbefehle wie etwa das Kopieren einer Adresse vom Befehlszählerregister ins Speicheradressregister können in einem Takt ausgeführt werden. Als Taktfrequenz bezeichnet man die Anzahl der Takte pro Sekunde eine Frequenz von z.B. 1 GHz (Gigahertz) bedeutet einen Takt von einer Nanosekunde bzw.eine Milliarde Takte pro Sekunde. Auch das Heraufzählen des Befehlszählregisters Befehlszählregister lässt sich in einem einzigen Arbeitstakt erledigen. Dagegen nimmt ein Hauptspeicherzugriff viele Arbeitstakte in Anspruch.13 Denn einerseits sind mehrere Maschinenbefehle auszuführen wie wir am Beispiel des Befehls A := A + (HL) oben gesehen haben. Andererseits vergeht eine gewisse Zeit zwischen dem Aufbringen einer Adresse auf den Adressbus und der Ankunft des zugehörigen Speicherzelleninhalts über den Datenbus. Diese Zeitspanne hängt von der physikalischen Beschaffenheit des Hauptspeichers und der Geschwindigkeit des Datenbusses ab sie wird als Zugriffszeit auf den Hauptspeicher bezeichnet. Cache Schnelle Hauptspeicherchips kosten pro MByte Speicherkapazität viel mehr als langsame Chips. Aus Kostengründen wird deshalb oft neben dem normalen Hauptspeicher ein kleinerer aber schnellerer Zwischenspeicher verwendet der meist als Cache bezeichnet wird.14 Häufig benutzte Daten werden vorübergehend vom Hauptspeicher in den Cache kopiert. Wenn Daten benötigt werden schaut man zunächst im Cache nach. Findet man dort eine Kopie der gesuchten Daten so verwendet man sie und braucht keinen Hauptspeicherzugriff auszuführen. Wird man aber im Cache nicht fündig ist ein Zugriff auf den Hauptspeicher unvermeidlich in diesem Fall legt man eine Kopie der Daten im Cache ab weil man davon ausgeht dass diese Daten bald noch einmal benötigt werden. Dieser einfache Cache-Algorithmus spielt eine wichtige Rolle er wird uns in Abschnitt 1.12 wiederbegegnen. Für das Betriebssystem stellen sich hier zwei wichtige Aufgaben: Cache-Management Wenn Daten im Cache abgelegt werden sollen und dort kein freier Platz mehr vorhanden ist müssen alte Daten überschrieben werden. Welche Daten soll man dafür opfern?15 Cache-Konsistenz Angenommen der Wert einer Variablen soll verändert werden. Wenn diese Änderung nur an der Kopie im Cache vollzogen wird so besteht anschließend ein Unterschied zwischen dem Original im Hauptspeicher und der Kopie im Cache. Das Betriebssystem muss dafür sorgen dass sich hieraus keine Fehler ergeben. Dieses Problem ist besonders gravierend bei Multiprozessorsystemen bei denen jeder Prozessor über einen eigenen Cache verfügt. Welchen Effizienzgewinn die Verwendung eines schnellen Zwischenspeichers bringt hängt zum einen von der Schnelligkeit der Hardware und vom Cache-Management ab im Einzellfall aber auch davon wie ein konkretes Programm auf seine Daten zugreift. Übungsaufgabe 1.4 Angenommen ein Schreib-/Lesezugriff auf den Cache ist neunmal so schnell wie ein Zugriff auf den Hauptspeicher. Welchen Zeitvorteil ergibt die Verwendung eines Cache nach dem oben beschriebenen Algorithmus wenn bei 80 Prozent aller Zugriffe die gesuchten Daten im Cache gefunden werden? Aufgabe jetzt bearbeiten Mit Hauptspeicher (und Cache) allein kann ein Rechner nicht auskommen denn selbst wenn der Hauptspeicher pro MByte preiswerter als der Cache ist kann man seine Kapazität aus Kostengründen nicht so groß auslegen dass alle benötigten Programme und Daten darin Platz finden beim Abschalten des Rechners oder bei einem Stromausfall geht der Inhalt des Hauptspeichers verloren. Außerdem braucht man Geräte mit deren Hilfe der Rechner Programme und größere Datenmengen mit seiner Umwelt austauschen kann. Aus diesem Grund besitzen die meisten Computer außer dem Hauptspeicher der auch als Primärspeicher bezeichnet wird noch Sekundärspeicher und Tertiärspeicher. Mehr hierzu finden Sie in den nächsten Abschnitten. Sekundärspeicher Der Hauptspeicher wird auch als flüchtiger Speicher bezeichnet d.h. die Daten im Speicher gehen beim Ausschalten oder bei einem Neustart verloren. Der Rechner kann aber erst anfangen Programme auszuführen wenn sie auch im Hauptspeicher vorliegen. Dafür benötigt man einen Speicher (Festspeicher) der die Programme und Daten dauerhaft speichern kann die dann in den Hauptspeicher geladen werden. Die Aufgabe für die dauerhafte Speicherung übernimmt ein Sekundärspeicher (secondary memory). Die gebräuchlichsten Typen von Sekundärspeicher sind zur Zeit die Magnetplatte und der Flashspeicher. Festplatte Eine Magnetplatte ist auf beiden Seiten mit einer magnetisierbaren Oberfläche beschichtet und dreht sich mit rund einhundert Umdrehungen pro Sekunde. Früher gab es Wechselplatten heute sind die Magnetplatten meist fest in ihr Laufwerk eingebaut. Bei manchen Plattenlaufwerken sind mehrere Platten übereinander angebracht die von einer gemeinsamen Spindel gedreht werden siehe Abbildung 1.3. Auf jeder Seite einer Platte gleitet auf einem dünnen Luftkissen ein Schreib-/Lesekopf. Er wird von einem Arm geführt der an einer Welle befestigt ist. Durch Verdrehen der Welle werden alle Köpfe simultan positioniert. Abbildung 1.3: Ein Plattenstapel mit Schreib-/Leseköpfen. Diese Anordnung mag an altmodische Plattenspieler erinnern es bestehen aber zwei wesentliche Unterschiede zur Vinyl-Schallplatte: Bei der Festplatte wird die Information magnetisch gespeichert und die Daten sind nicht spiralförmig auf der Festplatte angeordnet vielmehr ist jede Oberfläche einer Platte in einige tausend kreisförmige Spuren gleicher Breite unterteilt die wiederum aus einigen hundert Sektoren bestehen. Die jeweils übereinanderliegenden Spuren bilden einen Zylinder. Die weiter außen liegenden Spuren können mehr Sektoren enthalten als die inneren. Bei starken Erschütterungen kann es vorkommen dass ein Schreib-/Lesekopf die Plattenoberfläche im Betrieb berührt und sie beschädigt. Bei einem solchen head crash können Daten verlorengehen. Um dieses Risiko zu verringern sind Magnetplatten meist in staubdicht verschlossenen Gehäusen untergebracht und der Arm wird in einer sicheren Ruhestellung geparkt wenn keine Aufträge vorliegen oder wenn eine zu starke Beschleunigung (z.B. fallend) registriert wird. Die Magnetplatte ermöglicht wahlfreien Zugriff auf einzelne Sektoren d.h. jeder Sektor wird mit einer Adresse angesprochen. Um den Inhalt eines bestimmten Sektors zu lesen muss zunächst der Kopf durch Verdrehen der Welle auf den richtigen Zylinder gebracht werden. Die Zeit für die Positionierung des Kopfes nennt man Positionierungszeit. Dann wird abgewartet bis der gesuchte Sektor am Kopf vorbeiläuft (Latenzzeit). Schließlich kann der Sektorinhalt gelesen werden. Die gelesenen Daten kommen zunächst in einen Pufferspeicher (buffer). Die Zeit die hierfür benötigt wird nennt man Übertragungszeit. Schreibzugriffe funktionieren entsprechend. Die Zugriffszeit auf einen Sektor ist definiert als die Summe von Positionierungs- Latenz- und Übertragungszeit. Am längsten dauert gegenwärtig noch die Positionierung des Schreib-/Lesekopfes auf den richtigen Zylinder. Die Summe aus Positionierungs- und Latenzzeit nennt man Suchzeit (seek time). Die Zugriffszeit auf eine Magnetplatte ist deshalb sehr viel länger - etwa tausend mal - als die Zeit die beim Zugriff auf eine Zelle im Hauptspeicher vergeht. Im Unterschied zum Zugriff auf eine Zelle im Hauptspeicher ist die Zugriffszeit auf einen Sektor keine Konstante die nur von der physikalischen Beschaffenheit der Magnetplatte abhängt da die Suchzeit von der aktuellen Position des Lesekopfes abhängt. Die Suchzeit für die Kopfpositionierung entspricht in etwa der Distanz die der Kopf zurücklegt. Diese Distanz wird durch die Anzahl der Spuren gemessen über die der Kopf bewegt werden muss. Folgende Maßnahmen helfen die Suchzeit zu verringern: zusammengehörende Information sollte möglichst in benachbarten Sektoren und Zylindern gespeichert werden wenn bei einem Plattenlaufwerk mehrere Schreib-/Leseaufträge gleichzeitig vorliegen sollte eine günstige Bearbeitungsreihenfolge gewählt werden (disk scheduling). Auch diese Aufgaben werden vom Betriebssystem erledigt näheres hierzu folgt in Abschnitt 1.3. Übungsaufgabe 1.5 Wir betrachten ein Plattenlaufwerk mit nur einer einzelnen Platte die Spuren sind mit 0 beginnend von außen nach innen durchnummeriert. Wenn dafür nun mehrere Aufträge vorliegen kann man sie nach folgenden Strategien abarbeiten: FCFS (first-come first-served) bearbeitet die Aufträge in der Reihenfolge ihres Eingangs. SSTF (shortest-seek-time-first) bearbeitet jeweils denjenigen Auftrag als nächsten dessen Spur der momentanen Position des Schreib-/Lesekopfs am nächsten liegt. SCAN bewegt den Kopf abwechselnd von außen nach innen und von innen nach außen über die gesamte Platte und führt dabei die Aufträge aus deren Spuren gerade überquert werden. Angenommen es sind Zugriffsaufträge für Sektoren in den Spuren 32 185 80 126 19 107 in dieser Reihenfolge eingegangen. Der Kopf steht anfangs auf Spur 98 und bewegt sich im Falle der SCAN-Strategie gerade nach außen. Welche Distanz muss der Kopf bei Anwendung der drei Strategien jeweils insgesamt zurücklegen bis alle Aufträge erledigt sind? Aufgabe jetzt bearbeiten Zwar dauert ein Zugriff auf die Magnetplatte sehr viel länger als ein Hauptspeicherzugriff aber dafür liefert er ein sehr viel größeres Datenvolumen: Ein Sektor enthält zwischen 0 5 und 4 KByte Nutzinformation zusätzlich sind Sektor- und Zylindernummer und Hilfsdaten für die Fehlerkorrektur gespeichert . Zu diesem Zweck kann man sogenannte parity bits einführen deren Wert die Summe16 der Werte bestimmter Datenbits sind. Beim Schreiben bzw. Lesen der Nutzinformation eines Sektors werden diese Summen berechnet und in die PrüfbitsPrüfbit eingetragen bzw. mit den dort gespeicherten Werten verglichen. Ergeben sich Abweichungen so ist ein Fehler aufgetreten. Bei Verwendung geeigneter Codes kann man feststellen welches Bit einen falschen Wert hat und den Fehler automatisch korrigieren. Die Nutzinformation eines Sektors wird als ein Block bezeichnet. Um zeitaufwendige Plattenzugriffe nach Möglichkeit einzusparen kann man einen Cache im Hauptspeicher einrichten in dem einzelne Blöcke oder die Inhalte ganzer Spuren gespeichert werden in der Annahme dass man sie in Kürze noch einmal benötigt. Das Prinzip ist dasselbe wie in Abschnitt 1.11. Manche Computer verfügen über eine sogenannte RAM-disk. Hierunter versteht man einen fest reservierten Teil des Hauptspeichers der von Anwenderprogrammen und Anwendern wie eine Festplatte benutzt werden kann. Zum Beispiel werden Befehle zur Dateiverarbeitung wie wir sie in Abschnitt 1.6 kennenlernen werden in entsprechende Speicherzugriffe übersetzt. Das geht beinahe so schnell wie bei reinen Hauptspeicherzugriffen. Ein Nachteil: Auch bei der RAM-disk sind die Daten nach Abschalten des Stroms verschwunden. Flashspeicher Der Flashspeicher ist ein rein elektronischer Speicher der Art EEPROM (electrically erasable programmable read only memory). Ein EEPROM erlaubt es den Inhalt von Speicherzellen elektrisch zu löschen und wieder zu beschreiben. Flashspeicher besitzt keine beweglichen Teile wie Magnetplatten und ist widerstandsfähig gegen Erschütterung. Daher wurden Flashspeicher zuerst für mobile Geräte und in anspruchsvollen Umgebungen eingesetzt inzwischen wegen ihrer Geschwindigkeitsvorteile aber auch in jedem beliebigen Computer. In Allgemeinen werden sie als solid state disk (SSD) bezeichnet. Es gibt zwei Varianten von Flashspeichern nähmlich NOR und NAND. Beide Varianten wurden von dem japanischen Elektroingenieur Fujio Masuoka in den 1980er Jahren erfunden. Ein Flash-Chip besteht aus einer Anzahl von sogenannten Erase Blocks. Jeder Block hat die Größe 128 KByte oder 256 KByte. Bei NOR besteht ein Block aus Daten-Bits die einzeln gelesen und geschrieben werden können d.h. jedes Daten-Bit kann einzeln von 1 auf 0 gesetzt werden. Bei NAND besteht ein Block aus einer Anzahl von sogenannten Seiten. Eine Seite enthält nicht nur Daten-Bits sondern auch einen Out-Of-Band-Bereich(OOB) für die Fehlerbehandlung und die Kennzeichnung einer Beschädigung der Seite. Die Daten in einem Block können hier nur seitenweise gelesen und geschrieben werden. Die Löschen-Operation muss bei beiden Flash-Arten auf einem kompletten Block ausgeführt werden. Der Nachteil bei EEPROMs ist dass eine Seite erst wieder geschrieben werden kann nachdem sie gelöscht wurde. Das Löschen einer Seite bedeutet das Löschen des kompletten Blocks in dem sich die Seite befindet. Eine Löschoperation setzt alle Bits des Blocks auf den Wert 1. Also kann eine Schreib-Operation den Zustand eines Bits nur von 1 auf 0 setzen. Bevor ein Block gelöscht wird müssen zuerst die noch gültigen Seiten des Blocks auf andere freie Seiten kopiert werden. Ein Block kann nur eine bestimmte Anzahl von Löschoperationen vertragen typischerweise zwischen 100 000 und einer Million danach ist der Block abgenutzt (wear out) und kann nicht mehr für die Speicherung von Daten weiter verwendet werden. Deshalb benötigt ein Flashspeicher eine Organisationsstruktur um zu wissen welche Seiten und wie viele Seiten in einem Block gültig sind und wie oft die Löschoperation auf jedem Block stattgefunden hat. Diese Information ist wichtig für die Entscheidung welcher Block als nächster gelöscht werden soll. Auf der Magnetplatte und dem Flashspeicher gespeicherte Programme und Daten sind zwar gegen Stromausfall geschützt trotzdem sind Platten und Flash-Speicher für die Langzeitarchivierung aus folgenden Gründen nicht gut geeignet: Magnetplatten sind für wahlfreien Zugriff konzipiert und deshalb pro MByte Kapazität zu teuer um darauf Informationen abzulegen auf die nur selten zugegriffen wird. Sie sind oft im selben Gehäuse untergebracht wie Prozessor und Hauptspeicher und erlauben es deshalb nicht Informationen an getrenntem Ort zu verwahren oder auf andere Rechner zu transferieren wie man es für eine Sicherungskopie benötigt. Die Blöcke eines Flashspeichers können nur begrenzt oft gelöscht werden. Regelmäßiges Sichern erfordert daher regelmäßigen Austausch des Speichermediums. Aus diesen Gründen gibt es neben dem Sekundärspeicher auch Tertiärspeicher damit beschäftigt sich der folgende Abschnitt. Tertiärspeicher Das wesentliche Merkmal von Tertiärspeichern ist dass sich der Datenträger preiswert herstellen und leicht vom Rechner entfernen lässt. Sie eignen sich gut für den Transfer von Daten zwischen nicht vernetzten Rechnern. Bis ca. 2000 waren Disketten weit verbreitet besonders bei PCs. Danach wurden Diskettenlaufwerke aber immer seltener in neue Computer eingebaut heute sind sie praktisch verschwunden. Ihre Rolle als bequem transportierbares Speichermedium nahmen zunächst die CDs und DVDs ein die aber wegen der eingeschränkten Wiederbeschreibbarkeit nicht wirklich funktionsgleich sind und dann die USB-Sticks siehe dazu weiter unten in diesem Abschnitt. Diskettenlaufwerke sind im Prinzip ähnlich den Festplattenlaufwerken sie sind aber einfacher aufgebaut. Auch die Diskette bietet wahlfreien Zugriff dem im Verhältnis zur Magnetplatte deutlich geringeren Preis stehen aber eine erheblich längere Zugriffszeit und ein viel kleineres Datenvolumen (nur ca. 1 MByte) gegenüber. Für die Langzeitarchivierung von Daten und Programmen werden gern Magnetbänder verwendet. Ihre Kapazität ist sehr viel größer als die einer Magnetplatte da auch die magnetisierbare Oberfläche größer ist. Wie im Audiobereich gibt es Bandspulen und Kassetten die leichter zu wechseln sind. Auch Videokassetten finden Verwendung. Die Daten werden blockweise aufgezeichnet. Im Gegensatz zu den bis jetzt besprochenen Speicherarten bieten Magnetbänder aber nur sequentiellen Zugriff: Man kann nur auf die beiden Blöcke direkt zugreifen die sich gerade vor oder hinter dem Schreib-/Lesekopf befinden. Will man andere Blöcke lesen muss das Band erst vor- oder zurückgespult werden was zu extrem langen Zugriffszeiten führt. Im Unterschied zur Magnetplatte brauchen beim Magnetband die Blöcke nicht gleich lang zu sein. Weil auf Bändern oft keine Dateisysteme angelegt werden kann es Mühe bereiten ein fremdes Band richtig zu lesen. In den Rechenzentren früherer Jahre war die Bedienmannschaft (Operator) dafür verantwortlich bei Bedarf die benötigten Bänder oder Wechselplatten in die Laufwerke einzulegen. Diese Aufgabe kann heute von Robotern übernommen werden. Oft fasst man einige hundert Kassetten zu einer sogenannten juke box17 zusammen die mit einer Wechselmechanik versehen ist. Als Ersatz für die Disketten kamen zunächst die CD-ROMs (compact disks kurz CDs) und dann die DVDs (digital versatile disks) zum Einsatz. Es handelt sich um \(1 2\) mm dicke Scheiben aus transparentem Polycarbonat mit \(12\) cm Durchmesser die Speicherkapazität beträgt bei CDs mindestens \(650\) MByte und bei DVDs mindestens \(4 38\) und maximal \(8 5\) GByte. Entsprechende Laufwerke wurden seit ca. 1995 in praktisch alle Computer eingebaut kommerzielle Software wurde oft auf CDs oder DVDs ausgeliefert die bei hoher Kapazität extrem preisgünstig herzustellen und leicht zu transportieren sind. Die Daten werden optisch gespeichert und von einem Laser abgetastet. Zu den nur lesbaren CD-ROMs und DVD-ROMs kamen dann auch schnell die einmal beschreibbaren CD-Rs und DVD-Rs sowie die löschbaren und deshalb mehrfach wiederbeschreibbaren Varianten CD-RW (compact disk - read/write) und DVD-RW durch die die Anwendungsmöglichkeiten dieser Speichertechnik stark vergrößert wurden. Trotzdem sind diese Medien kein vollwertiger Ersatz für die Disketten weil nicht beliebige Sektoren schnell gelöscht und überschrieben werden können. Die Zugriffszeiten für CDs und DVDs variieren je nach Umdrehungsgeschwindigkeit und Zugriffsart sind aber jedenfalls wesentlich länger als die von Magnetplatten und kürzer als die von Disketten. Die optischen Speichermedien Blu-ray Disk (kurz BD) und HD DVD wurden als Nachfolger der DVD entwickelt. Die beiden Formate sind technisch sehr ähnlich den Formatkrieg hat schließlich die BD gewonnen. Die Speicherkapazität der Blu-ray Disk beträgt mindestens 25 GByte die Datenübertragungsrate ist viermal so hoch wie die einer DVD bei gleicher Umdrehungszahl beträgt also mindestens \(4 5\) MByte pro Sekunde bei sogenannter einfacher und \(36\) MByte pro Sekunde bei achtfacher Geschwindigkeit. Externe Festplatten sind handelsübliche Festplatten die statt direkt in den Computer in ein besonderes meist recht kleines Gehäuse eingebaut und über eine Standardschnittstelle wie z.B. USB Firewire oder eSATA angeschlossen werden. Oft erfolgt auch die Stromversorgung über diese Schnittstelle so dass die Festplatte sehr leicht angeschlossen beschrieben abgezogen transportiert und woanders wieder angeschlossen werden kann. Die Speicherkapazität ist im Vergleich zu CDs und DVDs sehr groß und die Zugriffszeiten sind je nach Schnittstelle fast so kurz wie die von internen Festplatten. Noch preisgünstiger als externe Festplatten sind Flash-basierte Speicher. Hier handelt es sich um Speicherchips die Daten permanent also auch ohne Stromversorgung speichern und die auch schnell überschrieben werden können. Solche Speicher existieren einerseits in der Form von sogenannten USB-Sticks d.h. direkt an einen USB-Stecker angeschlossene Speicherchips in einem sehr kleinen Gehäuse und andererseits als noch kleinere Speicherkarten die in Digitalkameras und Mobiltelefonen eingesetzt werden. Kartenschächte zur Aufnahme dieser Speicherkarten werden in viele Rechner eingebaut. Zum Abschluss unserer Besprechung der verschiedenen Speicherarten möchte ich einen dringenden Appell an Sie richten: Sichern Sie Ihre Programme und Daten regelmäßig und nutzen Sie dabei die Möglichkeiten die Ihnen die verschiedenen Speichertypen bieten! Konkret bedeutet das: Wer interaktiv an einem längeren Text oder an einem Progammierprojekt arbeitet sollte regelmäßig den aktuellen Stand auf der Festplatte sichern.18 Ansonsten kann es passieren dass bei einem Stromausfall oder bei einem Rechnerabsturz die Arbeit einiger Stunden verloren geht - vielleicht keine Katastrophe aber doch höchst ärgerlich. Speicherung auf einer Festplatte allein genügt aber nicht denn bei einem Laufwerksdefekt kann der Platteninhalt verloren sein. Verschwindet dabei zum Beispiel kurz vor dem Abgabetermin Ihre Abschlussarbeit so kann das durchaus katastrophale Folgen haben. Deshalb muss man regelmäßig von wichtiger Information Sicherheitskopien (backups) anlegen sei es nun auf Diskette Magnetband CD externer Festplatte Flash-Speicher oder über das Netzwerk auf anderen Computern. Wichtig ist dass diese Sicherungskopien an einem anderen Ort verwahrt werden als der Rechner. Denn sonst bleibt die Gefahr dass bei Einbruch Feuer- oder Wasserschaden doch alle Daten verloren gehen im Geschäftsleben könnte das den Ruin bedeuten. Zum Schluss ein paar Bemerkungen zu den Bezeichnungen. Die Namen Haupt- bzw. Primärspeicher Sekundärspeicher und Tertiärspeicher weisen auf eine hierarchische Ordnung hin: Was im Primärspeicher keinen Platz findet kommt in den Sekundärspeicher was dort nicht hineinpasst steht im Tertiärspeicher. Es gibt noch eine zweite Beziehung: Der Sekundärspeicher enthält eine (möglicherweise aktuellere) Kopie von Teilen der Information des Tertiärspeichers. Ebenso enthält der Hauptspeicher aktuellere Kopien von Sekundärspeicherinformationen. Jede Schicht in Abbildung 1.4 fungiert also als Cache für die Schicht unter ihr. Abbildung 1.4: Jede Schicht arbeitet als Cache für die nächsttiefere Schicht. Die Zugriffszeiten nehmen nach oben hin ab die Kosten nehmen zu. Übungsaufgabe 1.6 Sortieren Sie alle bis jetzt behandelten Speicherformen nach Zugriffszeit. Aufgabe jetzt bearbeiten Sekundär- und Tertiärspeicher werden manchmal auch Externspeicher genannt weil sie in eigenen Gehäusen untergebracht sein können. Man bezeichnet diese Komponenten auch als Ein-/Ausgabegeräte zusammen mit Drucker Monitor Tastatur und Maus denn sie ermöglichen den Fluss von Information in das Rechnerinnere herein und hinaus.  
2; mod_longpage;120; 1; 3; 1.3 Gerätesteuerung In den vorangegangenen Abschnitten haben wir uns mit wichtigen Komponenten vertraut gemacht aus denen die Hardware eines Computers besteht: Prozessor Cache Hauptspeicher Sekundär- und Tertiärspeicher. Nun wollen wir sehen wie die Komponenten eines Computers zusammenwirken welche Hilfe die Rechnerhardware dabei leistet und welche Aufgaben dabei das Betriebssystem übernimmt. Eine wichtige Aufgabe des Betriebssystems ist die Kommunikation mit Geräten. Controller In Abschnitt 1.10 wurde beschrieben wie CPU und Hauptspeicher über Adress- und Datenbus miteinander kommunizieren. Auch die Ein-/Ausgabegeräte sind mit Prozessor und Hauptspeicher über einen Bus verbunden. Ein Bus ist - grob gesagt - ein Bündel von Leitungen zusammen mit einem Protokoll das genau festlegt welche Nachrichten über den Bus geschickt werden können und wie diese Nachrichten durch Signale auf den Leitungen dargestellt werden. Ein Bus verbindet - anders als ein Kabel - nicht nur zwei Geräte miteinander sondern viele Geräte die alle über den Bus miteinander kommunizieren können. Zu jedem Zeitpunkt kann aber nur eine Nachricht über den Bus verschickt werden welches Gerät den Bus zuerst ergreift ist an der Reihe. Abbildung 1.5 zeigt als Beispiel einen Teil eines Computersystems.19 Abbildung 1.5: Ein Computersystem. Es fällt auf dass die Geräte nicht direkt an den Bus angeschlossen sind sondern über einen sogenannten Controller. Das hat folgenden Grund: Wenn die CPU alle Geräte selbst steuern müsste - man denke nur an das ständige Berechnen von PrüfsummenPrüfsumme beim Lesen einer Platte - wäre sie damit so belastet dass für die eigentliche Programmausführung zu wenig Zeit bliebe. Aus diesem Grund gibt es für jeden Gerätetyp einen Controller der die Steuerung des nackten Geräts übernimmt. Oft können mehrere Geräte gleichen Typs an einen Controller angeschlossen werden. Die Controller und die CPU arbeiten parallel aber nur einer von ihnen kann zu einem gegebenen Zeitpunkt den Bus benutzen.Gerätesteuerung Ein Controller ist ein Stück elektronische Hardware. Wie kompliziert er ist hängt vom jeweiligen Gerätetyp ab. So kommt zum Beispiel eine Maus mit einem sehr einfachen Controller aus während der Controller einer Magnetplatte schon recht kompliziert ist. Meist wird hierfür ein vollwertiger Prozessor verwendet der auf einer Platine im Gehäuse des Plattenlaufwerks untergebracht ist. Der Plattencontroller steuert zum Beispiel die Armbewegung führt Fehlererkennung durch und kann einen schadhaften Sektor der Platte dem Betriebssystem melden. Wenn keine Fehler vorliegen extrahiert der Controller die Nutzinformation aus dem gelesenen Sektor und schreibt den Block in einen controllereigenen Pufferspeicher.20 Gerätetreiber Das Betriebssystem hat die Aufgabe die Geräte zu steuern dazu kommuniziert es mit den Controllern. Weil sich die Controller sehr voneinander unterscheiden ist für jeden Controller ein eigener Teil des Betriebssystems zuständig: ein Gerätetreiber Gerätetreiber siehe Abbildung 1.6. Als Teil des Betriebssystems ist der Gerätetreiber ein Stück Software. Abbildung 1.6: Ein virtuelles Gerät (grau hinterlegt). Der Gerätetreiber kommuniziert nur mit dem Controller nicht direkt mit dem Gerät. Zu diesem Zweck besitzt der Controller mehrere Register darunter die folgenden: Datenausgangsregister (data-out) hierhin schreibt der Treiber Daten die für den Controller bestimmt sind 21 Dateneingangsregister (data-in) in das der Controller Daten schreibt die für den Gerätetreiber bestimmt sind Statusregister (status) hier kann der Treiber den Zustand des Geräts abfragen ob es zum Beispiel noch beschäftigt ist oder ob Daten aus dem Dateneingangsregister abgeholt werden können Kontrollregister (control) hier hinterlegt der Treiber Befehle an den Controller zum Beispiel einen Lesebefehl. Für die Kommunikation mit dem Controller hat jedes seiner Register eine Portnummer als Adresse mit der die CPU das Register ansprechen kann. Die CPU verwendet spezielle Ein-/Ausgabebefehle wie z.B. Lese-Befehl IN REG PORT: liest das Register des Controllers mit der Nummer Port und speichert den Inhalt ins CPU-Register REG Schreib-Befehl OUT PORT REG: schreibt den Inhalt von CPU-Register REG ins Register des Controllers mit der Nummer PORT. Mit diesen Befehlen kann die CPU über besondere Busadressen auf die Controllerregister zugreifen die am Bus an die durch PORT spezifizierten Adressen angeschlossen sind. Eine andere Technik für die Kommunikation zwischen der CPU und Controllern ist die speicherabgebildete Ein-/Ausgabe (memory-mapped I/O). Dabei werden die Register des Controllers als Teil des Hauptspeichers adressiert Der auf der CPU laufende Gerätetreiber kann dann die sehr viel schnelleren Prozessorbefehle für den Datenaustausch mit den Controllerregistern verwenden vergleiche das Beispiel in Abschnitt 1.10 Die Anwendungsprogramme und andere Teile des Betriebssystems können nur über den Gerätetreiber auf das Gerät zugreifen. Wie sich Controller und Gerätetreiber die Arbeit teilen ist von Fall zu Fall verschieden. Generell ist die Implementierung von Funktionen in Hardware effizienter als eine Softwarelösung und sie dient der Abstraktion weil die Details in der Hardware verborgen werden. Eine Implementierung in Software macht dagegen oft weniger Arbeit und lässt sich leichter ändern. Gerätetreiber Controller und das Gerät bilden konzeptuell eine Einheit die als virtuelles Gerät22 bezeichnet wird siehe Abbildung 1.6. Exkurs: Abstraktion Kapselung und Schichtenmodell Im Modell des virtuellen Geräts werden drei wichtige Konzepte verwendet: Abstraktion Kapselung auch Geheimnisprinzip genannt und Schichtenmodell. Diese Konzepte spielen in der Informatik eine große Rolle nicht nur bei den Betriebssystemen sondern auch beim Software Engineering und beim Algorithmenentwurf. Grund genug also sich etwas näher mit ihnen zu beschäftigen. Als Beispiel wählen wir ein Magnetplattenlaufwerk dessen Funktionsweise in Abschnitt 1.12 diskutiert worden ist. Abstraktion bedeutet von technischen Details abzusehen und sich auf das Wesentliche Prinzipielle zu konzentrieren. Wesentlich an der Festplatte ist dass wir sie als array[0 .. \(n-1\)] of block auffassen können und dass man auf die Blöcke wahlfrei zugreifen kann. Unwesentlich ist dagegen wie ein Block auf der Platte durch Sektoren realisiert wird und welche Verwaltungsinformation darin gespeichert wird denn diese zusätzliche Information wird nur intern vom Controller benötigt aber nicht außerhalb des virtuellen Geräts. virtuelles Gerät Unwesentlich ist auch wie weit der Arm des Laufwerks bewegt werden muss um einen bestimmten Block zu lesen auf der Abstraktionsebene des virtuellen Geräts - das heißt an der Schnittstelle des Gerätetreibers zum Rest des Betriebssystems - gibt es nur Befehle wie read(\(b\)) write(\(b\)) seek(\(i\)) wobei \(b\) einen Block bezeichnet und \(i\) eine Blocknummer. Rekapitulieren wir: Das Konzept der Abstraktion besagt dass man auf höherer Ebene gewisse Details nicht zu kennen braucht. Das Prinzip der Kapselung geht noch einen Schritt weiter: man darf die Details nicht einmal kennen! Aus dieser Forderung ist die alternative Bezeichnung Geheimnisprinzip entstanden. Im Beispiel mit der Magnetplatte bedeutet das: Die Befehle für die Bewegung des Arms existieren nur im Innern des virtuellen Geräts außerhalb kann man diese Befehle dagegen nicht aufrufen. virtuelles Gerät Warum ist solch eine strenge Kapselung sinnvoll? Zwei gute Gründe lassen sich anführen. Erstens: Wenn Sie den Auftrag bekommen einen Gerätetreiber zu schreiben so haben Sie volle und alleinige Kontrolle darüber dass Befehle zur Armsteuerung nur auf sinnvolle Weise verwendet werden also zum Beispiel nicht bei stehendem Motor. Könnte dagegen jeder Benutzer diese Befehle aufrufen so ergäbe sich hier ein Problem. Und zweitens: Nehmen wir an das Plattenlaufwerk wird irgendwann durch ein leistungsfähigeres ersetzt. Dann ändern sich möglicherweise auch die Befehle zur Armsteuerung. Würden nun diese Befehle im gesamten Betriebssystem und sogar von Anwenderprogrammen benutzt so müsste die gesamte Software erneuert werden - ein enorm hoher Aufwand! Bei guter Kapselung braucht man dagegen nur den Gerätetreiber zu ersetzen.23 Bei Magnetplatten sind noch weitere Funktionen im virtuellen Gerät gekapselt. Ein Beispiel ist die Erkennung schadhafter Sektoren. Manche Controller unterhalten schon ab Werk ein Verzeichnis aller Sektoren auf den Plattenoberflächen die nicht einwandfrei funktionieren. Die vom Betriebssystem an den Treiber übergebenen Blöcke werden natürlich nur auf intakte Sektoren geschrieben. Folglich kann es Unterschiede zwischen Block- und Sektornummern geben die intern vom virtuellen Gerät verwaltet werden. Auch die Optimierung der Armbewegungen wie wir sie in Übungsaufgabe 19 diskutiert hatten lässt sich im virtuellen Gerät kapseln. Soviel zu den Begriffen Abstraktion und Kapselung sie werden Ihnen im weiteren Studium wiederbegegnen insbesondere im Zusammenhang mit abstrakten Datentypen siehe z.B. die Kurse über Datenstrukturen und objektorientierte Programmierung. In beiden Fällen geht es darum Objekte und die Methoden (d.h. Operationen) mit denen auf die Objekte zugegriffen werden kann zu Einheiten zusammenzufassen und den Zugriff auf die Objekte nur über diese Methoden zu gestatten. Als drittes wesentliches Konzept erkennen wir am virtuellen Gerät den Aufbau in Schichten siehe Abbildung 1.6. In unserem Beispiel ist die oberste Schicht in Software implementiert die mittlere in elektronischer Hardware und die unterste in mechanischer Hardware. Die Kommunikation mit der Außenwelt erfolgt nur in der obersten Schicht. In jeder Schicht werden von oben ankommende Aufträge bearbeitet und zur weiteren Bearbeitung an die nächsttiefere Schicht weitergeleitet. Wenn in der untersten Schicht der Auftrag vollständig erledigt ist wird dort eine Antwort generiert und durch alle Schichten nach oben geleitet. Die oberste Schicht schickt dann die Antwort an den externen Auftraggeber. Ein solcher Ansatz ist natürlich nur dann sinnvoll wenn jede Schicht einen wesentlichen Beitrag zur Bearbeitung der Aufträge und Antworten leisten kann.24 Das Schichtenmodell bietet einen großen Vorteil: Die Implementierung einer einzelnen Schicht ist verhältnismäßig einfach weil nur die beiden Schnittstellen zur nächsthöheren und zur nächsttieferen Schicht realisiert werden müssen und weil im Innern der Schicht nur eine klar umrissenen Teilaufgabe gelöst werden muss. Beim Entwurf eines Systems nach dem Schichtenmodell muss man eine Abwägung (trade-off) vornehmen: Je dünner also je einfacher man die Schichten macht desto mehr Schichten werden benötigt und desto höher wird der Verwaltungsaufwand (overhead) für die Kommunikation zwischen den Schichten. Je dicker also je komplexer man die Schichten macht desto weniger Schichten werden benötigt und desto geringer wird der Verwaltungsaufwand für die Kommunikation zwischen den Schichten. Allerdings werden Änderungen in einer Schicht komplexer und potenziell fehleranfälliger. Unterbrechungen Für Menschen ist es ziemlich störend ständig bei der Arbeit unterbrochen zu werden. Für den Computer sind Unterbrechungen (interrupts) eine natürliche Form der Kommunikation. Warum das so ist wollen wir jetzt diskutieren. Hardware-Unterbrechungen Nehmen wir an der Gerätetreiber einer Magnetplatte bekommt den Auftrag einen bestimmten Block zu lesen. Wie in Abschnitt 1.15 besprochen hinterlegt der Treiber einen entsprechenden Lesebefehl im Kontrollregister des Controllers. Der Controller kann jetzt mit seiner Arbeit beginnen. Aber wie erfährt die CPU davon wenn der Leseauftrag ausgeführt ist und der gewünschte Block im internen Puffer des Controllers bereitliegt? Auf keinen Fall soll die schnelle CPU auf das langsame Gerät warten müssen. Drei bessere Möglichkeiten bieten sich an: Die CPU kann - neben ihrer anderen Arbeit - immer wieder das Statusregister des Controllers abfragen um festzustellen ob der Auftrag schon erledigt ist diesen Abfragebetrieb nennt man im Englischen polling. Liegen die Register im Hauptspeicherbereich - wie im Fall von speicherabgebildeter Ein-/Ausgabe - so lässt sich eine solche Abfrage zwar recht schnell erledigen aber wenn sie immer wieder erfolglos bleibt wird insgesamt viel CPU-Zeit damit verbraucht. Der Controller benachrichtigt die CPU sobald er den Auftrag ausgeführt hat hierzu unterbricht er die CPU bei ihrer augenblicklichen Arbeit. Dieser Unterbrechungsbetrieb bildet die Grundlage für die Arbeitsweise moderner Computersysteme. Ein modernes Computersystem besitzt einen Interrupt-Controller. Der Interrupt-Controller trennt die CPU von verschiedenen Geräten und vermittelt der CPU die Unterbrechungswünsche der dazugehörigen Geräte-Controller.Der Interrupt-Controller ist deshalb sowohl mit den verschiedenen Geräte-Controllern verbunden als auch über mehrere Leitungen mit der CPU: Eine Leitung ist mit dem Unterbrechungseingang der CPU verbunden eine Leitung dient zur Unterbrechungsbestätigung durch die CPU und eine Leitung für das Übertragen der sogenannten Unterbrechungsnummer die den betreffenden Geräte-Controller der den Unterbrechungswunsch signalisiert hat identifiziert.Sobald der Geräte-Controller mit einem Auftrag fertig ist z.B. Lesen eines Datenblocks von der Festplatte sendet er ein Signal an den Interrupt-Controller. Der Interrupt-Controller teilt diesen Unterbrechungswunsch der CPU mit. Wenn die CPU diese Unterbrechung bearbeiten will bestätigt sie dem Interrupt-Controller dies über die Bestätigungsleitung. Anschließend überträgt der Interrupt-Controller die Unterbrechungsnummer des Geräte-Controllers an die CPU. Wir werden uns deshalb in diesem Abschnitt damit befassen wie der Unterbrechungsbetrieb organisiert ist. Eine Unterbrechung signalisiert ein bestimmtes Ereignis das von einer Quelle (Hardware oder Software) gemeldet wird und das von der CPU auf eine spezielle Weise behandelt werden soll. Ein Beispiel für ein solches Ereignis ist das Vorliegen von neuen Daten im Tastaturpuffer. Deswegen gibt es für jeden Typ solcher Ereignisse eine Unterbrechungsnummer und eine dazugehörige Unterbrechungsroutine (Interrupt Service Routine ISR) die bei Auftreten eines Ereignisses dieses Typs vom Betriebssystem ausgeführt werden soll. Das Vorliegen einer Unterbrechung wird der CPU durch das Anlegen eines Signals an ihrem Unterbrechungseingang angezeigt. Die daraufhin ablaufende Unterbrechungsbehandlung schauen wir uns nun genauer an: Die CPU prüft im Instruktionszyklus zu Beginn der Holphase ob ein Unterbrechungssignal anliegt. Falls nicht dann wird der Instruktionszyklus weiter ausgeführt und der nächste Befehl des aktuell rechnenden Prozesses ausgeführt. Falls ein Unterbrechungssignal anliegt wird automatisch (durch entsprechende Schaltungen in der CPU) der aktuelle Registersatz in den systemeigenen Speicher (z.B. ein Stack25 im Cache auf dem CPU-Chip) kopiert. Mit dieser Kopie kann später der Zustand vor der Unterbrechung wiederhergestellt werden. Die CPU schaltet nun in den Systemmodus26 und muss die passende Unterbrechungsroutine aufrufen. Eine einfache Möglichkeit dies zu bewerkstelligen ist es die Startadresse einer allgemeinen Unterbrechungsroutine die an einer bekannten Adresse im Speicher steht in das Befehlszählregister der CPU zu laden. Dies bewirkt einen Sprung an diese Adresse und die allgemeine Unterbrechungsroutine wird ausgeführt. Die allgemeine Unterbrechungsroutine muss zuerst die Unterbrechungsnummer feststellen (z.B. durch Polling der Geräte oder durch Abfrage beim Interrupt-Controller). Im Unterbrechungsvektor siehe Abbildung 1.7 steht an dieser Stelle die Startadresse der passenden Unterbrechungsroutine. Diese Adresse wird in das Befehlszählregister der CPU geladen und damit in diese Routine gesprungen. Die Instruktionen in der Unterbrechungsroutine haben die Aufgabe den Unterbrechungswunsch des Geräts der durch das Unterbrechungssignal angezeigt wird zu bearbeiten: Sie kann zum Beispiel den Gerätetreiber darüber informieren dass der Controller seinen Lesebefehl ausgeführt hat und die Daten nunmehr bereitstehen. Am Ende der Unterbrechungsroutine muss der Registersatz des unterbrochenen Prozesses wie er vor der Unterbrechung vorlag durch den Befehl Return-from-Interrupt wiederherstellt werden. Glücklicherweise hat die CPU zu Beginn der Unterbrechungsbehandlung diesen Registersatz im systemeigenen Speicher abgelegt siehe Schritt 2. Von dort wird nun der Registersatz etc. wiederhergestellt. Die unterbrochene Berechnung des Prozesses wird nun weiter ausgeführt als ob es nie eine Unterbrechung gegeben hätte. Eine Unterbrechung führt deshalb immer zum Aufruf der passenden Unterbrechungsroutine im Systemmodus d.h. im Kernel des Betriebssystems. Daher können Unterbrechungen als Mechanismus gesehen werden mit dem man das Betriebssystem aufrufen bzw.aktivieren kann. Abbildung 1.7: Ein Unterbrechungsvektor. Es kann vorkommen dass während der Bearbeitung einer Unterbrechung ein weiterer Unterbrechungswunsch eintrifft. Zur Vermeidung eines Durcheinanders sind verschiedene Maßnahmen möglich. Zum einen lässt sich bei vielen Prozessoren der Unterbrechungseingang vorübergehend außer Betrieb setzen (interrupt disabled) so dass weitere Unterbrechungssignale wirkungslos bleiben.27 So kann die CPU die zuerst eingetroffene Unterbrechung ungestört bearbeiten. Dieses Verfahren bietet sich auch in anderen Situationen an wenn die CPU bei der Ausführung kritischer Abschnitte eines Programms nicht unterbrochen werden darf hierauf werden wir in Abschnitt 2.3 zurückkommen. Zum anderen kann man jedem Unterbrechungswunsch eine Priorität zuordnen Unterbrechungen niederer Priorität können dann von solchen mit höherer Priorität unterbrochen werden. Übungsaufgabe 1.7 Nach jeder Ausführung eines Befehls überprüft die CPU ob an ihrem Unterbrechungseingang ein Signal angekommen ist. Schleicht sich hier durch die Hintertür der ineffiziente Abfragebetrieb ein den wir durch Einführung des Unterbrechungsbetriebs eigentlich hatten vermeiden wollen? Aufgabe jetzt bearbeiten Übungsaufgabe 1.8 Was passiert wenn während der Bearbeitung einer niedrig priorisierten Unterbrechung das Signal einer höher priorisierten Unterbrechung beim Interrupt-Controller eintrifft? Aufgabe jetzt bearbeiten Wir haben oben gesehen dass Geräte (genauer: ihre Controller) Unterbrechungen der CPU auslösen wenn sie ihren Auftrag ausgeführt haben oder wenn ein Fehler aufgetreten ist. Wir bezeichnen diese Unterbrechung die durch ein externes Gerät verursacht wird als Hardware-Unterbrechung die nicht von dem gerade ausgeführten Programm verursacht wird. Diese Unterbrechung kann nicht wieder reproduziert werden wenn das Programm noch einmal ausgeführt wird da das Programm nichts mit der Unterbrechung zu tun hat. Software-Unterbrechung Aber auch die Software kann Unterbrechungen (sogenannte traps) auslösen. Der Unterschied zu der Hardware-Unterbrechung ist dass eine Software-Unterbrechung von dem geraden ausgeführten Programm verursacht wird. So eine Unterbrechung ist reproduzierbar: wenn das Programm noch einmal ausgeführt wird passiert die Unterbrechung genau an derselben Stelle des Programms. Für eine Software-Unterbrechung kann es ganz verschiedene Ursachen geben zum Beispiel will das Programm eine Division durch Null oder einen Zugriff auf nicht existierende oder geschützte Hauptspeicheradressen ausführen für solche Unterbrechungen wird auch die Bezeichnung Ausnahme (exception) verwendet. Eine andere Art von Software-Unterbrechung stellt dagegen keine Ausnahme dar sie ist vielmehr ein übliches Verfahren zur Kontrollübergabe an das Betriebssystem: Wann immer ein Programm Dienste des Betriebssystems in Anspruch nehmen will führt es einen Systemaufruf (system call) durch. Wir werden in Abschnitt 1.6 ausführlicher darauf eingehen welche Systemaufrufe ein Betriebssystem üblicherweise anbietet. Die verfügbaren Systemaufrufe bilden zusammen die Programmierschnittstelle zwischen den Anwenderprogrammen und dem Betriebssystem siehe auch Abschnitt 1.1. Solch ein Systemaufruf hat die Form eines Funktionsaufrufs. Insbesondere können dabei auch Parameter übergeben werden entweder direkt in einem Register der CPU oder indirekt durch Angabe der Anfangsadresse des zu übergebenden Datenbereichs im Hauptspeicher dieses Vorgehen empfiehlt sich bei großen Datenmengen wie zum Beispiel Bildschirminhalten. Man kann zur Parameterübergabe auch den Stapel des Prozesses benutzen. Wenn der Systemaufruf seine Parameter für die Übergabe vorbereitet hat führt er eine besondere Instruktion28 trap aus die die eigentliche Unterbrechung auslöst. Diese Instruktion ist bei allen Systemaufrufen dieselbe der gewünschte Systemdienst wird beim Aufruf durch einen Parameter bezeichnet. Jetzt geschieht dasselbe wie bei einer Hardware-Unterbrechung: Befehlszähler- und Registerinhalte werden gerettet und in Abhängigkeit vom übergebenen Parameter für den gewünschten Dienst springt die allgemeine Unterbrechungsroutine in die entsprechende gewünschte spezielle Unterbrechungsroutine des Betriebssystems. Bei Systemaufrufen werden meistens Daten an das aufrufende Programm zurückgegeben. Die Unterbrechung ist hier also der Mechanismus für den Aufruf einer Betriebsystemfunktion. Wer in einer Hochsprache programmiert wird möglicherweise nicht immer bemerken dass sein Programm Systemaufrufe auslöst: Zum Beispiel gibt es in Pascal einen Standardbefehl namens read mit dem ein Datensatz einer Datei gelesen werden kann. Ein solcher Befehl wird vom Compiler automatisch in einen entsprechenden Systemaufruf übersetzt ohne dass der Programmierer die Details kennen muss. Direkter Speicherzugriff (DMA) Wir hatten in Abschnitt 1.28 besprochen was geschieht wenn ein Plattenlaufwerk einen Leseauftrag ausgeführt hat: Der gewünschte Datenblock steht im Pufferspeicher des Controllers bereit und der Controller schickt ein Unterbrechungssignal an die CPU um sie hierüber zu informieren.29 Wie gelangt der Block nun in den Hauptspeicher? Hier gibt es zwei unterschiedliche Verfahren. Bei der unterbrechungsgesteuerten Ein-/Ausgabe schreibt der Controller jeweils ein Wort in sein Dateneingangsregister und löst eine Unterbrechung aus siehe auch Abschnitt 1.3. Dadurch wird der Gerätetreiber gestartet er veranlasst dass das Wort von der CPU in Empfang genommen und in den Hauptspeicher geschrieben wird. Wenn ein Block 512 Bytes enthält und ein Wort zwei Bytes umfasst wird die CPU bei diesem Verfahren 256-mal unterbrochen bevor der Datenblock endlich im Hauptspeicher steht! Eigentlich ist die CPU für solche niederen Übertragungsdienste zu schade. Deshalb wird für schnelle Peripheriegeräte häufig ein anderes Verfahren gewählt: der direkte Speicherzugriff (direct memory access = DMA). Hierbei wird im Computer ein spezieller DMA-Controller eingesetzt der selbständig über den Bus Daten in den Hauptspeicher übertragen kann ohne die CPU zu bemühen. Auch der Gerätecontroller muss für das DMA-Verfahren ausgelegt sein. Der Gerätetreiber teilt dem Gerätecontroller über dessen Register die Nummer des zu lesenden Blocks mit und dem DMA-Controller die Anfangsadresse des Hauptspeicherbereichs in den die Daten übertragen werden sollen. Danach kann sich die CPU anderen Aufgaben widmen. Der Gerätecontroller setzt sich nun über spezielle Leitungen mit dem DMA-Controller in Verbindung. Wann immer das nächste Wort im Register des Gerätecontrollers bereitsteht sendet er über eine Anforderungsleitung ein Signal an den DMA-Controller dieser schreibt die entsprechende Hauptspeicheradresse auf den Adressbus und signalisiert über die Bestätigungsleitung dem Gerätecontroller die Übertragung durchzuführen. Dann zählt der DMA-Controller die Adresse für das nächste Wort hoch. Erst wenn alle Wörter des gesamten Blocks übertragen sind löst er eine Unterbrechung der CPU aus. Ein Beispiel für den Aufbau eines DMA-fähigen Systems zeigt Abbildung 1.8. Abbildung 1.8: Eine mögliche Rechnerarchitektur für direkten Speicherzugriff (DMA). Übungsaufgabe 1.9 Das Ziel des DMA-Verfahrens liegt in einer Entlastung der CPU. Wieso kann es trotzdem vorkommen dass die Arbeit der CPU verzögert wird während DMA-Vorgänge ablaufen? Aufgabe jetzt bearbeiten Eine Kommunikation wie die zwischen DMA- und Gerätecontroller wird im Englischen als handshaking bezeichnet sie stellt einen einfachen Fall eines Protokolls dar. System- und Benutzermodus Speicherschutz Auch das Betriebssystem selbst ist ein Programm. Wenn der Rechner eingeschaltet wird steht ein kleiner Teil des Systems - der sogenannte Ur-Lader (bootloader) - in einem speziellen Teil des Hauptspeichers der seinen Inhalt auch nach Abschalten des Stroms behält. Meist ist der Ur-Lader in einem ROM (read only memory) fest eingebrannt das auch Firmware genannt wird bzw. BIOS bei PCs. Der Ur-Lader wird beim Einschalten des Rechners automatisch ausgeführt und lädt zunächst den eigentlichen Lader von der Magnetplatte.30 Der Lader wird nun gestartet und lädt das Betriebssystem - zumindest seine wesentlichen Teile - in den Hauptspeicher. Dieser Vorgang wird als hochfahren (booting) bezeichnet. Wenn jetzt ein Anwenderprogramm ausgeführt wird könnte es im Prinzip versuchen Teile des Hauptspeicherbereichs zu überschreiben in dem das Betriebssystem steht dazu braucht das Programm ja nur die entsprechenden Speicherzellen zu adressieren wie wir in Abschnitt 1.10 besprochen haben. Ebenso könnte ein Benutzerprogramm sich selbst oder andere Programme die sich gerade im Hauptspeicher befinden verändern. Ob eine solche Veränderung nun versehentlich oder mit Absicht geschieht: die Folgen können schlimm sein.31 Aus diesem Grund kann es einem Benutzerprogramm nicht gestattet werden auf beliebige Teile des Hauptspeichers zuzugreifen. Auch der Sekundärspeicher muss vor unbefugtem Zugriff geschützt werden denn von dort wird ja das Betriebssystem beim nächsten Einschalten geladen und es befinden sich dort auch andere Programme. Schließlich bedarf auch der Zugriff auf die anderen Geräte der Kontrolle wenn zum Beispiel mehrere Programme gleichzeitig auf den Drucker zugreifen kann sonst ein Durcheinander entstehen. Um diesen Schutz zu gewährleisten können moderne Prozessoren im Systemmodus32 (system mode) oder im Benutzermodus (user mode) arbeiten 33 zur Unterscheidung wird ein besonderes Bit im Prozessor verwendet. Alle Anwendungsprogramme laufen im Benutzermodus. Bestimmte privilegierte Maschinenbefehle können aber nur im Systemmodus ausgeführt werden. Damit solch ein Schutzmechanismus wirksam ist muss natürlich der Befehl zum Umschalten vom Benutzer- in den Systemmodus selbst auch privilegiert sein. Muss man also schon privilegiert sein um Privilegien zu bekommen? Dank des Betriebssystems nicht! Dieses Problem wird so gelöst: Wenn eine Unterbrechung auftritt - insbesondere wenn ein Benutzerprogramm einen Systemaufruf auslöst - schaltet die privilegierte Prozedur zur Unterbrechungsbehandlung den Prozessor in den Systemmodus und startet dann die entsprechende Unterbrechungsroutine vergleiche Abschnitt 1.17. Sie ist Teil des Betriebssystems und deshalb vertrauenswürdig. Bevor die Unterbrechungsroutine terminiert schaltet sie in den Benutzermodus zurück. Kein Benutzer kann also direkt auf ein Gerät zugreifen man muss zu diesem Zweck das mit höheren Rechten ausgestattete Betriebssystem um Hilfe bitten. Übungsaufgabe 1.10 Beim Return-from-Interrupt stellt die CPU den Registersatz vor der Unterbrechung wieder her. Was gehört dazu? Aufgabe jetzt bearbeiten Übungsaufgabe 1.11 Beim Return-from-Interrupt wird der Modus gesetzt auf ... Aufgabe jetzt bearbeiten Mit diesen Hilfsmitteln lässt sich auch ein wirksamer Speicherschutz realisieren und zwar auf folgende Weise: Bevor ein Programm gestartet wird weist ihm das Betriebssystem einen bestimmten Bereich im Hauptspeicher zu seinen sogenannten Adressraum. Das Programm darf nur auf solche Adressen zugreifen die in seinem eigenen Adressraum liegen. Das bedeutet: Alle möglicherweise benötigten Daten aber auch das Programm selbst müssen in diesem Adressraum enthalten sein. Ein zusammenhängender Adressraum lässt sich durch die Inhalte von zwei speziellen Registern der CPU festlegen. Im Basisregister (base register) steht die niedrigste Adresse des Adressraums das Grenzregister (limit register) enthält die Länge des Adressraums also die Differenz aus der höchsten und der niedrigsten erlaubten Adresse plus Eins. Die Inhalte von Basis- und Grenzregister können nur mit speziellen Maschinenbefehlen verändert werden. Diese Befehle sind privilegiert. Wann immer das Programm zur Laufzeit versucht auf eine Speicherzelle zuzugreifen wird zunächst überprüft ob die angegebene Adresse im erlaubten Bereich liegt siehe Abbildung 1.9. Wenn das der Fall ist wird der Speicherzugriff durchgeführt andernfalls wird eine Software-Unterbrechung ausgelöst wie in Abschnitt 1.17 beschrieben. Abbildung 1.9: Speicherschutz mit Basis- und Grenzregister. Mit einer leichten Modifikation dieser Technik lässt sich gleich noch ein zweites Problem erledigen. Wenn ein Programm vom Compiler in Maschinensprache übersetzt wird steht noch nicht fest wo im Hauptspeicher der Adressraum des Programms später liegen wird - das hängt ja auch davon ab welche anderen Programme dann gerade im Hauptspeicher stehen. Der Compiler kann deshalb an die Befehle und die Daten zunächst nur relative Adressen vergeben wie etwa 17 Wörter hinter dem Anfang dieses Programmstücks.34 Wird das Programm nun in seinen Adressraum geladen stimmen die im Programm verwendeten relativen Adressen nicht mit den absoluten Adressen überein an denen die Befehle und Daten tatsächlich stehen. Man könnte deshalb vor dem Laden alle relativen Adressen im Programm um die Anfangsadresse des Adressraums erhöhen. Diese Lösung ist nicht so elegant sie benötigt nämlich zusätzliche Information darüber wo im Programm die Adressen stehen.35 Diese Hilfsinformation müsste eigens vom Computer generiert werden. Viel günstiger ist es die Neuberechnung der Adressen nach der Formel absolute Adresse = (Basisregister) + relative Adresse erst zur Laufzeit durchzuführen! Das heißt: Wenn das Programm eine Speicherzelle adressieren will wird die im Programm enthaltene relative Adresse zur niedrigsten Adresse des Adressraums addiert diese steht im Basisregister. Damit wird auch sichergestellt dass die absolute Adresse nicht unterhalb des Adressraums liegt. Vor der Addition ist noch der erste Test aus Abbildung 1.9 nötig damit der Zugriff nicht über die obere Grenze des Adressraums ausgeht. Weil die Programme zusammen mit ihren Adressräumen beliebig im Hauptspeicher verschoben werden können spricht man bei diesem Verfahren von relokierbaren Programmen (relocatable code). Übungsaufgabe 1.12 Warum darf ein Anwendungsprozess nicht auf den Unterbrechungsvektor zugreifen? Aufgabe jetzt bearbeiten Übungsaufgabe 1.13 Muss der Befehl mit dem der Unterbrechungseingang der CPU außer Betrieb gesetzt (maskiert) wird privilegiert sein? Aufgabe jetzt bearbeiten  
2; mod_longpage;120; 1; 4; 1.4 Prozesse In Abschnitt 1.3 haben wir untersucht wie das Betriebssystem mit Hilfe des Mechanismus der Unterbrechung die Geräte steuert. Wir wollen uns in diesem Abschnitt damit beschäftigen wie das Betriebssystem die Ausführung von Programmen ermöglicht und warum wir das Gefühl haben dass mehrere Prozesse quasiparallel laufen. Prozesszustände und Übergänge In Abschnitt 1.17 haben wir beobachtet was geschieht wenn die CPU bei der Ausführung eines Programms unterbrochen wird: Die Adresse des nächsten auszuführenden Befehls und die Registerinhalte werden im Systemstapel gespeichert damit wird die CPU für andere Aufgaben frei. Später können die Registerinhalte wieder geladen werden und die CPU kann ihre Arbeit an der richtigen Stelle fortsetzen so als hätte es die Unterbrechung nicht gegeben. Ein Programm das sich gerade in Ausführung befindet heißt Prozess. Zum Prozess gehört aber nicht nur ein Programm (Code der ausgeführt werden soll) sondern auch der Prozesskontext bestehend aus den Registerinhalten insbesondere Befehlszähler und Grenzen des Adressraums sowie der Prozessnummer Priorität (benötigt) Modus (im Systemmodus- oder im Benutzermodus) und Zustand und anderen Informationen. Diese Angaben werden im Prozesskontrollblock (process control block = PCB) zusammengefasst. Der PCB steht im Speicherbereich des Betriebssystems und ist somit vor dem Zugriff durch Anwendungsprogramme geschützt. Ein Prozess wird zu irgendeinem Zeitpunkt erzeugt z.B. durch einen anderen Prozess und zu einem späteren Zeitpunkt beendet wenn er mit der Bearbeitung fertig ist. Dazwischen kann er mehrfach zwischen drei Zuständen wechseln die in Abbildung 1.10 dargestellt sind.36 Prozesszustände Abbildung 1.10: Mögliche Prozesszustände Nach der Erzeugung eines Prozesses wird der PCB vom Betriebssystem angelegt und initialisiert d.h. die benötigten Ressourcen wie z.B. ein Speicherbereich werden zugeteilt. Nun ist der Prozess bereit für die Bearbeitung und er geht in den Zustand bereit. Bei den Einprozessorsystemen wie wir sie in diesem Kurs betrachten ist zu jedem Zeitpunkt genau ein Prozess im Zustand rechnend nämlich der Prozess dessen Programm gerade von der CPU ausgeführt wird. Jeder andere existierende Prozess ist entweder bereit und bewirbt sich mit den übrigen bereiten Prozessen um die Zuteilung der CPU oder er ist blockiert und wartet darauf dass seine Ein-/Ausgabeanforderung erledigt wird oder ein bestimmtes Ereignis eintritt. Wenn zum Beispiel ein laufendes Benutzerprogramm auf eine Eingabe von der Tastatur wartet führt es einen Systemaufruf durch. Bei der Durchführung des Systemaufrufs durch das Betriebssystem wird der Prozess vom Zustand rechnend in den Zustand blockiert versetzt da er ohne die Eingabedaten nicht weiterarbeiten kann. Danach ist die CPU frei um andere Prozesse zu bearbeiten. Wenn dann die Daten bereitstehen und der Tastaturcontroller eine Unterbrechung auslöst wird der Benutzerprozess in den Zustand bereit gebracht denn seine Arbeit kann nun weitergehen. Das heißt aber nicht dass er sofort die CPU zugeteilt bekommt: Welcher der bereiten Prozesse als nächster rechnen darf entscheidet der CPU-Scheduler.37 Der Scheduler definiert die Ausführungsreihenfolge der bereiten Prozesse im nächsten Zeitabschnitt. Er kann seine Entscheidungen nach unterschiedlichen Strategien treffen siehe auch Abschnitt 1.21. Der Dispatcher führt den Prozesswechsel durch. Ein rechnender Prozess kann auch durch einen Systemaufruf mitteilen dass er auf ein bestimmtes Ereignis warten will er wird dann blockiert und kommt in eine spezielle Warteschlange in der möglicherweise noch andere blockierte Prozesse stehen die auf dasselbe Ereignis warten. Wenn dann dieses Ereignis eintritt kann der Prozess welcher es auslöst ein Signal abschicken. Dieses Signal bewirkt dass alle Prozesse aus der Warteschlange wieder in den Zustand bereit versetzt werden. Wenn ein Prozess im Zustand rechnend ist muss er irgendwann die CPU wieder abgeben. Grundsätzlich gibt es zwei Arten wie entschieden wird wie ein Prozess vom Zustand rechnend in den Zustand bereit gehen kann: Bei einem nicht präemptiven System gibt ein Prozess die CPU freiwillig ab. Man bezeichnet so ein System auch als kooperatives System. Bei einem präemptiven System wird einem Prozess die CPU entzogen. Die meisten modernen Systeme sind präemptive Systeme. Übungsaufgabe 1.14 Mehrere Prozesse sollen in einem Rechensystem ausgeführt werden. Welche Aussagen sind richtig? Aufgabe jetzt bearbeiten Scheduling-Strategien für nicht präemptive Systeme Wir betrachten zwei Scheduling-Strategien für ein nicht präemptives System: Bei FCFS (first-come first-served) darf zuerst rechnen wer zuerst kommt vergleiche Abschnitt 1.12. Dieses Verfahren ist leicht zu implementieren es genügt eine Warteschlange (queue) einzurichten bei der immer der erste Prozess als nächster an die Reihe kommt und neu eintreffende bereite Prozesse sich hinten anstellen. Der Nachteil: Ein früh eintreffender Prozess mit hohem Rechenzeitbedarf oder einer Endlosschleife hält alle späteren Prozesse auf. Das Verfahren SJF (shortest job first) kann den Nachteil von FCFS vermeiden hier werden die Prozesse im Zustand bereit in der Reihenfolge aufsteigenden Rechenzeitbedarfs bearbeitet: die kurzen zuerst.38 Voraussetzung für eine sinnvolle Anwendung von SJF ist dass sich die benötigte Rechenzeit (bis zur nächsten Unterbrechung) aus Erfahrungswerten gut vorhersagen lässt.39 Unter dieser Voraussetzung ist SJF sehr effizient wie die folgende Aufgabe zeigt. Übungsaufgabe 1.15 Gegeben sei eine feste Menge von endlich vielen Prozessen mit bekannten Rechenzeiten. Beweisen Sie dass SJF die Gesamtwartezeit minimiert also die Summe aller Wartezeiten der einzelnen Prozesse. Dabei ist die Wartezeit eines Prozesses diejenige Zeit in der sich der Prozess im Zustand bereit befindet. Aufgabe jetzt bearbeiten In einem dynamischen System kommen allerdings immer wieder neue Prozesse hinzu. Je mehr kurze Prozesse generiert werden desto länger müssen die langen Prozesse warten. Wenn also immer neue kurze Prozesse erzeugt werden kommen die langen Prozesse nie an die Reihe. Dieses Problem wird Verhungern (engl.starvation) genannt. Übungsaufgabe 1.16 Wie kann man die SJF-Strategie so modifizieren dass das Problem des Verhungerns vermieden wird? Aufgabe jetzt bearbeiten Das Verfahren SJF eignet sich besonders gut für den Stapel- oder Batch-Betrieb bei dem der Rechner gleich einen ganzen Schub von Aufträgen (jobs) erhält die keine Interaktion mit dem Benutzer erfordern und regelmäßig auszuführen sind so dass man ihre Laufzeiten in etwa kennt. Scheduling-Strategien für präemptive Systeme Moderne Rechner werden meist im Time-Sharing-Betrieb verwendet: Mehrere Benutzer können zur selben Zeit an einem Rechner arbeiten und jeder kann gleichzeitig mehrere Programme laufen lassen zum Beispiel in einem Fenster einen Compiler in einem anderen ein Werkzeug für elektronische Post und in einem dritten Fenster einen WWW-Browser.40 Diese Gleichzeitigkeit ist natürlich eine Illusion denn bei einem Computer mit nur einer CPU kann nur ein einziger Prozess rechnend sein. Der Eindruck von Gleichzeitigkeit entsteht dadurch dass der Scheduler in schnellem Wechsel jedem bereiten Prozess ein gewisses Quantum an Rechenzeit zukommen lässt. Wenn das Quantum verbraucht wird wird dem Prozess die CPU entzogen. Danach erfolgt die Umschaltung zwischen den Prozessen die so schnell ist dass die Unterbrechungen nicht spürbar werden. Drei Strategien können wir uns für präemptive Systeme vorstellen: Ein sehr einfaches Verfahren namens Round Robin ist weit verbreitet bei dem jeder Prozess im Zustand bereit vom Scheduler eine Zeitscheibe (time slice) derselben Länge an Rechenzeit zugewiesen bekommt und die Prozesse im Zustand bereit reihum bedient werden. Dazu kann man sie in der Reihenfolge ihres Eintreffens in einer kreisförmigen Warteschlange speichern. Bei der Strategie Round Robin werden alle Prozesse gleich behandelt. Diese Eigenschaft kann auch ein Nachteil für die interaktiven Prozesse sein die oft eine Eingabe oder Ausgabe benötigen wenn z.B. die Länge der Zeitscheibe zu groß gewählt wird. Eine Verbesserung von Round Robin ist dass der Scheduler einem Prozess die Länge der Zeitscheibe in Abhängigkeit von der Priorität des Prozesses oder davon wieviel Rechenzeit der Prozess insgesamt schon verbraucht hat zuweist.41 Der Scheduler wählt dann den Prozess als nächsten aus der die höchste Priorität hat. Dieses Vorgehen kann aber dazu führen dass bei ständig neu erzeugten Prozessen mit höherer Priorität die niedrig priorisierten Prozesse trotz längerer Zeitscheibe nie rechnend werden. Die Priorität eines Prozesses kann sogar nach seinem Verhalten dynamisch vergeben werden. Beispielsweise warten die interaktiven Prozesse oft auf eine Ein-/Ausgabe und sie benötigen für deren Verarbeitung nur eine kurze Zeitscheibe. Insbesondere benötigen sie hierfür aber oft die CPU um auf die Ein-/Ausgabe reagieren zu können. Um diesem Verhalten zu entsprechen können sie vom Scheduler anhand dieser Charakteristik erkannt und eine kurze Zeitscheibe sowie eine höhere Priorität bekommen. Die technische Realisierung mit der Zeitscheibe kann folgendermaßen geschiehen: Eine Hardware (Zeitgeber (Timer)) wacht darüber dass der Prozess sein Quantum nicht überschreitet oft ist dafür ein eigener Chip im Rechner vorhanden der an den Takt der CPU angeschlossen ist. Die Länge der zugewiesenen Zeitscheibe wird in einem Register des Zeitgebers gespeichert. Nach jeder verstrichenen Zeiteinheit wird der Inhalt dieses Registers um Eins verringert. Ist der Wert bei Null angekommen so ist die zugewiesene Zeitscheibe abgelaufen und der Zeitgeber löst eine Unterbrechung der CPU aus. Der Prozess wird unterbrochen und wieder in die Menge der bereiten Prozesse eingereiht vergleiche Abbildung 1.10. Nun kommt ein anderer Prozess an die Reihe. Übungsaufgabe 1.17 Muss der Zugriff auf das Register des Zeitgebers privilegiert sein? Aufgabe jetzt bearbeiten Wie effizient dieses Verfahren arbeitet hängt von der Länge der Zeitscheiben ab. Die nächste Aufgabe zeigt dass extreme Werte unzweckmäßig sind. Das Umschalten zwischen Prozessen bezeichnet man auch als Kontextwechsel (context switch). Während der Scheduler die Strategie für die Rechenzeitvergabe festlegt führt der Dispatcher die eigentlichen Kontextwechsel durch. Da solche Kontextwechsel häufig vorkommen ist es wichtig dass der Dispatcher möglichst schnell arbeitet. Übungsaufgabe 1.18 Welche Gefahren drohen wenn beim Verfahren Round Robin die Zeitscheiben zu lang oder zu kurz gewählt werden? Aufgabe jetzt bearbeiten  
2; mod_longpage;120; 1; 5; 1.5 Rekapitulation: ein Gerätezugriff In diesem Abschnitt wollen wir im Zusammenhang darstellen was das Betriebssystem leistet wenn es bei der Ausführung eines Programms auf eine Ein-/Ausgabeanforderung stößt etwa auf einen Lesebefehl read(\(f b\)) bei dem der Datensatz b von der Datei f gelesen werden soll. Die meisten Teilschritte haben wir in den vorangegangenen Abschnitten bereits diskutiert. So wurde am Ende von Abschnitt 1.17 festgestellt dass schon bei der Übersetzung des Programms der Hochsprachen-Befehl read durch einen entsprechenden Systemaufruf ersetzt wird der den Ein-/Ausgabeteil des Betriebssystems startet. Außerdem muss die logische Beschreibung der gewünschten Daten - Datensatz b in Datei f - in eine physische Beschreibung abgebildet werden wie etwa Block Nr. i auf der Magnetplatte M . Dies geschieht zur Laufzeit des Programms mit Hilfe des Dateisystems (file system) auf das wir in Abschnitt 2.4.2 ausführlicher eingehen werden. Übungsaufgabe 1.19 Warum wird nicht schon beim Compilieren die logische Datenbeschreibung (logische Adresse) durch die physische Beschreibung (physische Adresse) ersetzt? Aufgabe jetzt bearbeiten Im Time-Sharing-Betrieb kann es vorkommen dass viele Prozesse kurz nacheinander auf ein Gerät zugreifen wollen. Deshalb wird zu jedem Speichergerät eine eigene Geräte-Warteschlange (device queue) eingerichtet an die der Ein-/Ausgabeteil des Betriebssystems Aufträge anhängen kann jeder Auftrag wird mit der Nummer des Prozesses versehen der ihn erteilt hat. Der Gerätetreiber holt die Aufträge einzeln aus der Warteschlange ab und führt sie dem Controller zu. Dabei ist der Treiber nicht an die Reihenfolge in der Warteschlange gebunden er kann vielmehr versuchen durch geschickte Wahl des nächsten Auftrags die Zugriffszeit des Geräts zu minimieren wie in Übungsaufgabe 19 besprochen.42 Nun verfolgen wir was bei Ausführung des zu read gehörigen Systemaufrufs in einem Prozess \(P\) geschieht: Eine Software-Unterbrechung wird ausgelöst der Kontext von \(P\) wird zunächst gerettet dann wird der geräteunabhängige Ein-/Ausgabeteil des Betriebssystems aufgerufen. Dieser prüft zunächst ob die Parameter zulässig sind und ob die benötigten Daten schon im Cache im Hauptspeicher stehen in diesem Fall werden sie in den Adressraum von \(P\) kopiert und nach der Rückkehr von der Software-Unterbrechung kann der Prozess \(P\) weiter rechnen. Stehen die Daten nicht im Cache ist ein Plattenzugriff erforderlich. Der Prozess \(P\) wird deshalb in den Zustand blockiert versetzt. Der Leseauftrag wird an die Geräte-Warteschlange des Plattenlaufwerks angehängt und der Gerätetreiber benachrichtigt. Sobald der Gerätetreiberprozess rechnend wird entnimmt er den Auftrag der Geräte-Warteschlange. Der Gerätetreiber reserviert im betriebssystemeigenen Bereich Speicherplatz für die zu lesenden Daten bestimmt die gerätespezifische Datenblockadresse etc. und überträgt einen Lesebefehl an den Gerätecontroller. Der Treiber kann nun andere Aufgaben ausführen er wird später informiert wenn der Lesebefehl ausgeführt worden ist siehe Schritt 6. Der Gerätecontroller bestimmt zuerst die physische Adresse dann führt er den Leseauftrag aus und überträgt zusammen mit dem DMA-Controller die Daten in den reservierten Bereich im systemeigenen Speicher währenddessen können auf der CPU beliebige andere Prozesse rechnen. Danach wird eine DMA-Unterbrechung ausgelöst. Bei der Unterbrechungsverarbeitung wird mittels des Unterbrechungsvektors die DMA-Unterbrechungsroutine gestartet. Sie informiert den Gerätetreiber dass der Lesebefehl ausgeführt worden ist. Sobald der Gerätetreiber diese Information empfängt sieht er in der Gerätewarteschlange nach von welchem Prozess der Leseauftrag zu dieser Information stammt und informiert den geräteunabhängigen Ein-/Ausgabeteil des Betriebssystems. Sobald der geräteunabhängige Ein-/Ausgabeteil des Betriebssystems die Information empfängt kopiert er die Daten aus dem systemeigenen Speicher in den Adressraum des Prozesses \(P\) und versetzt \(P\) in den Zustand bereit. Der Systemaufruf ist damit erfolgreich bearbeitet worden. Sobald der Prozess \(P\) wieder rechnen darf setzt er seine Arbeit hinter dem read fort. An manchen Stellen mag man sich fragen warum der Ablauf gerade so und nicht anders organisiert ist. In der Regel sprechen gute - wenn auch nicht immer zwingende - Gründe für die hier beschriebene Organisation. Beispielsweise werden die gelesenen Daten erst im systemeigenen Speicherbereich abgelegt weil der Platz für die Daten im Adressraum von \(P\) erst in Schritt 8 zugewiesen werden soll oder zwischendurch schon wieder ausgelagert worden sein könnte vergleiche dazu Abschnitt 2.1.  
2; mod_longpage;120; 1; 6; 1.6 Programmierschnittstelle In den vorangegangenen Abschnitten haben wir bei der Rechnerhardware begonnen und uns von unten nach oben (bottom-up) in die Aufgaben und Strukturen eines Betriebssystems eingearbeitet. Nun können wir einen Blick auf ein ganzes System werfen. Abbildung 1.11: Die Struktur des Betriebssystems UNIX. In Abbildung 1.11 ist die Struktur von UNIX skizziert. Der sogenannte Kern (kernel) des Betriebssystems ist hellgrau dargestellt. Er enthält geräteunabhängige Teile für blockweise Ein- und Ausgabe wie sie bei Platten- und Bandgeräten gebräuchlich ist und für zeichenweise Ein- und Ausgabe wie bei Modems und Terminals. Ein weiterer Teil des Kerns ist für die Hauptspeicherverwaltung zuständig. Der CPU-Scheduler gehört zum Kern und in UNIX und Linux auch die Gerätetreiber. Daneben gibt es weitere Teile für die Erledigung anderer Aufgaben. Wir sehen in Abbildung 1.11 dass der Kern zwei Schnittstellen aufweist: Die Verbindung nach unten zur Hardware erfolgt über die Controllerschnittstelle. Die Programmierschnittstelle stellt die Dienste des Betriebssystems nach oben also für die Programme zur Verfügung dies geschieht durch die Gesamtheit der Systemaufrufe (system calls) wie wir sie in Abschnitt 1.17 besprochen haben. Oberhalb der Programmierschnittstelle sind in Abbildung 1.11 die Systemprogramme eingezeichnet. Sie bilden zusammen die Benutzerschnittstelle mit der wir uns in Abschnitt 1.8 befassen werden. In diesem Abschnitt wollen wir eine Übersicht über einige wichtige Dienste geben die als Systemaufrufe angeboten werden.43 Diese Systemaufrufe lassen sich grob folgenden Aufgabenbereichen zuordnen: Prozesse Dateien Information und Kommunikation. Einige Beispiele für Systemaufrufe im Zusammenhang mit Prozessen haben wir bereits kennengelernt: Ein Prozess kann auf ein Ereignis warten (wait event) und wird blockiert bis ein anderer Prozess durch ein Signal mitteilt dass das erwartete Ereignis eingetreten ist (signal event) vergleiche das Ende von Abschnitt 1.20. Ein Prozess muss freiwillig anhalten können (halt) wenn er mit seiner Arbeit fertig ist er wird dadurch in den Zustand beendet versetzt. Wenn ein Fehler auftritt muss der Prozess abgebrochen werden (abort) dabei sollte es möglich sein eine Fehlermeldung auszugeben oder den Inhalt von Registern und Adressraum für die Fehlersuche zu speichern (dump). Ein Prozess sollte ein anderes Programm laden (load) und ausführen (execute) können. So kann ein neuer Prozess entstehen der entweder erst beendet sein muss bevor der Vaterprozess weiterrechnen kann oder neben seinem Vaterprozess existiert hierfür ist oft ein besonderer Systemaufruf Prozess!erzeugen (create process) zuständig.44 Manchmal will der Vaterprozess beim Kindprozess bestimmte Attribute setzen (set attributes) etwa einen Adressraum im Adressraum des Vaters oder eine maximale Rechenzeit. Es kann auch notwendig sein den Kindprozess zu terminieren (terminate). Den Begriff Datei (file) haben wir schon früher verwendet ohne genau zu sagen was damit gemeint ist. Eine Datei ist grob gesagt eine Sammlung zusammengehöriger Information. Dabei kann es sich um ein Programm in Maschinencode handeln ein druckbares PostScript-Dokument eine Videosequenz oder um eine HTML-Seite. Dateien befinden sich oft im Sekundär-Sekundärspeicher und Tertiärspeicher manchmal aber auch im Hauptspeicher (sogenannte RAM-Disk) um den Zugriff zu beschleunigen. Die Magnetplatte eines PC enthält meist sehr viele Dateien schon das Betriebssystem und die Systemprogramme belegen oft mehrere hunderttausend Dateien. Aus der Sicht des Benutzers sind Dateien abstrakte Datentypen (siehe auch Kurse 1661 oder 1663 Datenstrukturen) ähnlich wie virtuelle Geräte vergleiche Abschnitt 1.16. Sie sind in logische Datensätze (records) aufgeteilt die einzelne Informationseinheiten darstellen und oft dieselbe Länge haben. So kann zum Beispiel in einer Personaldatei für jede Mitarbeiterin ein Datensatz vorhanden sein. Es gibt Operationen um Dateien zu erzeugen zu löschen und um einzelne Datensätze zu lesen oder zu schreiben. Dabei kann der Zugriff auf die Datensätze sequentiell oder wahlfrei sein. Manchmal kann man über einen Schlüssel zugreifen: Bei der Personaldatei gibt man dann die Personalnummer vor und erhält den Datensatz des entsprechenden Mitarbeiters.45 Das Dateisystem hat die Aufgabe diese logisch-abstrakte Sicht auf die physische Organisation einer Datei in Blöcke abzubilden vergleiche den Anfang von Abschnitt 1.5 das kann oft dadurch geschehen dass jeweils \(k\) Datensätze gleicher Größe in einen Block geschrieben werden. Man benötigt dazu Systemaufrufe mit denen sich eine Datei physisch anlegen (create) oder löschen (delete) lässt. Vor dem ersten Zugriff muss man eine Datei öffnen (open) nach dem letzten Zugriff schließen (close). Durch das Öffnen wird die Datei in eine Liste der geöffneten Dateien im PCB eingetragen bei den folgenden Zugriffen braucht dann nicht jedesmal aufs neue die physische Adresse der Datei bestimmt zu werden. Außerdem lässt sich damit verhindern dass zwei Benutzer sich beim Zugriff auf dieselbe Datei stören. Es gibt Systemaufrufe zum Lesen (read) und zum Schreiben (write) eines Blocks. Ferner lässt sich eine Datei vor- oder zurück-spulen um auf einen bestimmten Block direkt zugreifen zu können die Vorstellung des Umspulens stammt dabei von Magnetbandgeräten auf denen ursprünglich große Dateien gespeichert wurden. Systemaufrufe zur Beschaffung von Information können vom System die Zeit erfragen oder das Datum. Manche Systeme stellen Aufrufe bereit welche die Nummer der laufenden Betriebssystemversion oder eine Liste mit den Nummern aller vorhandenen Prozesse liefern. Die Kommunikation zwischen Prozessen kann auf zwei verschiedene Arten erfolgen: über den Versand von Nachrichten (message passing) oder durch einen gemeinsamen Speicherbereich (shared memory).46 Beim Versand von Nachrichten gibt es zwei Varianten: Der verbindungslose (connectionless) Datenaustausch entspricht dem Versenden eines Briefs: Man gibt die Adresse des Empfängers und des Absenders an und überlässt die Übermittlung dem System. Ein Beispiel für den verbindungslosen Austausch ganz kurzer Nachrichten sind die Signale die oben schon erwähnt wurden. Für längere Nachrichten gibt es Systemaufrufe zum Senden (send) und Empfangen (receive) die besonders beim Client-Server-Betrieb47 wichtig sind. Beim verbindungsorientierten (connection-oriented) Datenaustausch wird dagegen erst eine Verbindung zwischen Sender und Empfänger aufgebaut über die dann die Kommunikation erfolgt dem entspricht ein Telefongespräch. Hierfür braucht der Sender Systemaufrufe zum Öffnen und Schließen der Verbindung der Empfänger kann schon auf die Verbindung warten (wait) oder sie doch wenigstens akzeptieren (accept). Sobald die Verbindung zustande gekommen ist kann man Nachrichten schreiben und lesen. In UNIX und Linux werden solche Verbindungen durch pipes realisiert. Viel effizienter ist es für die Kommunikation zwischen Prozessen einen gemeinsamen Speicherbereich zu verwenden in den der Sender schreibt und von dem der Empfänger liest denn dann kann man die schnellen CPU-Befehle für den Hauptspeicherzugriff benutzen. Hier treten aber zwei Schwierigkeiten auf: Zum einen achtet das Betriebssystem ja darauf dass verschiedene Prozesse getrennte Adressräume haben siehe Abschnitt 1.19. Hier braucht man Systemaufrufe um (vorübergehend) einen Teil des einen Adressraums in den anderen abzubilden. Zum anderen kann es Probleme geben wenn sich die Prozesse beim Lesen und Schreiben stören. Hier sind Synchronisationsmechanismen erforderlich wie wir sie in Abschnitt 2.3 behandeln werden.  
2; mod_longpage;120; 1; 7; 1.7 Generierung von Prozessen Bei der Diskussion der Programmierschnittstelle eines Betriebssystems in Abschnitt 1.6 hatten wir gesehen dass das Betriebssystem Systemaufrufe zur Erzeugung und zur Beendigung von Prozessen bereitstellen muss. In diesem Abschnitt wollen wir an einem einfachen Beispiel untersuchen wie diese Systemaufrufe in UNIX48 eingesetzt werden. Wir wollen für einen Wiedergabeprozess z.B. einen Audioplayer eine Benutzungsoberfläche schreiben die dem Benutzer erlaubt den Player zu starten und zu stoppen. Aus Gründen der Einfachheit und Systemunabhängigkeit benutzen wir hier einen sehr simplen Player. Es geht in dem Beispiel49 also darum in einem laufenden Prozess namens forkdemo einen Kindprozess beeper zu erzeugen der eine Folge von Pieptönen produziert bis er von seinem Elternprozess forkdemo wieder beendet wird. Wir haben die Programme der Prozesse in C geschrieben weil Linux und UNIX in C und C++ geschrieben sind. /* beeper.c Endlosschleife mit Pieptönen */ /* kompilieren: cc -o beeper beeper.c */ /* starten mit: beeper */ /* beenden mit: kill oder Control-C */ # include int main() while (!0) printf ( Das C-Programm forkdemo für den Elternprozess enthält eine Schleife in der die Eingabe von 1 2 oder 0 angefordert wird Eingabe von 1 startet den Kindprozess Eingabe von 2 beendet ihn wieder und wenn man 0 eingibt werden der Elternprozess und auch der Kindprozess beendet. /* forkdemo.c erzeugt und beendet Kindprozess beeper */ /* kompilieren: cc -o forkdemo forkdemo.c */ /* starten mit: forkdemo */ /* beeper starten lassen mit 1 stoppen mit 2 */ /* Programm beenden mit 0 */ #include #include #include #include int main(int argc char*argv[]) int prozessnr ergebnis piep /* piep = 0 Piepton ist aus */ /* piep = 1 Piepton ist an */ char c piep = 0 prozessnr = -1 /* kein Prozess gestartet */ do printf(1: Piepton starten\n) printf(2: Piepton stoppen\n) printf(0: Programm beenden\n) scanf( printf(\n) printf(Eingegeben ist if (c == 1 & & piep == 0) piep = 1 /* Piepton wird nur einmal gestartet */ prozessnr = fork() /* neuen Prozess erzeugen: */ if (prozessnr == 0) /* fork liefert prozessnr */ /* den Wert 0 im Kindprozess */ /* und die Prozessnummer des */ /* Kindprozesses im Elternprozess */ ergebnis = execvp( beeper argv) /* beeper starten */ exit(0) /* if */ if (c == 2 & & prozessnr != -1) piep = 0 /* Piepton wird abgeschaltet */ ergebnis = kill( prozessnr 9) /* Kindprozess beenden */ /* if */ /* do */ while (c != 0) if ( piep != 0) ergebnis = kill(prozessnr 9) /* nicht vergessen den */ /* Piepton abzuschalten */ /* if */ /* main */ Schauen wir uns die interessanten ersten beiden Fälle näher an. Bei Eingabe von 1 wird ein Kindprozess erzeugt. Hierzu wird der Systemaufruf fork benutzt. Die Funktion fork hat kein Argument. Ihr Aufruf bewirkt dass eine genaue Kopie vom Elternprozess erzeugt und bereit gemacht wird auch die Registerinhalte insbesondere der Inhalt des Befehlszählers werden kopiert. Der Kindprozess konkurriert mit dem Elternprozess um Zuteilung der CPU. Beide führen als nächstes die Anweisung if prozessnr == 0 then aus. Und hier gehen Eltern- und Kindprozess nun getrennte Wege: Denn der vorausgegangene Funktionsaufruf prozessnr = fork() liefert im Elternprozess die Prozessnummer des Kindprozesses zurück im Kindprozess aber den Wert 0! Nur der Kindprozess wird also als nächstes die Anweisung ergebnis = execvp( beeper argv) ausführen. Sie bewirkt dass das gegenwärtige Programm des Kindprozesses das ja mit dem seines Elternprozesses identisch ist durch das Programm in der Datei beeper ersetzt wird und veranlasst die Ausführung dieses Programms vom Programmanfang.50 Jetzt wird eine endlose Folge von Pieptönen erzeugt. In dem Fall dass beim Aufruf von execvp im Kindprozess ein Fehler auftritt -- es könnte ja sein dass gar keine Datei namens beeper im aktuellen Verzeichnis existiert -- erhält die Variable ergebnis einen Wert danach veranlasst die Exit-Anweisung die Beendigung des Kindprozesses. Ohne diese Vorsichtsmaßnahme würde der Kindprozess jetzt dasselbe Programm ausführen wie sein Elternprozess was nur Verwirrung stiften könnte. Mit dem Befehl top der Benutzerschnittstelle von UNIX kann man sich eine Liste aller existierenden Prozesse und ihres Ressourcenverbrauchs ausgeben lassen die ständig aktualisiert wird. Hier ist jetzt neben dem Prozess forkdemo auch sein Kind beeper zu sehen. Beim Elternprozess forkdemo hat die Variable prozessnr als Wert die Prozessnummer von beeper sie ist von null und von der Prozessnummer von forkdemo verschieden so dass es nicht zu Verwechselungen kommen kann. Der Test in der Anweisung if prozessnr == 0 then verläuft für den Elternprozess also negativ. Folglich durchläuft er die Schleife erneut und fordert eine neue Eingabe an. Wir gehen einmal davon aus dass der Benutzer die Pieptöne gerne wieder abstellen möchte und deshalb 2 eingibt. Dann wird der Systemaufruf ergebnis = kill ( prozessnr 9 ) durchgeführt. Er bewirkt dass das Signal mit der Nummer 9 an den Prozess mit der Nummer prozessnr geschickt wird. Dies ist der Kindprozess beeper und Signal Nummer 9 bewirkt seine Beendigung.51 Über Varianten der hier benutzten Systemaufrufe und ihre Funktion kann man sich mit Hilfe des Befehls man informieren. Geben Sie zum Beispiel einmal man fork ein um nachzulesen was ein Kind von seinem Elternprozess erbt.  
2; mod_longpage;120; 1; 8; 1.8 Benutzungsschnittstelle In Abschnitt 1.6 haben wir uns mit der Programmierschnittstelle eines Betriebssystems beschäftigt nun wenden wir uns der Benutzungsschnittstelle zu. Sie dient dem Benutzer zusammen mit den Schnittstellen der Anwendungsprogramme zur Kommunikation mit dem Rechner. Wie angenehm es sich mit einem Rechner arbeiten lässt hängt entscheidend von der Gestaltung der Benutzungsschnittstelle ab. Weit verbreitet sind heute graphische Benutzungsoberflächen mit Fenstern Menüs und Maussteuerung. Während die Programmierschnittstelle durch die Gesamtheit aller Systemaufrufe gegeben ist besteht die Benutzungsschnittstelle aus der Gesamtheit aller Systemprogramme (system programs). In Abbildung 1.11 sieht man am Beispiel von UNIX wie die Systemprogramme über die Programmierschnittstelle auf die Systemaufrufe zugreifen. Auch die Systemprogramme lassen sich grob den folgenden Aufgabenbereichen zuordnen: Programme Dateien und Verzeichnisse Information und Kommunikation. Sie sehen: Diese Liste entspricht der Einteilung der Systemaufrufe in Abschnitt 1.6. Der Unterschied besteht darin dass Systemprogramme oft auf einer höheren Abstraktionsebene angesiedelt sind als Systemaufrufe und dass sie in eine Benutzungsoberfläche integriert sind. Ein Beispiel für eine solche integrierte Oberfläche ist das Common Desktop Environment (CDE)52 . Es vereinigt viele Systemprogramme mit den nachfolgend beschriebenen Funktionen in sich man kann auch CDE selbst als ein großes Systemprogramm auffassen. Gehen wir die einzelnen Bereiche durch. Von Systemprogrammen werden alle Benutzeraktivitäten unterstützt die mit dem Herstellen und Ausführen von Programmen zu tun haben. Dazu gehört ein Editor zum Schreiben der Programmtexte 53 ein Compiler oder Interpreter für die verwendete Programmiersprache ein Binder (linker) der einzelne Module zu einem Lademodul zusammenfasst und schließlich den Lader (loader) der das Lademodul in den Hauptspeicher bringt vergleiche Abschnitt 1.19. Außerdem ist ein Debugger (bug = Wanze) bei der Fehlersuche hilfreich. Systemprogramme helfen dem Benutzer Dateien anzulegen zu kopieren zu drucken umzubenennen und zu löschen. Auch das Setzen der Zugriffsberechtigung d.h. die Festsetzung welcher Benutzer wie auf eine Datei zugreifen darf kann mit einem Systemprogramm erfolgen.54 Zusammengehörige Dateien kann man in Verzeichnissen (directories) zusammenfassen. Ein Verzeichnis kann selbst andere Verzeichnisse enthalten. So lassen sich hierarchische Strukturen aufbauen in denen man sich gut zurechtfindet. Für die Verwaltung von Verzeichnissen gibt es ähnliche Systemprogramme wie für Dateien. Wir werden uns hiermit in Abschnitt 2.4 ausführlicher beschäftigen. Neben Zeit und Datum lassen sich wichtige Systemdaten abfragen wie freier Speicherplatz CPU-Auslastung und eine Liste der anderen Benutzer die zur Zeit am Rechner arbeiten. Während die Systemaufrufe insbesondere mit der Kommunikation zwischen Prozessen zu tun haben sind Systemprogramme auf höherer Ebene für die Kommunikation zwischen Benutzern zuständig diese können an voneinander weit entfernten Rechnern arbeiten. Besonders bekannt sind elektronische Post (electronic mail) Dateitransfer (file transfer z.B. mit dem file transfer protocol) und Benutzerzugriff auf entfernte Rechner (remote login). Hieran sind auch die Rechnernetze beteiligt die die Rechner miteinander verbinden. Näheres hierzu finden Sie in den Kurseinheiten 3 und 4 dieses Kurses. Ein besonders wichtiges Systemprogramm ist der Kommandointerpreter (command interpreter) der die Eingaben des Benutzers entgegennimmt und ihre Ausführung veranlasst. Während die Kommandos früher eingetippt werden mussten kann heute für viele Aufgaben die Maus eingesetzt werden so kann man zum Beispiel eine Datei löschen indem man ihr Bild mit der Maus in einen Papierkorb bewegt. Noch komfortabler ist der Start des zugehörigen Anwendungsprogramms durch Doppelklicken auf eine Datei: so wird bei einer Textdatei das Textprogramm gestartet das zum Typ der Datei passt und bei einer Tondatei ein Abspielprogramm ausführbare Dateien in Maschinencode werden geladen und ausgeführt. Früher waren die Programme zur Ausführung der Benutzerkommandos selbst Teil des Kommandointerpreters wodurch dieser unhandlich und Änderungen mühsam wurden. Heute kann man in UNIX und Linux jedem Benutzer leicht einen maßgeschneiderten Kommandointerpreter - eine sogenannte shell - zur Verfügung stellen. Um ein neues Kommando namens neukommando einzuführen genügt es eine Datei mit dem Namen neukommando anzulegen in der das zugehörige ausführbare Systemprogramm steht und dem System beim Aufruf mitzuteilen in welchem Verzeichnis sich diese Datei befindet.  
2; mod_longpage;120; 1; 9; 1.9 Komplexere Systeme In den vorangegangenen Abschnitten haben wir angenommen dass der Rechner nach dem von-Neumann-Modell aufgebaut ist also nur über eine einzige CPU verfügt siehe Abschnitt 1.2. Am Ende dieser Kurseinheit wollen wir noch einen kurzen Blick auf weitere Rechnerarchitekturen werfen. Parallelrechner Ein naheliegender Gedanke ist es die Bearbeitungszeit für die Lösung komplexer Probleme dadurch zu verkürzen dass man die Arbeit auf mehrere Prozessoren verteilt dieser Ansatz führt zum Konzept des Parallelrechners oft findet sich auch die Bezeichnung Multiprozessorsystem (multiprocessor system). Hier sollte man sich allerdings vor übergroßem Optimismus hüten: Ein Rechner mit \(k\) Prozessoren löst ein Problem nicht unbedingt \(k\) mal so schnell wie ein Einprozessorsystem!55 Denn zum einen setzt das Zusammenwirken mehrerer Prozessoren Kommunikation voraus auch sie benötigt Zeit. Zum anderen ist zunächst zu prüfen ob sich ein gegebenes Problem überhaupt gut parallelisieren lässt. Bei Schachproblemen und dem Brechen von Verschlüsselungscodes geht das zum Beispiel recht gut aber wie steht es mit dem Problem \(n\) Zahlen der Größe nach zu sortieren?56 Neben der möglichen Effizienzsteigerung bieten Parallelrechner zwei weitere Vorteile: Indem man viele CPUs in einem gemeinsamen Gehäuse unterbringt lassen sich Kosten einsparen. Und man gewinnt eine gewisse Sicherheit gegen Störungen wenn nämlich eine CPU ausfällt können die anderen ihre Arbeit mit übernehmen. Man kann sogar einen spontan auftretenden Rechenfehler einer CPU neutralisieren wenn man dieselbe Berechnung zur Kontrolle auch von anderen Prozessoren des Systems durchführen lässt. Solche fehlertoleranten Systeme (fault tolerant systems) werden dort eingesetzt wo es besonders auf Zuverlässigkeit ankommt. Bei einem Parallelrechner teilen sich die Prozessoren einen gemeinsamen großen Hauptspeicher. Der Zugriff erfolgt entweder über einen Bus oder der gemeinsame Hauptspeicher wird in kleinere Stücke zerlegt und ein Schalterwerk sorgt dafür dass jeder Prozessor auf jeden Teil des Speichers zugreifen kann. Verteilte Systeme Bei einem verteilten System hat jeder Prozessor seinen eigenen Speicher auf den nur er selbst zugreifen kann. Die Kommunikation zwischen den Prozessoren kann also nur über den Versand von Nachrichten erfolgen wie in Abschnitt 1.6 beschrieben. Es kann sein dass die Prozessoren im selben Rechnergehäuse untergebracht sind und zwischen einigen von ihnen feste Verbindungen bestehen. Ein verteiltes System kann aber auch aus vielen Ein- oder Mehrprozessorrechnern bestehen die auf der ganzen Welt verteilt sind dies ist zum Beispiel beim World Wide Web der Fall.57 Ein Beispiel für den ersten Fall ist die Hypercube-Architektur. Beim \(d\)-dimensionalen Hypercube gibt es für jeden der \(2^d\) möglichen Vektoren die man aus \(d\) Nullen und Einsen bilden kann einen Prozessor. Nun ist aber nicht jeder Prozessor direkt mit jedem anderen verbunden sondern nur mit den \(d\) Prozessoren deren Vektor sich an genau einer Stelle vom eigenen Vektor unterscheidet. Von \((10011000)\) nach \((10011010)\)gibt es also eine Verbindung nach \((10111010)\) aber nicht. Übungsaufgabe 1.20 Über wie viele Zwischenstationen muss man beim \(d\)-dimensionalen Hypercube höchstens laufen um von einem Prozessor zu einem anderen zu gelangen? Aufgabe jetzt bearbeiten Für verteilte Systeme Software zu entwickeln ist nicht einfach. Ein Grund liegt darin dass viele Prozesse gleichzeitig ablaufen die sich durch Nachrichten gegenseitig beeinflussen können dass sich aber nicht genau planen lässt welcher Prozessor zu welcher Zeit welche Anweisung ausführt. Wer hierüber mehr erfahren möchte sei auf die Thematik Rechnernetze und Verteilte Systeme und die Kurse Betriebssysteme Verteilte Systeme und Sicherheit im Internet verwiesen. Realzeitsysteme Bei unseren bisherigen Betrachtungen war unser Ziel ein Programm korrekt ablaufen zu lassen und dabei die Rechenzeit möglichst gering zu halten. Es gibt aber eine ganze Reihe von Anwendungen bei denen das allein noch keine befriedigende Lösung garantiert. Wenn zum Beispiel die Sensoren eines mobilen Industrieroboters den Kontakt mit einem festen Hindernis melden muss die eingebaute Steuerung einen Haltebefehl auslösen bevor der Roboter die Wand durchbricht. Hier ist es Teil der Korrektheitsanforderung dass das Ergebnis innerhalb einer vorgegebenen Zeit berechnet wird. Mit gängigen Multitaskingsystemen lassen sich solche harten Anforderungen nicht erfüllen man denke nur an die Folgen wenn im kritischen Moment die Prozedur zum Halten des Antriebs erst von der Magnetplatte geladen werden muss oder wenn andere Prozesse mit höherer Priorität die CPU für sich beanspruchen. Aus diesem Grund gibt es spezielle Realzeitsysteme bei denen große Teile der Software in ROMs untergebracht sind. Die Konstruktion von Realzeitsystemen ist ein interessantes Spezialgebiet. Weitere Weblinks zum Thema Betriebssysteme und Linux finden sie unter https://www.fernuni-hagen.de/ks/lehre/kurse/1801.shtml  
2; mod_longpage;154; 2; 0; Kurstext Kurseinheit 2 Rolf Klein Christian Icking Lihong Ma 2 Hauptspeicher und Dateisysteme In der ersten Kurseinheit haben wir uns einen Überblick über verschiedene konkrete Aufgaben eines Betriebssystems verschafft. Jetzt wollen wir auf drei wichtige Bereiche näher eingehen mit denen Anwendungs- und Systemprogrammierer in Berührung kommen: Hauptspeicherverwaltung Synchronisation und Dateisysteme.  
2; mod_longpage;154; 2; 1; 2.1 Hauptspeicherverwaltung In modernen Betriebssystemen sind Prozesse die elementaren Arbeitseinheiten. Eine der Hauptaufgaben eines Betriebssystems besteht darin die existierenden Prozesse quasi-parallel auf der CPU ablaufen zu lassen. Dabei sollte es fair zugehen die Prozesse sollen sich gegenseitig nicht behindern und jeder Prozess soll möglichst schnell bearbeitet werden. Damit der Prozesswechsel ohne Schwierigkeiten funktioniert werden für jeden bereiten oder blockierten Prozess die Inhalte des Befehlszählers und der übrigen Register zum Zeitpunkt der letzten Unterbrechung im Prozesskontrollblock gespeichert vergleiche Abschnitt 1.4.1. Man muss aber auch sicherstellen dass die Inhalte der Hauptspeicherbereiche erhalten bleiben die den existierenden Prozessen zugewiesen sind. Hier stellt sich die Frage wie diese Speicherplatzzuweisung überhaupt erfolgt. Darauf wollen wir jetzt eingehen. In Abschnitt 1.3.6 sind wir davon ausgegangen dass das Betriebssystem jedem Prozess einen zusammenhängenden Hauptspeicherbereich zuweist der als physischer Adressraum bezeichnet wird. Eine physische Adresse zeigt auf eine aktuelle Speicherstelle im Hauptspeicher. Der Prozess selbst braucht die physischen Hardwareadressen nicht zu kennen er kann logische Adressen verwenden. Eine logische Adresse ist ein Verweis auf eine Speicherstelle die unabhängig vom physischen Hauptspeicher ist zum Beispiel die relative Position eines Befehls in einem Programmstück. Ein Programm erzeugt eine Menge von logischen Adressen die zusammen den logischen Adressraum des Prozesses bilden. Bevor ein Zugriff auf den Hauptspeicher stattfindet muss eine logische Adresse auf eine physische abgebildet werden. Die Abbildung der logischen auf die physischen Adressen ist die Aufgabe des Betriebssystems. Sie wird oft mit der Hilfe einer Hardware der MMU (Memory Management Unit) die eine logische Adresse von der CPU erhält und eine physische Adresse ausgibt erledigt. Zusammenhängende Hauptspeicherzuweisungen Jeder Prozess soll ein Stück vom Hauptspeicher zugewiesen bekommen. Die einfachste Zuweisungsstrategie ist dass jeder einen zusammenhängenden Bereich im Hauptspeicher erhält. Es gibt zwei Möglichkeiten: Der Hauptspeicher wird fest in zusammenhängende Bereiche unterschiedlicher Größe aufgeteilt und jeder Prozess bekommt einen davon dessen Größe er mindestens benötigt. Der vom Prozess nicht benutzte Teil des Bereiches ist die sogenannte interne Fragmentierung. Jeder Prozess bekommt einen zusammenhängenden Bereich genau der Größe die er auch angefordert hat. In beiden Fällen ist die Abbildung einer logischen Adresse auf die physische sehr einfach. Dafür brauchen wir zwei speziellen CPU-Register: Basisregister: Zur Speicherung der Startadresse eines Adressraums. Grenzregister: Zur Speicherung der Länge des logischen Adressraums. Die physische Adresse zu einer logischen Adresse ergibt sich durch Addition der Startadresse. Wir sprechen hier auch von relativer Adressierung (relativer zum Basis-Register). Abbildung 2.1 zeigt ein Beispiel zur Umrechnung der Adressen. Abbildung 2.1: Die Abbildung logischer Adressen auf physische Adressen bei zusammenhängenden Adressräumen. Vor der Addition testet die MMU ob die logische Adresse kleiner als die im Grenzregister ist. Falls nein erzeugt die MMU eine Unterbrechung so wird der Speicherschutz gewährleistet. Durch die beiden Register wird gleichzeitig erreicht dass die Abbildung auf die physischen Adressen flexibel ist das heißt dass Programme relokierbar bleiben. Sie können zur Laufzeit in verschiedene Bereiche des Hauptspeichers geladen werden. Wenn viele Prozesse gleichzeitig existieren kann es vorkommen dass der Hauptspeicher zu klein wird um alle physischen Adressräume aufzunehmen. In diesem Fall werden einige bereite oder blockierte Prozesse durch ein Verschieben ihres Adressraums in den Sekundärspeicher ausgelagert. Die Entscheidung darüber welche Prozesse ausgelagert werden trifft ein Langzeit-Scheduler.1 Bevor ein ausgelagerter Prozess weiter rechnen kann muss er erst wieder eingelagert werden der Vorgang des Aus- und Einlagerns wird im Englischen als swapping bezeichnet. Beim Einlagern ist es dank der Relokierbarkeit nicht notwendig den Prozess an exakt dieselbe Stelle im Hauptspeicher zurückzuschreiben an der er vorher gestanden hat. Weil sich die Menge der existierenden Prozesse ständig ändert entstehen zwischen den physischen Adressräumen der Prozesse im Hauptspeicher zwangsläufig Lücken dieses Phänomen nennt man externe Fragmentierung. Es kann sein dass keine der vorhandenen Lücken ausreicht um den physischen Adressraum eines neu erzeugten Prozesses aufzunehmen. Dann müsste ein Prozess ausgelagert werden obwohl insgesamt noch ausreichend Hauptspeicher frei ist - ein wenig effizientes Vorgehen. Zur Vermeidung dieses Problems kann man verschiedene Strategien anwenden. Eine Möglichkeit besteht darin die im Hauptspeicher eingelagerten Prozesse hin und wieder zusammenzuschieben damit aus vielen kleinen Lücken eine große wird. Eine solche Kompaktifizierung ist aber mit einem hohen Aufwand verbunden. Man gewinnt mehr Flexibilität in der Speichervergabe wenn man die Forderung aufgibt dass jeder Prozess ein zusammenhängendes Stück vom Hauptspeicher bekommen soll. Stattdessen kann man zum Beispiel jedem Prozess mehrere zusammenhängende Segmente im Hauptspeicher zuweisen die unterschiedlich lang sein dürfen. Dieser im Englischen als segmentation bezeichnete Ansatz ist auch aus der Sicht des Programmierers sinnvoll: Man kann verschiedene Programmmodule in unterschiedlichen Segmenten unterbringen und zum Beispiel ein weiteres Segment für die Daten verwenden. Die Adressierung innerhalb eines Segments erfolgt analog zu Abbildung 2.1 zur Bezeichnung des Segments wird zusätzlich eine Segmentnummer angegeben. Paging Ganz beseitigt wird das Problem der externen Fragmentierung wenn man den Hauptspeicher in sehr viele kleine Stücke gleicher Größe aufteilt und jedem Prozess die erforderliche Anzahl solcher Stücke zuweist diese Stücke brauchen dabei im Hauptspeicher nicht hintereinander zu liegen. Dieser Ansatz heißt auf Englisch paging. Man teilt den logischen Speicher2 in gleichgroße Stücke auf die Seiten (pages) genannt werden. Der physische Speicher wird in Seitenrahmen (frames)3 aufgeteilt. Eine Seite passt genau in einen Seitenrahmen. Die Seitentabelle (page table) legt fest welche Seite in welchem Seitenrahmen steht und liefert damit eine Abbildung vom logischen auf den physischen Speicher siehe Abbildung 2.2. Abbildung 2.2: Die Seitentabelle bildet Seiten auf Seitenrahmen ab. Wie funktioniert aber bei Paging die Umrechnung einer logischen Adresse in die physische? Abbildung 2.3: Ein logischer Adressraum wird in 8 Seiten der Größe 128 Byte aufgeteilt. Die ersten drei Bits einer Adresse stellen die Seitennummer der Adresse dar. Die restlichen 7 Bits sind der Offset die Position der Adresse innerhalb der Seite. Als Beispiel betrachten wir einen logischen Adressraum der Größe \(2^{10}=1024\) Byte mit einer Seitengröße \(128\) Byte siehe Abbildung 2.3. Hier zeigt jede logische Adresse auf ein Byte. Also muss es insgesamt \(1024\) Adressen geben von \(0\) bis \(1023\). Um die 1024 Adressen darzustellen benötigen wir \(10\) Bit. Also ist jede Adresse des logischen Adressraums eine binäre Zahl der Länge 10 Bit. Legen wir nun die Seitengröße mit \(128\) Byte fest dann hat der logische Adressraum in unserem kleinen Beispiel insgesamt \( \frac{1024 Byte}{128 Byte}=\frac{2^{10}}{2^7}=2^{3}=8 \) Seiten. Man kann nun die Anzahl der logischen Adressen \( 1024=8 \cdot 128 \quad oder \quad 2^{10}=2^{3} 2^{7} \) auch als Produkt der Anzahl der Seiten und der Seitengröße schreiben. Daraus ergibt sich die die Idee der für das Paging typischen Aufteilung von logischen Adressen in zwei Abschnitte: Der vordere Teil hier die ersten 3 Bit stellt die Seitennummer und der hintere Teil hier die letzten 7 Bit den Offset dar. Rechnen wir nun konkrete Beispiele durch. Wenn wir wissen wollen in welcher Seite z.B. die logische Adresse \(254\) liegt dann berechnen wir die Division mit Rest von \(254\) durch \(128\) und erhalten den Quotient \(1\) mit Rest \(126\) da \( 254 = 1 \cdot 128 + 126 \) gilt. Dies bedeutet dass die logische Adresse \(254\) in der Seite mit Nummer 1 liegt und innerhalb der Seite die relative Position \(126\) hat. Übungsaufgabe 2.1 Ein Prozess benötigt zur Laufzeit einen logischen Speicherbereich von 8000 Bytes. Wieviele Seiten muss das Betriebssystem diesem Prozess zuteilen wenn die Seitenrahmengröße 1024 Byte beträgt? Wie groß ist die interne Fragmentierung bei dieser Zuweisung? Aufgabe jetzt bearbeiten Die MMU hat es aber noch einfacher als wir die lieber im Dezimalsystem rechnen. Sie bekommt die binäre Zahl \(0011111110\) und interpretiert die ersten drei Bits \(001\) als die Seitennummer 1 und die letzten 7 Bits \(1111110\) als den Offset \(126\) die Division mit Rest ergibt sich mühelos durch die Aufteilung der Bits an der richtigen Stelle. Da eine Seite genau so groß wie ein Seitenrahmen ist bleibt der Offset einer logischen Adresse gleich dem physischen Offset im Seitenrahmen. Die Konsequenz ist dass ein Eintrag in der Seitentabelle einfach nur die Nummer des Seitenrahmens zu sein braucht in dem die Seite auch steht. Wir fassen die Erkenntnisse aus diesem Beispiel zusammen: Die Bits einer logischen Adresse setzen sich aus Seitennummer und Offset zusammen. Der Offset ist die Position des Bytes innerhalb einer Seite und eines Seitenrahmens. Der Offset in einer logischen Adresse ist derselbe wie in der zugehörigen physischen Adresse. Ein Index in die Seitentabelle ist eine Seitennummer und ein Eintrag in der Seitentabelle ist eine Seitenrahmennummer. Die Bits einer physischen Adresse werden aus Seitenrahmennummer und Offset zusammengesetzt. Für die Umrechnung von logischen in physische Adressen braucht die MMU nur die ersten Bits der Adresse als Seitennummer zu entnehmen und sie als Index in der Seitentabelle zu verwenden. Dort steht die zugehörige Seitenrahmennummer die nur noch mit dem Offset zusammengesetzt werden muss um die physische Adresse zu erhalten. Wir betrachten als Beispiel wie die MMU die logische Adresse \( 254=0011111110 \) auf die physische Adresse abbildet siehe dazu Abbildung 2.7. Abbildung 2.7: Adressberechnung beim Paging. Die MMU erhält die Binärzahl \(0011111110\) entnimmt die ersten drei Bits \(001\) sucht in der Seitentabelle an der Stelle \(001\) nach der Seitenrahmennummer und erhält die Zahl \(011\). Nun setzt die MMU sie mit dem Offset \(1111110\) zusammen und erhält die physische Adresse \(0111111110\) das ist umgerechnet \( 0111111110= 3 \cdot 128 + 126 = 510. \) Das Betriebssystem legt die maximale Größe eines logischen Adressraums fest unabhängig von einzelnen Prozessen. Es legt auch die Seitengröße fest und damit auch wie viele Seiten der logische Adressraum eines Prozesses maximal haben kann. Dadurch liegt auch die Anzahl der Bits für die Seitennummer und für den Offset fest. Nun konfiguriert das Betriebssystem die MMU so dass sie immer die gleiche Anzahl von ersten Bits von einer logischen Adresse als Seitennummer entnimmt und sie als Index für die Suche nach der Seitenrahmennummer in der Seitentabelle verwendet. Die Seitentabelle ist Teil des Prozesskontextes. Weil jeder Hauptspeicherzugriff zunächst einen Zugriff auf die Seitentabelle auslöst kommt es hier auf Effizienz an. Deshalb werden einige Einträge einer Seitentabelle die momentan verwendet werden oft in einem schnellen Speicher auf dem CPU-Chip bzw. in der MMU gehalten. Übungsaufgabe 2.2 Warum darf ein Prozess nicht auf seine Seitentabelle zugreifen? Aufgabe jetzt bearbeiten Durch die Verwendung von Seiten kann zwar keine externe Fragmentierung auftreten weil aber immer nur ganze Seiten zugeteilt werden lässt sich eine gewisse interne Fragmentierung nicht vermeiden. Zum Beispiel könnte ein Prozess eigentlich nur \(3 1\) Seiten benötigen und von den vier zugeteilten Seiten würde die letzte zu 90 Prozent ungenutzt bleiben. Dies stellt aber kein Problem dar bei aktuellen Systemen mit Hunderten von Prozessen und Millionen von Seiten. Übungsaufgabe 2.3 Welche Auswirkungen hat es wenn man die Seiten sehr groß oder sehr klein auslegt? Aufgabe jetzt bearbeiten Übungsaufgabe 2.4 Da die Seitentabellen oft recht groß werden schnelle Hardwareregister jedoch teuer sind hält man oft die gesamte Seitentabelle im Hauptspeicher und sieht im Prozessor einen Registersatz als Cache-Speicher für einige Einträge der Seitentabelle vor. Angenommen die Zugriffszeiten für einen solchen Cache beträgt 20 ns und für den Hauptspeicher 100 ns. Um wieviel Prozent steigt die durchschnittliche Zeit für einen lesenden und schreibenden Zugriff gegenüber einem System ohne Paging wenn die Trefferrate für den Cache 80 Prozent (98 Prozent) beträgt? Aufgabe jetzt bearbeiten Um Prozessen große Hauptspeicherbereiche zuweisen zu können braucht das Betriebssystem effiziente Mechanismen zur Verwaltung der belegten und freien Seiten dabei kann auch eine höhere interne Fragmentierung in Kauf genommen werden. Als Beispiel für eine Zuweisungstrategie betrachten wir ein Verfahren in Linux: Für den physischen Speicher gibt es in Linux einen Seitenallokierer der Bereiche von aufeinander folgenden Seitenrahmen bereitstellen kann. Dabei bedient er sich der sogenannten Buddy-Strategie: Der Hauptspeicher besteht aus zusammenhängenden Stücken die jeweils eine Zweierpotenz viele Seiten enthalten. Ein Stück ist entweder belegt oder frei. Wenn der Allokierer einen zusammenhängenden Bereich einer bestimmten Länge benötigt nimmt er das kleinste freie Stück das mindestens die erforderliche Länge aufweist. Wenn es mehr als doppelt so lang ist wie der benötigte Bereich so wird es halbiert das eine halbe Stück wird benutzt das andere - sein Buddy4 - bleibt frei. Wann immer ein Stück frei wird und sein Buddy bereits frei ist werden sie wieder verschmolzen. Dieses Verfahren ist recht einfach zu implementieren kann aber zu erhöhter interner Fragmentierung führen denn wenn ein Prozess 33 Seiten benötigt belegt er schon 64. Bei Verwendung von mehreren Segmenten oder Seiten je Prozess kann man neben den privaten Speicherbereichen auf die nur der Prozess selbst zugreifen darf auch öffentliche Bereiche einrichten die von mehreren Prozessen gemeinsam benutzt werden können: Es genügt ja bei jedem Segment oder jeder Seite zu vermerken welche Prozesse Schreib- oder Leserecht daran haben. So kann zum Beispiel das Code-Segment eines Pascal-Compilers von mehreren Anwendern gleichzeitig benutzt werden ohne dass jeder Prozess eine eigene Kopie des Compilerprogramms benötigt. Natürlich benötigt jeder Prozess sein eigenes Datensegment und Stacksegment. Außerdem kann man gemeinsame Speicherbereiche für die Kommunikation zwischen Prozessen benutzen vergleiche den Schluss von Abschnitt 1.6. Hierauf gehen wir in Abschnitt 2.3 näher ein. Virtueller Hauptspeicher Am Ende dieses Abschnitts wollen wir die Technik des virtuellen Speichers (virtual memory) zur Hauptspeicherverwaltung besprechen die heute große Bedeutung erlangt hat und das in praktisch allen modernen Betriebssystemen implementiert ist. Es kombiniert zwei Ansätze die wir oben vorgestellt haben: die Einteilung des physischen Speichers in Seitenrahmen und die Idee nur die Informationen im Hauptspeicher zu halten die gerade benötigt werden. Für die Programmierer ist die Verwendung virtuellen Speichers sehr angenehm: Sie können große logische Adressräume verwenden ohne auf mögliche physische Grenzen achten zu müssen. Übungsaufgabe 2.5 Wodurch ist die maximale Größe des logischen Adressraums begrenzt? Aufgabe jetzt bearbeiten Ein bereiter Prozess wird rechnend gemacht auch wenn nicht alle seine Seiten in Seitenrahmen des Hauptspeichers stehen die fehlenden Seiten stehen im Sekundärspeicher und sind in der Seitentabelle entsprechend markiert. Das Betriebssystem merkt sich wo die fehlenden Seiten im Sekundärspeicher stehen. Ein Zugriff auf eine Seite die nicht im Hauptspeicher steht wird als Seitenfehler bezeichnet. Wie kann die MMU wissen ob eine Seite im Hauptspeicher vorhanden ist wenn sie in der Seitentabelle nach der Seitenrahmennummer sucht? Dazu wird in der Seitentabelle für jeden Eintrag zusätzlich ein present-Bit verwaltet das zeigt ob die Seite im Hauptspeicher vorliegt. Im Fall dass die Seite nicht im Hauptspeicher vorhanden ist löst die MMU eine Software-Unterbrechung (trap) aus da die Software auf eine derzeit nicht verfügbare Seite zugreifen will vergleiche Abschnitt 1.3.4. Die fehlende Seite wird von der Festplatte gelesen danach kann der Prozess weiterrechnen. Diese Technik wird als demand paging bezeichnet bei der eine Seite erst in den Hauptspeicher eingelagert wird wenn sie auch gebraucht wird. Es kann beim Einlagern einer Seite vorkommen dass kein Seitenrahmen mehr frei ist. Dann muss eine andere Seite in den Sekundärspeicher ausgelagert werden. Für die Wahl der auszulagernden Seite gibt es verschiedene Strategien hier liegt dieselbe Situation vor wie beim Hauptspeicher-Cache den wir in Abschnitt 1.2.2 betrachtet hatten. Die optimale Strategie lagert diejenige Seite aus die erst am weitesten in der Zukunft wieder benötigt wird um künftige Seitenfehler möglichst zu vermeiden. Leider steht diese Information in der Regel nicht zur Verfügung.5 Man muss sich daher mit sub-optimalen Strategien begnügen. Die Strategie LRU (least recently used) lagert die Seite aus deren letzte Benutzung am weitesten zurückliegt. LRU ist eine Annäherung der optimalen Strategie. Es soll möglichst eine Seite ausgelagert werden die seit der letzten Einlagerung nicht mehr verändert wurde. Der Vorteil in diesem Fall ist dass die Seite nicht mehr auf den Sekundärspeicher zurückgeschrieben zu werden braucht. Dazu wird ein zusätzliches dirty-Bit zu jeder Seite in der Seitentabelle verwaltet das bei jeder Schreiboperation gesetzt wird. Dadurch kann festgestellt werden ob die Seite verändert wurde. Übungsaufgabe 2.6 Welche Informationen benötigt das Betriebssystem z.B. in der Seitentabelle um virtuellen Hauptspeicher zu realisieren? Aufgabe jetzt bearbeiten  
2; mod_longpage;154; 2; 2; 2.2 Leichtgewichtige Prozesse: Threads Zum Kontext eines Prozesses - das hatten wir in Abschnitt 2.1 gesehen - gehören neben den Registerinhalten auch Informationen über seinen Adressraum bei seitenbasierter Speicherorganisation zum Beispiel eine Seitentabelle vergleiche Abbildung 2.2. Die Verwaltung dieser Informationen kostet beim Prozesswechsel Rechenzeit. Nun gibt es Probleme bei deren Lösung man mehrere Prozesse einsetzen möchte die quasi-parallel ablaufen und dabei alle auf denselben Speicherbereich zugreifen. Eine zeitaufwendige Einrichtung neuer Adressräume ist also beim Wechsel zwischen diesen Prozessen nicht erforderlich. Für solche Fälle hat man das Konzept der leichtgewichtigen Prozesse (Threads = Fäden)6 entwickelt das in diesem Abschnitt vorgestellt wird. Betrachten wir zunächst ein Beispiel. In Rechnernetzen werden oft dedizierte Dateiserver (file server) verwendet Rechner also die darauf spezialisiert sind auf Magnetplatten gespeicherte Dateien zu bearbeiten. Von jedem Rechner im Netz können Klienten Aufträge an den Server übermitteln die dieser ausführt und beantwortet. Weil der Dateiserver nur diese eine Aufgabe wahrnimmt könnte man meinen dass er im wesentlichen mit einem einzigen Anwendungsprozess auskommt. Dieser Prozess schaut in einem Briefkasten (mail box) nach ob ein Auftrag \(A_i\) vorliegt. Wenn das so ist wird ihm die logische Dateiadresse entnommen und es wird zunächst geprüft ob die entsprechenden Daten im Hauptspeicher-Cache des Servers stehen. Ist das nicht der Fall wird die physische Adresse der Daten auf der Festplatte berechnet. Dann erteilt der Prozess einen Befehl an den Gerätetreiber. Bei diesem Ansatz gibt es zwei Möglichkeiten: Der Serverprozess könnte jetzt blockieren bis Gerätetreiber und Controller den Befehl ausgeführt haben siehe Abschnitt 1.5. Dann könnte er die Antwort an den Klienten formulieren und abschicken. Dieses Verfahren ist recht einfach aber sehr ineffizient denn während der Serverprozess blockiert ist wäre die CPU des Dateiservers die ganze Zeit über untätig. Oder aber der Serverprozess könnte eine Notiz über den Zustand der Bearbeitung von Auftrag \(A_i\) ablegen und schon einmal den nächsten Auftrag aus dem Briefkasten holen. Wenn dann später der Gerätetreiber den zu \(A_i\) gehörenden Befehl ausgeführt hat könnte der Serverprozess mit der Bearbeitung des aktuellen Auftrags \(A_{i+j}\) innehalten und zunächst dem Klienten von Auftrag \(A_i\) seine Antwort schicken. Dieses verschachtelte Vorgehen versucht mit einem einzelnen Prozess Parallelität nachzumachen das ist zwar effizient aber nicht so leicht zu programmieren und sehr unübersichtlich. Viel eleganter ist die Verwendung mehrerer Serverprozesse von denen jeder einen kompletten Auftrag sequentiell ausführt. Während einer von ihnen blockiert weil er auf die Erledigung eines Ein-/Ausgabebefehls wartet braucht die CPU nicht untätig zu sein denn inzwischen kann ja ein anderer Serverprozess rechnen. Diese Serverprozesse greifen alle auf dieselben Hauptspeicherbereiche zu: auf den Briefkasten und den Hauptspeicher-Cache. Es besteht also kein Grund ihnen individuelle Adressräume zuzuweisen die den Prozesswechsel verlangsamen. Deshalb verwendet man zur Steuerung eines Dateiservers am besten leichtgewichtige Prozesse (Threads). Mehrere Threads teilen sich ein Programm einen Adressraum und dieselben Dateien. Jeder Thread hat aber seine eigenen Registerinhalte -- insbesondere seinen eigenen Befehlszähler -- und einen eigenen Stapel (Stacksegment). Solch eine Gruppe von zusammengehörigen leichtgewichtigen Prozessen wird als Task (Aufgabe) bezeichnet siehe Abbildung 2.4.7 Abbildung 2.4: Drei Threads eines Tasks bei der quasi-parallelen Ausführung desselben Programmstücks Codesegment Datensegment und Dateien werden gemeinsam genutzt. Leichtgewichtige Prozesse können dieselben Zustände annehmen wie gewöhnliche Prozesse sie können Kinder generieren und blockierende Systemaufrufe ausführen. Beim Zugriff auf den gemeinsamen Speicherbereich kann es zu den Problemen kommen die wir in Abschnitt 2.3 beschreiben werden so dass Synchronisationsmechanismen benötigt werden. Dabei kann man allerdings davon ausgehen dass sich zusammengehörende leichtgewichtige Prozesse kooperativ verhalten. Übungsaufgabe 2.7 Schlagen Sie eine Anwendung vor die von der Benutzung von Threads profitiert und eine bei der dies nicht der Fall ist. Aufgabe jetzt bearbeiten Es gibt verschiedene Möglichkeiten leichtgewichtige Prozesse zu implementieren: Kernel-Threads und Benutzer-Threads. Kernel-Threads werden im Betriebssystemkern realisiert Benutzer-Threads hingegen im privaten Speicherbereich eines Prozesses. Dazu kommen noch Mischformen aus beiden Implementierungen. Bei einer Implementierung als Kernel-Threads im Betriebssystemkern werden die leichtgewichtigen Prozesse genau wie die schwergewichtigen behandelt insbesondere wird jeder Prozesswechsel und das Scheduling vom Kern ausgeführt wie in Abschnitt 1.4.1 beschrieben wurde. Diese Situation liegt zum Beispiel in Linux vor. Bei der Generierung eines leichtgewichtigen Prozesses verwendet man nicht den Befehl fork wie wir ihn in Abschnitt 1.7 kennengelernt haben sondern clone. Hierdurch wird ein Kindprozess erzeugt der nicht nur sein Programm sondern auch seinen Speicherbereich vom Erzeugerprozess erbt. Benutzer-Threads hingegen werden mit Hilfe von Bibliotheksprozeduren auf Benutzerebene implementiert. Wann immer ein Thread einen Systemaufruf ausführen möchte -- zum Beispiel eine Semaphoroperation -- so ruft er stattdessen eine Bibliotheksprozedur auf. Sie entscheidet ob der Thread suspendiert werden muss. Wenn das der Fall ist vertauscht die Bibliotheksprozedur die aktuellen Registerinhalte mit denen eines anderen bereiten Threads ohne dass der Betriebssystemkern involviert wird das Betriebssystem weiß also gar nichts von der Existenz der Threads und behandelt den gesamten Task wie einen einzigen schwergewichtigen Prozess. So ergeben sich sehr kurze Umschaltzeiten beim Wechsel zwischen den leichtgewichtigen Prozessen. Außerdem lässt sich das Scheduling der Threads bei diesem Ansatz vom Anwendungsprogrammierer steuern. Diesen Vorteilen steht aber ein gravierender Nachteil gegenüber: Wenn ein leichtgewichtiger Prozess eines Tasks einen blockierenden Systemaufruf durchführt wird der gesamte Task blockiert. Bei einer Implementierung leichtgewichtiger Prozesse im Betriebssystemkern könnte dagegen jetzt ein anderer Thread desselben Tasks rechnend gemacht werden. Übungsaufgabe 2.8 Welche Art der Implementierung leichtgewichtiger Prozesse ist beim Dateiserver vorzuziehen? Aufgabe jetzt bearbeiten Auch die nächste Aufgabe beschäftigt sich mit den Unterschieden bei der Implementierung leichtgewichtiger Prozesse. Übungsaufgabe 2.9 Angenommen Task \(T_1\) enthält nur einen leichtgewichtige Prozess und Task \(T_{100}\) enthält 100 Threads. Wieviel CPU-Zeit entfällt bei den beiden Implementierungsarten auf diese Tasks wenn wir Round-Robin-Scheduling voraussetzen vergleiche Abschnitt 1.4.1? Aufgabe jetzt bearbeiten  
2; mod_longpage;154; 2; 3; 2.3 Prozesssynchronisation Aus Abschnitt 2.2 wissen wir dass die Threads eines Prozesses das gemeinsame Datensegment benutzen können um miteinander zu kommunizieren. Beispielsweise können sich Threads dort Nachrichten schreiben und lesen und so miteinander kommunizieren. Angenommen in einem Rechnersystem finden in unregelmäßigen Abständen Ereignisse statt über deren Häufigkeit Buch geführt werden soll. Ein Thread namens Beobachter zählt immer dann eine Variable Zähler hoch wenn ein Ereignis stattgefunden hat. Ein zweiter Thread namens Berichterstatter druckt hin und wieder den Zählerstand aus und setzt die Variable auf Null zurück. Beide Threads existieren für immer. Thread Beobachter begin repeat beobachte Ereignis Zähler := Zähler + 1 until false end Thread Berichterstatter begin repeat print(Zähler) Zähler := 0 until false end Die Variable Zähler liegt in dem gemeinsamen Datensegment der Threads auf den sie lesend und schreibend zugreifen können vergleiche Abschnitt 2.1. Sie wird anfangs auf Null gesetzt. Man sollte meinen dass zu jedem Zeitpunkt die Summe aller ausgedruckten Zählerstände zusammen mit dem aktuellen Wert von Zähler die Anzahl aller Ereignisse angibt die bisher stattgefunden haben. Das stimmt aber nicht! Denn die beiden Threads laufen (quasi-)parallel ab und dadurch kann es zum Beispiel zu folgender Ausführungsreihenfolge der Anweisungen kommen: Wenn der Berichterstatter gerade den aktuellen Zählerstand ausgedruckt hat und in diesem Moment wegen Ablauf seiner Zeitscheibe unterbrochen wird könnte der Beobachter rechnend werden. Angenommen jetzt treten viele Ereignisse ein 8 und die Variable Zähler wird entsprechend hochgezählt. Sobald der Berichterstatter wieder rechnend wird setzt er als erstes den Zähler auf Null und alle soeben aufgetretenen Ereignisse sind verloren. Dieser Fehler hängt nicht davon ab dass wir nur über eine CPU verfügen er kann ebenso auftreten wenn die Threads auf verschiedenen Prozessoren mit unbekannten Geschwindigkeiten ablaufen. Hier liegen sogenannte Wettkampfbedingungen (race conditions) vor welcher Thread als erster ein bestimmtes Ziel erreicht ist nicht vorhersehbar. Die Fehlerursache liegt vielmehr darin dass die beiden Anweisungen print(Zähler) Zähler := 0 im Programm des Berichterstatters einen kritischen Abschnitt (critical section) bilden der keine Unterbrechung durch den Beobachter verträgt. Ein kritischer Abschnitt ist ein Abschnitt im Programm in dem gemeinsame Ressource wie z.B. Variable und Datenstrukturen benutzt werden auf die mehrere Threads oder Prozesse lesend und schreibend zugreifen so dass eine race condition entstehen kann. Übungsaufgabe 2.10 Gibt es auch im Programm des Beobachters einen kritischen Abschnitt? Aufgabe jetzt bearbeiten Offenbar muss man die beiden Threads synchronisieren um zu verhindern dass beide zur gleichen Zeit in ihren kritischen Abschnitt eintreten. Also soll die Synchronisation den exklusiven Zugriff auf den kritischen Abschnitt garantieren. Eine naheliegende Möglichkeit besteht darin eine Synchronisationsvariable namens switch zu verwenden die wie ein Schalter den Zugang zu den kritischen Abschnitten regelt. Hat sie den Wert 0 so ist der Beobachter an der Reihe beim Wert 1 darf der Berichterstatter rechnen: Prozess Beobachter begin repeat while switch = 1do no-op (* Beginn kritischer Abschnitt *)\+ beobachte Ereignis Zähler := Zähler + 1 (* Ende kritischer Abschnitt *)\+[1ex] switch := 1 until false end Prozess Berichterstatter begin repeat while switch = 0do no-op (* Beginn kritischer Abschnitt *)\+ print(Zähler) Zähler := 0 (* Ende kritischer Abschnitt *)\+[1ex] switch := 0 until false end Dabei steht no-op für no operation der Thread Beobachter bleibt also in der while-Schleife und tut nichts bis die Bedingung falsch wird das heißt bis switch den Wert 0 erhält. Dann führt er seinen kritischen Abschnitt aus und setzt switch auf 1 damit der Berichterstatter in seinen kritischen Abschnitt eintreten kann. Man sieht schnell dass diese Lösung zwei Nachteile aufweist: Beide Threads verbrauchen wertvolle CPU-Zeit während sie in ihren while-Schleifen warten. Ein solches geschäftiges Warten (busy waiting) ist unerwünscht. Die beiden Threads können nur abwechselnd in ihre kritischen Abschnitte eintreten. Wenn einer von ihnen innerhalb des kritischen Abschnitts beendet werden sollte kann der andere nie wieder den kritischen Abschnitt betreten. Diese Schwierigkeiten lassen sich vermeiden wenn man das Konzept des Semaphors9 verwendet. Es wurde von Dijkstra10 zur Lösung von Problemen entwickelt bei denen mehrere Prozesse oder Threads ein Betriebsmittel belegen wollen von dem insgesamt \(n\) Stück zur Verfügung stehen. Dabei kann es sich um \(n\) freie Speicherplätze handeln um \(n\) CPUs oder um das Recht in den kritischen Abschnitt eintreten zu dürfen. Im letzten Fall ist \(n=1\) weil ja zu einem Zeitpunkt immer nur ein Prozess seinen kritischen Abschnitt betreten darf. Ein Semaphor S kann als abstrakter Datentyp11 spezifiziert werden. Der Zustand von S besteht aus der Anzahl freier Betriebsmittel gespeichert in einer Zählvariablen count und einer Prozessmenge W. Falls count � 0 so ist W leer ansonsten enthält W alle Prozesse die sich bisher vergeblich um ein Betriebsmittel bemüht haben und darauf warten dass wieder eines frei wird. Auf S sind zwei Operationen definiert down und up. Wenn ein Prozess ein Betriebsmittel benutzen will ruft er die Operation down auf. Procedure down(S) begin if S.count \(> \) 0 then S.count := S.count \(-\) 1 (* aufrufender Prozess kann Betriebsmittel benutzen *) else (* alle Betriebsmittel belegt *) begin füge aufrufenden Prozess in die Menge S.W ein blockiere aufrufenden Prozess end end Wenn ein Prozess sein Betriebsmittel wieder freigibt ruft er die Operation up auf. Sie bewirkt folgendes: Procedure up(S) begin if S.W ist nicht leer then (* andere Prozesse warten S.count ist 0 *) begin entferne einen Prozess aus S.W mache ihn bereit end else S.count := S.count \(+\) 1 end Damit lässt sich das Problem vom Beobachter und Berichterstatter folgendermaßen lösen. Zunächst wird eine Semaphorvariable S definiert und ihre Zählvariable count auf 1 gesetzt. Die Variable Zähler erhält - wie oben - den Anfangswert 0. Prozess Beobachter begin repeat down(S) (* Beginn kritischer Abschnitt *)\+ beobachte Ereignis Zähler := Zähler + 1 (* Ende kritischer Abschnitt *)\+[1ex] up(S) until false end Prozess Berichterstatter begin repeat down(S) (* Beginn kritischer Abschnitt *)\+ print(Zähler) Zähler := 0 (* Ende kritischer Abschnitt *)\+[1ex] up(S) until false end Damit der Semaphor korrekt arbeitet müssen die Operationen down und up in geeigneter Weise implementiert sein. So darf zum Beispiel bei einem Aufruf down(S) nach dem Test der Zählvariablen count kein anderer Prozess down(S) aufrufen bevor der Wert von count -- falls er positiv war -- um eins heruntergezählt worden ist. Operationen die nicht unterbrochen werden dürfen nennt man atomare Operationen oder auch unteilbare Operationen hiervon handelt die folgende Übungsaufgabe. Übungsaufgabe 2.11 Begründen Sie warum in der soeben beschriebenen Situation Fehler auftreten können. Aufgabe jetzt bearbeiten Wir stellen fest: Die Semaphor-Operationen down und up zur Realisierung kritischer Abschnitte enthalten selbst kritische Abschnitte! Haben wir also unser Problem nur verlagert? Ja aber dadurch wird es leichter lösbar. Wenn man die Semaphor-Operationen nämlich im Betriebssystem implementiert kann man während der Ausführung von down oder up alle Unterbrechungen sperren und damit den gerade beschriebenen Fehler verhindern vergleiche Abschnitt 1.3.4.12 Falls die Semaphorvariable count nur die Werte 0 oder 1 annehmen kann spricht man von einem binären Semaphor der meistens dazu verwendet wird den wechselseitigen Ausschluss (mutual exclusion) von zwei Prozessen sicherzustellen. Beim folgenden Erzeuger-Verbraucher-Problem treten Semaphore auf deren Zählvariablen größere Werte als 1 annehmen können. Angenommen ein Erzeugerprozess \(E\) erzeugt bestimmte Objekte die von einem Verbraucherprozess \(V\) verbraucht werden. Zum Beispiel kann \(E\) ein Compiler sein der Anweisungen in Assemblersprache erzeugt und \(V\) ein Assembler der sie entgegennimmt und daraus Anweisungen im Binärcode macht. Der Erzeuger übergibt die Objekte nicht einzeln an den Verbraucher sondern legt sie in einem Zwischenspeicher ab einem sogenannten Puffer (buffer). Wann immer der Verbraucher ein Objekt verbraucht hat holt er sich aus dem Puffer das nächste. Der Puffer kann maximal \(n\) Objekte speichern. Hierbei treten folgende Synchronisationsprobleme auf: Erzeuger und Verbraucher sollten nicht gleichzeitig auf den Puffer zugreifen der Erzeuger sollte nicht versuchen ein Objekt in den vollen Puffer zu schreiben der Verbraucher sollte nicht versuchen ein Objekt aus dem leeren Puffer zu entnehmen. Wir verwenden zur Lösung jedes Teilproblems einen eigenen Semaphor: var Zugriff : semaphor (* binär                       *) Frei : semaphor (* für \(n\) Betriebsmittel *) Belegt : semaphor (* für \(n\) Betriebsmittel *) begin (* Initialisierung *) Zugriff.count := 1 Frei.count := \(n\) Belegt.count := 0 end Prozess Erzeuger begin erzeuge Objekt down(Frei) down(Zugriff) lege Objekt in Puffer up(Zugriff) [1ex] up(Belegt)   end Prozess Verbraucher begin   down(Belegt) down(Zugriff) entnimm ein Objekt aus Puffer up(Zugriff) up(Frei) verbrauche Objekt end Die Zählvariable count des Semaphors Frei wird in Pascal-Notation mit Frei.count bezeichnet sie gibt an wieviele Plätze im Puffer mindestens noch frei sind. Der Zählerstand von Belegt entspricht der Anzahl der mindestens belegten Plätze. Beachten Sie dass die beiden Prozesse über Kreuz symmetrisch sind. Das Programm löst die obigen drei Synchronisationsprobleme. Bevor ein Erzeuger ein neues Objekt in den Puffer legt muss er prüfen dass der Puffer nicht schon voll ist indem er eine down-Operation auf Frei ausführt. Wenn der Puffer voll ist muss er sich in die Warteschlange von Frei stellen. Wenn nicht dann kann der Erzeuger versuchen auf den Puffer zuzugreifen. Er führt eine down-Operation auf Zugriff aus damit er warten muss wenn gerade ein paraleller Zugriff auf den Puffer stattfindet. Nachdem das erzeugte Objekt in den Puffer gelegt wird führt der Erzeuger eine up-Operation auf Zugriff aus um den exklusiven Zugriff wieder freizugeben. Danach wird noch eine up-Operation auf Belegt ausgeführt da jetzt ein Objekt mehr im Puffer liegt. Bevor der Verbraucher auf den Puffer zugreift muss er zuerst testen ob der Puffer leer ist. Er führt eine down-Operation auf Belegt aus. Wenn der Puffer leer ist dann stellt er sich in die Warteschlange von Belegt. Wenn nicht kann er auf den Puffer zugreifen. Nachdem der Verbraucher den exklusiven Zugriff freigegeben hat führt er eine up-Operation auf Frei aus da der Puffer einen freien Platz mehr hat. Übungsaufgabe 2.12 Hätte man bei den Prozessen Erzeuger und Verbraucher ebensogut die Aufrufe down(Zugriff) ganz vorn und up(Zugriff) ganz am Schluss durchführen können? Aufgabe jetzt bearbeiten Ein berühmtes Problem der Prozess-Synchronisation ist das auf Dijkstra zurückgehende Problem der dinierenden Philosophen. Es lautet wie folgt: \(n\geq 2\) Philosophen sitzen an einem runden Tisch. Jeder dieser \(n\) Philosophen durchläuft zyklisch die drei Zustände Denken Hungrig und Essen . Um essen zu können braucht jeder Philosoph gleichzeitig ein links und ein rechts von seinem Teller liegendes Essstäbchen. Den \(n\) Philosophen stehen aber nur insgesamt \(n\) Stäbchen zur Verfügung (zwei an dem runden Tisch aneinander angrenzende Teller sind durch genau ein Stäbchen getrennt vgl. Abbildung 2.8). Wenn also zwei benachbarte Philosophen gleichzeitig hungrig werden und dann essen wollen so wird es Schwierigkeiten geben13 . Abbildung 2.8: Dinierende Philosophen. Betrachten wir jeden Philosophen als Prozess so wird es gerade darauf ankommen das gleichzeitige Essen zweier benachbarter Philosophen (oder auch nur den Versuch dazu) zu vermeiden. Die einfachste Form einer Lösung dieses Problems wäre die Einführung von Semaphoren \(S_0 ... S_{n-1}\) für jedes Stäbchen. Für jeden Philosophen \(i\) liefere links\((i)\) (rechts\((i)\)) den Index des Semaphors für das Stäbchen links (rechts) vom ihm. Alle Semaphore werden mit 1 initialisiert. Für den \(i\)-ten Philosophen könnte eine Lösung wie folgt lauten: repeat denken hungrig down(\(S_\rm links(i)\)) (* versuche das linke Stäbchen zu nehmen *) down(\(S_\rm rechts(i)\)) (* versuche das rechte Stäbchen zu nehmen *) essen up(\(S_\rm rechts(i)\)) (* lege das rechte Stäbchen zurück *) up(\(S_\rm links(i)\)) (* lege das linke Stäbchen zurück *) until false Übungsaufgabe 2.13 Welche Blockade kann bei dieser einfachen Lösung entstehen? Aufgabe jetzt bearbeiten Man löst das Problem dieser Blockade indem ein Philosoph nur dann Stäbchen aufnehmen darf wenn beide frei sind. Dazu braucht man einen Semaphor mutex 14 der mit \(1\) initialisiert wird. Bevor ein Philosoph versucht die Stäbchen zu nehmen oder zurückzulegen führt er eine down-Operation auf mutex aus. Nachdem er die Stäbchen genommen oder zurückgelegt hat führt er eine up-Operation auf mutex aus. Es kann also immer nur ein Philosoph gleichzeitig Stäbchen nehmen oder zurücklegen. Kann aber ein Philosoph nicht beide Stäbchen aufnehmen so wird er schlafen gelegt. Das wird realisiert indem jeder Philosoph \(i\) einen Semaphor \(p[i]\) hat der mit \(0\) initialisiert wird. Das Ziel des Semaphors \(p[i]\) ist den \(i\)-ten hungrigen Philosoph zu blockieren d.h. warten zu lassen falls eins seiner beiden Stäbchen nicht frei ist. Jeder Philosoph \(i\) benötigt noch eine Statusvariable status\([i]\) um zu verfolgen ob er gerade hungrig ist oder denkt oder isst. Ein Philosoph kann nur in den Zustand von Essen übergehen wenn seine beiden Nachbarn nicht beim Essen sind. Dieser status\([i]\) wird zu Anfang auf Denken gesetzt. Folgender Prozess beschreibt den Philosophen \(i\): repeat denken stäbchen_nehmen(\(i\)) essen stäbchen_weglegen(\(i\)) until false Die Prozedur stäbchen_nehmen(\(i\)): down(\(mutex\)) \(status[i]\) := Hungrig teste(\(i\)) (* nimmt beide Stäbchen wenn sie frei sind *) up(\(mutex\)) down(p\([i]\)) (* hier schläft der Philosoph ein wenn er nicht beide Stäbchen bekommen hat *) Die Prozedur stäbchen_weglegen(\(i\)): down(\(mutex\)) \(status[i]\) := Denken teste(links(\(i\))) (* evtl. Nachbarn aufwecken *) teste(rechts(\(i\))) (* evtl. Nachbarn aufwecken *) up(\(mutex\)) Zum Schluss die Prozedur teste(\(i\)): if (\(status[i]\) = Hungrig and \(status[links(i\))] < > Essen and \(status[rechts(i\))] < > Essen) then begin \(status[i]\) := Essen up(p[\(i\)]) end Die Prozedur teste\((i)\) stellt fest ob der Philosoph \(i\) hungrig ist und die zwei Stäbchen bekommen kann. Wenn ja dann geht er in den Zustand Essen über. Man beachten dass die Prozedur teste sowohl von Philosophen für sich selbst aufgerufen wird (in stäbchen_nehmen) als auch für seine Nachbarn (in stäbchen_weglegen). Außerdem wird es hier nicht automatisch gerecht zugehen was die Verteilung der Essenszeiten angeht wie man sich leicht überlegen kann.  
2; mod_longpage;154; 2; 4; 2.4 Dateisysteme Neben den Prozessen sind Dateien und Dateisysteme die Objekte mit denen jeder Anwender und Programmierer zu tun hat sie bestimmen zu einem großen Teil seine Arbeitsumgebung. Wir wollen uns im folgenden Abschnitt zunächst mit der logischen Sicht auf die Dateien beschäftigen die ein Dateisystem den Benutzern bietet. Dabei interessieren uns besonders Verzeichnisse und Pfade wie UNIX sie verwendet und die Befehle zu ihrer Verwaltung. In Abschnitt 2.9 diskutieren wir dann wie das Dateisystem intern die logische Sicht der Dateien auf die physische Realität des Sekundärspeichers abbildet. Dateien Verzeichnisse und Pfade Dateisysteme dienen der Verwaltung von Dateien (files). Eine Datei ist - wie schon in Abschnitt 1.6 erwähnt - eine Folge von Datensätzen die zusammengehörige Information enthalten. Für den Benutzer wird eine Datei durch ihren Namen (filename) kenntlich. Es empfiehlt sich sprechende Namen zu vergeben aus denen der Inhalt der Dateien ersichtlich wird.15 Oft wird der Typ einer Datei durch eine Erweiterung (extension) auch Suffix genannt des Dateinamens bezeichnet. Wer zum Beispiel mit dem Formatierer LaTeX arbeitet kennt wohl Dateien der Art SeminarArbeit.tex SeminarArbeit.pdf SeminarArbeit.aux die - in dieser Reihenfolge - einen zu formatierenden Text namens SeminarArbeit den geräteunabhängig als PDF formatierten Text zum Anzeigen und Drucken und die automatisch generierten Hilfsinformationen zum Formatieren enthalten. In UNIX sind Dateien in Verzeichnissen (directories) zusammengefasst. Der Benutzer befindet sich zu jedem Zeitpunkt in einem aktuellen Arbeitsverzeichnis dessen Namen man sich mit dem Befehl pwd (print working directory) ausgeben lassen kann.16 Ein Verzeichnis kann Dateien und weitere Unterverzeichnisse enthalten. Welche Objekte im aktuellen Verzeichnis enthalten sind kann man sich mit dem Befehl ls auflisten lassen durch Angabe weiterer Parameter lässt sich festlegen welche Attribute der im Verzeichnis enthaltenen Objekte ausgegeben werden. So kann zum Beispiel der Befehl ls -ls eine Ausgabe folgender Art erzeugen: 1 drwxrwxr-- 2 mueller bteam 1024 Jul 27 16:43 archiv 4 -rw------- 2 mueller bteam 4038 Jul 27 18:46 entwurf.tex 30 -rw-r----- 1 fischer bteam 29710 Mär 20 2015 flip 2 -rwxr-xr-- 1 mueller bteam 1730 Jul 15 10:53 myprogram 1 drwx------ 4 mueller bteam 1024 Mai 12 09:01 privat Ganz rechts stehen die Namen der fünf in diesem Verzeichnis enthaltenen Objekte nach ihnen ist die Ausgabe alphabetisch sortiert.17 In der Spalte davor ist aufgelistet wann zum letzten Mal schreibend auf diese Objekte zugegriffen worden ist. Liegt dieser Zeitpunkt in einem vergangenen Jahr so wird statt der Uhrzeit das Jahr angegeben. Das Datum des letzten lesenden Zugriffs liefert ls -lsu. Links vom Datum steht die GrößeDateigröße des jeweiligen Objekts in Bytes. Die Größe in Blöcken wie sie in Abschnitt 1.2.3 eingeführt wurden wird in der Spalte ganz links angegeben dafür sorgt der Parameter s des ls-Kommandos bei ls -l wird diese Spalte nicht mit ausgegeben. Übungsaufgabe 2.14 Können Sie bei diesem Beispiel die Blockgröße in Byte bestimmen? Wie erklären sich die Abweichungen? Aufgabe jetzt bearbeiten Die nächsten Eintragungen haben mit den Zugriffsberechtigungen auf die Objekte zu tun. Hierzu ein paar Vorbemerkungen. Auf einem Rechner mit mehreren Benutzern kann man nicht jedem Benutzer den Zugriff auf alle überhaupt vorhandenen Daten erlauben in einem solchen System ließe sich kein wirksamer Datenschutz realisieren und die Gefahr der versehentlichen Zerstörung von Information wäre zu groß. Es ist aber auch nicht sinnvoll wenn jeder Benutzer nur auf seine eigenen Dateien zugreifen darf denn das beschränkt die Möglichkeit zur Kooperation. Benötigt werden also Mechanismen zur Vergabe abgestufter Zugriffsrechte. Naheliegende Ansätze wären zu jeder Datei eine Liste aller Benutzer anzulegen die Zugriff auf die Datei haben oder für jeden Benutzer eine Liste mit allen Dateien auf die er zugreifen darf. Beide Verfahren sind recht ineffizient. In UNIX wird deshalb ein anderer Weg beschritten um die Zugriffsrechte auf eine Datei oder ein Verzeichnis festzulegen. Man teilt die Systembenutzer in drei Klassen ein: den Eigentümer des Objekts - er wird in diesem Zusammenhang als user bezeichnet - die Arbeitsgruppe (group) usergroupother der er angehört und alle übrigen Systembenutzer (other). In der Ausgabe des Befehls ls -ls steht in der vierten Spalte von links der Name des Besitzers in der fünften der Name seiner Arbeitsgruppe. Außerdem unterscheidet man zwischen den Zugriffsarten Lesen (read) Schreiben (write) und Ausführenausführen (execute). readwriteexecute Das Recht auf Schreibzugriff (write permission) erlaubt dabei auch ein Überschreiben und enthält deshalb das Recht zum Löschen des Objekts. Bei einem Verzeichnis bedeutet das Ausführungsrecht dass man dieses Verzeichnis zum aktuellen Arbeitsverzeichnis machen darf. Nun kommen wir zur Ausgabe der Objekte in unserem Beispielverzeichnis zurück und betrachten die zweite Spalte von links. Das erste Zeichen bei den Objekten archiv und privat ist ein d für directory -- hier handelt es sich also um Verzeichnisse. Bei den übrigen Objekten finden wir an dieser Stelle einen Strich dies sind Dateien. Die in der zweiten Spalte auf das erste Zeichen folgenden 9 Zeichen beschreiben die Zugriffsberechtigungen für die verschiedenen Benutzerklassen in der Reihenfolge rwxrwxrwx u g o Wo ein Strich steht ist das betreffende Recht nicht vergeben. So hat zum Beispiel die vierte Zeile -rwxr-xr-- 1 mueller bteam 1730 Jul 15 10:53 myprogram folgende Bedeutung: Der Eigentümer Müller hat an seiner Datei myprogram das Lese- Schreib - und Ausführungsrecht. Die übrigen Mitglieder seiner Arbeitsgruppe bteam dürfen das Programm lesen und ausführen aber nicht schreiben. Alle übrigen Benutzer dürfen das Programm zwar lesen aber weder ausführen noch schreiben. Und die Zeile drwxrwxr-- 4 mueller bteam 512 Jul 27 16:43 archiv besagt dass der Eigentümer und alle übrigen Gruppenmitglieder im Verzeichnis archiv Einträge von Objekten lesen und schreiben dürfen und dieses Verzeichnis auch zum aktuellen Arbeitsverzeichnis machen dürfen hierzu verwendet man den Befehl cd archiv wobei cd für change directory steht. Alle übrigen Benutzer dürfen aber nur lesen welche Objekte im Verzeichnis archiv enthalten sind. Übungsaufgabe 2.15 Kann Benutzer Müller die Datei flip editieren wenn Müller und Fischer Mitglieder der Gruppe bteam sind? Aufgabe jetzt bearbeiten Angenommen Müller möchte allen Mitgliedern seiner Arbeitsgruppe gestatten an seinem Programm myprogram mitzuschreiben. Dann kann er den Befehl chmod g+w myprogram erteilen. Hierbei steht chmod für change mode und g+w bedeutet dass der Gruppe das Schreibrecht hinzugefügt wird mit g-x hätte Müller ihr das Recht zum Ausführen des Programms entzogen. Außer dem Superuser kann nur der Eigentümer eines Objekts dessen Rechte ändern. Übungsaufgabe 2.16 Was bewirkt der Befehl chmod go+rw entwurf.tex? Aufgabe jetzt bearbeiten Rechte an einem Objekt vererben sich übrigens nicht automatisch auf kleinere Benutzerklassen: Im Beispiel ----r--r-- 1 meier cteam 402 Jul 27 16:43 akte hat jeder Benutzer das Leserecht an der Datei akte - nur ihr Eigentümer Meier nicht! Meier kann sich jedoch das Leserecht wieder zuweisen. Wer im aktuellen Verzeichnis schreibberechtigt ist kann dort mit touch neudatei oder mkdir neuverzeichnis eine neue Datei mit 0 Byte Länge beziehungsweise ein neues Verzeichnis anlegen hierbei steht mk für make.18 Der Befehl rm neudatei löscht die Datei neudatei wieder natürlich bedeutet rm hier remove. Zum Löschen eines leeren Verzeichnisses verwendet man rmdir. Mit dem Befehl rm -r neuverzeichnis kann man das Verzeichnis neuverzeichnis und alle darin enthaltenen Objekte löschen selbst wenn man an diesen nicht schreibberechtigt ist! (Vorsicht mit dieser Option!) Mit dem Befehl cp altdatei neudatei kann man eine Datei die man lesen darf kopieren (copy) und dabei den Namen der Kopie festlegen. Dabei wird man zum Eigentümer der Kopie. Man kann mit Varianten dieses Befehls auch Verzeichnisse kopieren. Übungsaufgabe 2.17 Kann in unserem Beispiel Benutzer Müller sich eine editierbare Version der Datei flip verschaffen? Aufgabe jetzt bearbeiten Wenn man in Verzeichnissen Unterverzeichnisse anlegt entstehen baumförmige Strukturen ein Beispiel ist in Abbildung 2.5 zu sehen.baumförmige Struktur Abbildung 2.5: Ein Ausschnitt aus einer Verzeichnisstruktur unter UNIX. Ganz oben befindet sich das Wurzelverzeichnis (root) das immer mit dem Schrägstrich (slash) / bezeichnet wird. Darin sind unter anderem eine Datei unix mit dem Betriebssystemkern19 enthalten und ein Verzeichnis home mit Benutzerdateien. Weiter unten sehen wir im Verzeichnis mueller ein Verzeichnis namens projekt dessen Inhalt wir bei unserem Beispiel oben aufgelistet hatten. Jedem Benutzer wird vom Systemverwalter ein Heimatverzeichnis (home directory) zugewiesen das bei jeder Anmeldung (log-in) zunächst das aktuelle Arbeitsverzeichnis ist. Für den Benutzer Müller könnte das zum Beispiel das Verzeichnis mueller sein. Mit dem Befehl cd projekt kann er sich nach der Anmeldung in das Verzeichnis projekt begeben. Wir sehen dass sowohl Meier als auch Müller eine Datei namens myprogram besitzen. Dies führt aber nicht zu Verwechselungen denn für das Filesystem sind die Zugriffspfade im Verzeichnisbaum entscheidend -- und die sind bei diesen beiden Dateien verschieden! So hat Meiers Datei den vollständigen Namen /home/meier/myprogram während die vollständige Bezeichnung für Müllers Datei /home/mueller/projekt/myprogram lautet. Neben diesen absoluten Pfadnamen können die Benutzer auch relative Pfadnamen verwenden die im aktuellen Arbeitsverzeichnis beginnen. So kann Müller seine Datei einfach durch myprogram ansprechen während er die gleichnamige Datei von Benutzer Meier ../../meier/myprogram nennen kann dabei bezeichnet .. stets das Elternverzeichnis. Für das aktuelle Verzeichnis gibt es übrigens die Bezeichnung . und das Heimatverzeichnis wird mit \(\sim\) bezeichnet. Diese Pfadnamen werden auch bei zahlreichen Befehlen verwendet. Will zum Beispiel Müller seine Datei entwurf.tex an Meier abgeben so kann er den Befehl mv für move dazu verwenden und mv entwurf.tex ../../meier eingeben.20 Voraussetzung ist dass Müller im Verzeichnis meier schreibberechtigt ist. Name Eigentümer und Zugriffsrechte der Datei bleiben hierbei erhalten. Damit Meier mit der Datei entwurf.tex etwas anfangen kann sollte Müller vorher zumindest allen Benutzern seiner Gruppe Schreib- und Leserecht gewähren siehe Übungsaufgabe 2.16. Um zu erreichen dass Meier und Müller beide bequem an der Datei entwurf.tex arbeiten können gibt es noch eine andere Möglichkeit: Müller setzt zunächst die Zugriffsrechte um wie gerade besprochen. Dann gibt er den Befehl ln entwurf.tex ../../meier ein. Hierbei steht ln für link und bewirkt dass im Verzeichnis meier ein weiterer Verweis (hard link) auf die Datei entwurf.tex angelegt wird. Dieser Verweis heißt hier dann ebenfalls entwurf.tex könnte aber auch einen anderen Namen bekommen. Wenn einer von beiden an der Datei Änderungen vornimmt sind sie auch für den anderen sichtbar.21 Wenn Meier oder Müller seinen Eintrag entwurf.tex löscht bleibt die Datei für den anderen erhalten da noch ein anderer Verweis auf diese Datei besteht. Damit können wir in unserer Beispielausgabe des Befehls ls -ls auch die dritte Spalte von links erklären: Sie gibt die Anzahl der Verweise auf das betreffende Objekt an. Bei Dateien beträgt diese Zahl eins plus die Anzahl der zusätzlich eingerichteten Verweise bei entwurf.tex sehen wir deshalb eine 2. Bei Verzeichnissen verweist zusätzlich jedes Verzeichnis auf sich selbst (mit .) und auf sein Elternverzeichnis (mit ..). Deshalb steht bei dem leeren Verzeichnis archiv eine 2 und bei privat die Zahl 4 weil privat noch zwei Unterverzeichnisse enthält vergleiche Abbildung 2.5. Interne Struktur von Dateisystemen In Abschnitt 2.8 haben wir uns damit beschäftigt welche logische Sicht auf die Dateien das Dateisystem als der für die Ein- und Ausgabe zuständige Teil des Betriebssystems den Benutzern bietet. Jetzt wollen wir untersuchen wie das Dateisystem intern die logische Sicht auf die physische Sicht abbildet. Während eine Datei sich aus logischer Sicht als eine Folge von Datensätzen darstellt ist sie aus physischer Sicht eine Folge von gleich großen Blöcken. Auf der Magnetplatte wird jeder Block mit Zusatzinformation versehen und in einem Sektor gespeichert vergleiche Abschnitt 1.2.3 und Übungsaufgabe 2.14. Aus zwei Gründen wäre es wünschenswert die Blöcke einer Datei möglichst hintereinander22 auf der Platte zu speichern: Bei sequentiellem Zugriff auf mehrere aufeinander folgende Blöcke wird dadurch die Zeit für die Bewegung der Schreib-/Leseköpfe minimiert. Und bei wahlfreiem Zugriff auf einzelne Blöcke kann man leicht die Blocknummern berechnen: Wenn der \(i-\)te Block der Datei gelesen werden soll und die Datei bei Block \(b\) beginnt so muss der Gerätetreiber einen Leseauftrag für den Block mit der Nummer \(b+i\) erhalten. Leider führt der Wunsch nach zusammenhängender Speicherung von Dateien zu demselben Problem der externen Fragmentierung das wir in Abschnitt 2.1 bei der Hauptspeichervergabe beobachtet haben: Zwischen den Dateien entstehen Lücken die sich nicht mehr zur Speicherung längerer Dateien eignen. Eine Kompaktifizierung durch Zusammenschieben der vorhandenen Dateien ist zwar möglich wird aber wegen des hohen Zeitaufwands in der Regel nicht bei laufendem Betrieb durchgeführt. Man hat deshalb auch beim Sekundärspeicher die Forderung nach zusammenhängender Speicherung aufgegeben und stattdessen Speicherverfahren entwickelt bei denen die Blöcke einzeln gespeichert werden können wo immer gerade Platz frei ist. Hier stellt sich die Frage wie man die Blöcke effizient wiederfindet. Eine naive Lösung könnte darin bestehen die Blöcke einer Datei in einer Liste zu verketten: In dem Verzeichnis das die Datei enthält wäre dann beim Dateinamen die physische Adresse von Block Nr. 0 der Datei aufgeführt am Schluss dieses Blocks stünde die physische Blocknummer von Dateiblock Nr. 1 und so fort. Diese Idee geht zwar effizient mit dem Speicherplatz um und vermeidet externe Fragmentierung sie hat aber einen anderen schwerwiegenden Nachteil: Wahlfreier Zugriff wird nicht unterstützt. Um Block Nr. \(i\) zu lesen sind nämlich \(i\) Zugriffe auf den externen Speicher auf alle Blöcke \(0\) bis \(i-1\) notwendig. Die Betriebssysteme MS-DOS und OS/2 umgingen diesen Nachteil durch einen einfachen Trick: Verkettet werden nicht die Blöcke sondern ihre physischen Blockadressen. Abbildung 2.6 zeigt ein Beispiel für diese als file-allocation table bekannte Struktur. Sie wird als FAT : array[0..MaxBlockNr\(–1\)] of BlockNr implementiert und enthält für jeden Block der Magnetplatte23 einen Eintrag. Im Dateiverzeichnis steht - wie oben - bei jedem Dateinamen myprogram die physische Adresse \(i\) von Dateiblock Nr. 0. Der Eintrag FAT\([i]\) enthält dann die Adresse \(j\) des zweiten Blocks der Datei myprogram in FAT\([j]\) steht die Adresse des dritten Blocks und so fort. Für den letzten Block \(l\) der Datei hat FAT\([l]\) den speziellen Wert eof der für end of file steht. Für freie Blöcke lautet der Eintrag free. Wenn eine Datei verlängert werden soll kann man also die FAT dazu benutzen einen unbelegten Block zu finden. Abbildung 2.6: Ein Beispiel für eine file-allocation table und die zugehörige Verteilung der Dateien auf der Festplatte. Die FAT wird auf konsekutiven Blöcken im Externspeicher abgelegt. Die Blockgröße sollte so bemessen sein dass die FAT zur Laufzeit in den Cache im Hauptspeicher passt. Dann kann man die physische Adresse des \(i\)-ten Blocks einer Datei mit \(i\) Hauptspeicherzugriffen herausfinden also rund tausendmal schneller als bei einer Verkettung der Blöcke im Externspeicher. Übungsaufgabe 2.18 Wieviel Platz belegt die file-allocation table einer Partition von 32 MByte bei einer Blockgröße von 512 Byte? Können Sie eine allgemeine Formel aufstellen die den Platzbedarf der FAT in Blöcken angibt? Aufgabe jetzt bearbeiten Anstatt die Blöcke einer Datei als verkettete Liste zu verwalten kann man für jede Datei einen Index anlegen. Ein Index ist eine Tabelle die zu jeder logischen Blocknummer die zugehörige physische Blocknummer enthält. Sie entspricht der Seitentabelle eines Prozesses bei der Hauptspeicherverwaltung vergleiche Abbildung 2.2. Diese Indextabelle wird selbst auch im Externspeicher abgelegt. Auch mit diesem Ansatz ist das Problem der externen Fragmentierung beseitigt. Heikler ist dagegen die Frage nach der Indexgröße: Bei einer sehr kurzen Datei ist es nicht zu rechtfertigen einen ganzen Block für die Speicherung des Index zu verwenden. Wenn dagegen die Datei sehr lang ist reicht ein einzelner Block hierfür nicht aus es kann dann sogar notwendig werden einen Index für den Index anzulegen. In UNIX wird eine recht elegante Variante des Indexprinzips verwendet. Für jede Datei und für jedes Verzeichnis gibt es eine Struktur die als inode24 bezeichnet wird ein Beispiel sehen Sie in Abbildung 2.9. Abbildung 2.9: Ein Beispiel für einen inode mit Index- und Dateiblöcken wobei jeder Indexblock \(x\) Blocknummern speichert. Am Anfang eines inode stehen die Attribute des Objekts die wir uns in Abschnitt 2.8 mit dem Befehl ls -ls angesehen hatten wie Zugriffsrechte Eigentümer und Gruppe Zeitstempel GrößeDateigröße und Anzahl der Verweise auf das Objekt. Es folgen die physischen Adressen der ersten 12 Blöcke25 der Datei. Dann kommt die Adresse eines Blocks der die Adressen der nächsten logischen Dateiblöcke enthält - ein einfach-indirekter Index. Daran schließen die Adressen eines zweifach-indirekten und schließlich eines dreifach-indirekten Indexblocks an dieser enthält die Adressen von zweifach-indirekten Indexblöcken von denen jeder die Adressen einfach-indirekter Indexblöcke enthält. Übungsaufgabe 2.19 Angenommen wir haben eine Partition von 32 MByte bei einer Blockgröße von 512 Byte. Wie viele Blöcke darf eine Datei haben damit sie sich durch einen inode beschreiben lässt? Aufgabe jetzt bearbeiten Dieses Schema bietet mehrere Vorteile. Zum einen sind alle wesentlichen Informationen über eine Datei oder ein Verzeichnis im inode auf beschränktem Raum zusammengefasst im Unterschied zur file-allocation table wird aber nur für die wirklich vorhandenen Objekte Speicherplatz belegt. Auf kurze Dateien -- oder allgemeiner: auf die ersten 12 Blöcke jeder Datei -- kann man sehr effizient zugreifen. Allgemein genügen maximal 5 Zugriffe auf den externen Speicher um einen beliebigen Block einer langen Datei zu lesen. Wir sehen also: Durch inodes wird wahlfreier Zugriff recht gut unterstützt. Die FAT bietet eine gute Übersicht darüber welche Blöcke auf der Festplatte noch frei sind. Das leisten die inodes nicht. Man kann zu diesem Zweck zusätzlich einen langen freie Blöcke Bitvektor verwenden der für jeden Block ein Bit enthält welches angibt ob der Block frei oder belegt ist. Der sequentielle Dateizugriff kann bei allen hier besprochenen Formen der nicht-zusammenhängenden Speicherung mühsam sein -- wenn nämlich die Blöcke über die gesamte Magnetplatte verteilt sind. UNIX versucht dieses Problem durch eine Anhebung der Blockgröße auf mehrere KByte zu mildern. Die inodes der vorhandenen Dateien und Verzeichnisse sind in UNIX in einer Tabelle an fester Position auf der Festplatte gespeichert. In den Verzeichnissen (directories) steht beim Namen eines jeden enthaltenen Objekts die Nummer seines inodes in dieser Tabelle man kann sich die inode-Nummern durch den zusätzlichen Parameter i des ls-Befehls ausgeben lassen.26 Für jeden Prozess gibt es eine Liste mit den Namen aller Dateien die dieser Prozess geöffnet27 hat diese Liste ist Teil des Prozesskontextes. Es kann durchaus vorkommen dass mehrere Prozesse gleichzeitig auf dieselbe Datei zugreifen. Deshalb existiert zusätzlich ein systemweites Verzeichnis aller in Gebrauch befindlicher Dateien auf das die Einträge in den Prozessverzeichnissen verweisen. In dem globalen Verzeichnis werden ebenfalls die Nummern der inodes verwendet. Damit sind wir am Ende der ersten beiden Kurseinheiten über Betriebssysteme angekommen. Wir hoffen die Lektüre hat Ihnen Vergnügen gemacht und Sie ein wenig dazu ermutigt sich mit Linux zu beschäftigen -- falls Sie nicht ohnehin schon mit Linux arbeiten. Für die Bearbeitung der nächsten beiden Kurseinheiten über Rechnernetze wünschen wir Ihnen viel Spaß und viel Erfolg! Literaturhinweise zu Betriebssystemen und speziell zu Linux/Unix siehe z.B. in Kurseinheit 1. 
2; mod_longpage;185; 3; 0; Kurstext Kurseinheit 3 Anja Haake Jörg Haake Christian Icking Lihong Ma 3 Anwendungen und Transport Einleitende Bemerkungen Die beiden Kurseinheiten 3 und 4 beschäftigen sich mit Rechnernetzen. Rechnernetze oder Computernetzwerke bezeichnen mehrere miteinander verbundene Computer. Mit Hilfe eines Rechnernetzes können Anwendungen die auf den angeschlossenen Computern laufen miteinander kommunizieren z.B. kann eine E-Mail-Anwendung über ein Rechnernetz eine Nachricht an einen Empfänger verschicken. Die an Rechnernetze angeschlossenen Computer auf denen die Anwendungsprogramme laufen werden oft als Hosts oder Endsysteme bezeichnet und Server nennen wir solche die bestimmte Dienstleistungen für andere Hosts bereitstellen. Rechnernetze sind üblicherweise in mehreren übereinander liegenden Schichten organisiert dieses Konzept von Abstraktion und Kapselung wird in der Informatik oft verwendet siehe Abschnitt 1.3.3. Die unterste Schicht behandelt die physikalische Bitübertragung zwischen direkt verbundenen Geräten. Höhere Schichten stellen zusätzliche Funktionen zur Verfügung wie Fehlerkorrektur oder Routing. Hierbei verwenden höhere Schichten jeweils die Funktionen der darunter liegenden Schicht. Auf der höchsten Schicht finden wir die Anwendungen mit denen die Benutzer arbeiten z.B. Web-Browser oder Buchungssysteme. Häufig werden Rechnernetze in aufsteigender Reihenfolge ihrer Schichten von Schicht 1 (Bitübertragung) bis zur Anwendungsschicht eingeführt. Bei diesem Bottom-Up Ansatz bleibt allerdings oft unklar warum die Funktionen einer Schicht so definiert sind da ja die sie verwendende darüber liegende Schicht erst später behandelt wird. Darum gehen wir in Kurseinheit 3 und 4 den umgekehrten Weg: beginnend mit der Anwendungsschicht behandeln wir diesen Stoff in der Top-Down-Reihenfolge bis zur Bitübertragungsschicht. Außerdem werden wir frühzeitig ein konkretes Netzwerk - das Internet - einführen um die praktische Bedeutung der Konzepte klarer darzustellen. Dazu gehört auch ein Überblick über den Aufbau von komplexen Rechnernetzen. Aus Sicht der Komponenten eines Netzwerks und ihrer Verbindungen (der sogenannten Netzwerktopologie) arbeiten wir uns von außen also von den Rechnern mit denen die Nutzer arbeiten nach innen in den Kern des Netzwerks vor. Von diesem Top-Down Ansatz versprechen wir uns eine leichtere Verständlichkeit des Stoffes: Fast alle Anwender sind heute mit Netzwerkanwendungen wie E-Mail oder dem World Wide Web (WWW) vertraut. Wir gehen davon aus dass die Erläuterung der Funktionsweise von solchen Netzwerkanwendungen zu Beginn des Kurses eine motivierende Wirkung hat. Die Notwendigkeit der Dienstleistungen und Konzepte der unteren Schichten ergibt sich aus den Bedürfnissen der oberen Schichten d.h. letztendlich aus den Bedürfnissen des Anwenders die intuitiv einzusehen sind. Darüber hinaus betont der Top-Down Ansatz die Anwendungsschicht die die größten Wachstumsmöglichkeiten im Bereich der Rechnernetze darstellt. Außerdem bekommen Sie schon früh einen Einblick in die Entwicklung von Netzwerkanwendungen. Natürlich kann das Thema Rechnernetze nicht in zwei Kurseinheiten erschöpfend behandelt werden. Es ist unser Ziel Ihnen in diesen beiden Kurseinheiten ein grundlegendes und praktisch orientiertes Verständnis des Aufbaus und der Funktionsweise von Rechnernetzen und der damit zusammenhängenden Probleme zu vermitteln. Außerdem wollen wir die Benutzung von Rechnernetzen zur Realisierung verteilter Anwendungen an einigen Beispielen illustrieren. Wenn Sie dieses Thema im weiteren Verlauf Ihres Studiums vertiefen wollen dann können Sie eine Reihe weiterführender Kurse belegen. Der Kurs 1802 Betriebssysteme betrachtet den Aufbau und die Funktionsweise moderner Betriebssysteme. Im Kurs 1678 Verteilte Systeme wird genauer dargestellt wie verteilte Systeme auf der Basis moderner Betriebssysteme und Rechnernetze entworfen und realisiert werden können. Der Kurs 1690 Kommunikations- und Rechnernetze betrachtet mit mehr Detail wie Netzwerke entworfen werden und wie sie funktionieren. Schließlich betrachtet der Kurs 1866/67/68 Sicherheit im Internet mit welchen Mechanismen im Internet Sicherheit gegen Spione und Einbrecher gewährleistet werden kann. Trotz sehr sorgfältigen Korrekturlesens ist leider damit zu rechnen dass sich Fehler in den Kurs eingeschlichen haben. Sie helfen uns und Ihren Kommiliton(inn)en wenn Sie uns auf Fehler Ungenauigkeiten oder Schwierigkeiten beim Durcharbeiten des Kurses hinweisen! Bitte schicken Sie dazu Ihre Hinweise an die Kursbetreuer.  
2; mod_longpage;185; 3; 1; 3.1 Einführung Von der Anwendungsseite aus betrachtet geht es bei dem Thema Rechnernetze um die Kommunikation zwischen Prozessen die auf verschiedenen Endsystemen laufen. In Kurseinheit 1 und 2 haben wir das Konzept des Prozesses zur Ausführung von Programmen oder Anwendungen in einem Endsystem kennen gelernt. Wenn Prozesse auf dem gleichen Endsystem laufen kommunizieren sie miteinander mit Hilfe der prozessübergreifenden Kommunikation wie z.B. über einen gemeinsamen Speicher (vgl. Abschnitt 1.6 und Abschnitt 2.3). In Kurseinheit 3 und 4 geht es nun darum wie Prozesse miteinander kommunizieren die auf unterschiedlichen Endsystemen mit durchaus auch unterschiedlichen Betriebssystemen laufen. Wenn ein Anwender ein Programm startet dann wird das Programm vom Betriebssystem als Prozess ausgeführt. Dabei kann ein Programm zur Ausführungszeit durchaus mehrere Prozesse starten. Wenn ein Programm nun Dienste eines anderen Programms aufruft dann müssen zur Laufzeit die Prozesse welche die entsprechenden Programme ausführen miteinander kommunizieren. Laufen diese Prozesse auf verschiedenen Rechnern z.B. das E-Mail-Leseprogramm auf einem PC und ein E-Mail-Serverprogramm auf einem Server dann spricht man auch von verteilten Anwendungen oder Netzwerkanwendungen. Die Konzepte aus dem Gebiet der Rechnernetze können also immer aus der Perspektive betrachtet werden wie sie dazu beitragen verteilte Anwendungen zu ermöglichen. Im Folgenden werden Anwendungsbeispiele gegeben sowie die gebräuchlichste Klassifikation für Computernetze nach ihrer Größe und Komplexität eingeführt. In der Entwicklungsgeschichte der Computertechnologie hat sich neben dem Konzept einer vorwiegend zentralen Datenverarbeitung auch das Konzept einer eher verteilten Datenverarbeitung entwickelt. Während bei der zentralen Datenverarbeitung die Verarbeitung der Daten auf einem zentralen Rechner passiert werden bei der verteilten Datenverarbeitung die Daten auf mehreren Rechnern arbeitsteilig verarbeitet. Die Motive Rechner über Kommunikationsnetze miteinander zu verbinden haben sich im Laufe der Zeit geändert. Stand zu Beginn dieser Entwicklung die Datenfernverarbeitung im Vordergrund erlauben heute internationale Kommunikationsnetze die Kommunikation und Kooperation zwischen weltweit verteilten Rechnern und ihren Benutzern. Der sekundenschnelle Austausch von Nachrichten und der Zugriff auf Daten über Kontinente hinweg ist für viele Anwendungszwecke nicht mehr wegzudenken. Ein klassisches Anwendungsbeispiel der Datenfernverarbeitung sind Flugbuchungssysteme bei denen viele Datensichtgeräte der einzelnen Büros einer oder mehrerer Fluggesellschaften mit einem zentralen Datenverarbeitungssystem verbunden sind. Buchungswünsche einzelner Kunden können somit dezentral erfasst zentral ausgewertet und dezentral bestätigt oder geändert werden. Jedes Büro steht in Kontakt zum Rechner. Der zentrale Rechner kennt ständig den aktuellen Stand aller Buchungen und kann dementsprechend Buchungswünsche bestätigen ablehnen oder Ersatzvorschläge ausarbeiten. Ferner können zentral Passagierlisten etc. erstellt und dezentral z.B. auf den betroffenen Flughäfen ausgegeben werden. Ähnliche Beispiele finden sich im Bereich großer Banken Versandhäuser usw. Die skizzierten Aufgaben wären ohne die Möglichkeit der Datenfernverarbeitung nicht durchführbar. Zur Datenübertragung selbst können bestehende Datennetze oder eigens eingerichtete private Netze verwendet werden die auch Richtfunkstrecken und Satellitenverbindungen einschließen können. Die Gründe Rechner miteinander zu vernetzen lassen sich grob wie folgt klassifizieren: Beim Betriebsmittel- oder Funktionsverbund (resource sharing) steht das Ziel im Vordergrund sämtliche im Netz vorhandenen Betriebsmittel wie Daten Programme oder Geräte auf jedem am Rechnernetz beteiligten Rechner zugänglich zu machen. Auch das gesamte Kommunikationsnetz kann in diesem Zusammenhang als Betriebsmittel betrachtet werden. Besteht das Hauptziel eines Betriebsmittelverbundes darin auf netzweit verteilte Datenbestände zuzugreifen spricht man auch von einem Datenverbund (data sharing) wobei auch das mehrfache Halten von Datenbeständen aus Sicherheitsgründen eine Rolle spielen kann. Der Last- und Leistungsverbund gestattet es anfallende Rechenlasten gleichmäßig auf mehrere Rechner zu verteilen bzw. mehrere Rechner für das Lösen einer einzigen umfangreichen Aufgabe einzusetzen. Ein Wartungsverbund schließlich ermöglicht die zentrale Wartung Störungserkennung und -behebung bei einer Vielzahl räumlich verteilter Rechner. Seit dem internationalen Siegeszug des Internets und insbesondere des auf ihm realisierten World Wide Web (WWW) stehen der Online-Zugriff auf weltweit erfasste Dokumente sowie die Kommunikation und Kooperation zwischen menschlichen Benutzern im Vordergrund des Einsatzes von Rechnernetzen. Der Austausch von Nachrichten durch E-Mail das Arbeiten an gemeinsamen Projekten oder das Veranstalten internationaler Videokonferenzen sind aus dem beruflichen und zunehmend auch aus dem privaten Alltag nicht mehr wegzudenken. Das Fachgebiet der Informatik das sich mit der Unterstützung der Zusammenarbeit von Menschen durch den Einsatz von Computertechnologie befasst wird als Computerunterstützte Gruppenarbeit (Computer Supported Cooperative Work CSCW) bezeichnet. Je nach ihrer räumlichen Ausdehnung unterteilt man Rechnernetze in Lokale Netze (Local Area Network LAN) mit Ausdehnungen von höchstens wenigen Kilometern Regionale Netze (Metropolitan Area Network MAN) mit Ausdehnungen von ca. 100 km und Weitverkehrsnetze (Wide Area Network WAN) die ganze Länder und Kontinente und im Weitverkehrsverbund die gesamte Erde umspannen. Wir werden später sehen dass an lokale Netze und an Weitverkehrsnetze in der Regel sehr unterschiedliche Anforderungen gestellt werden und sich diese in ihren Architekturen und Betriebsweisen erheblich unterscheiden. Selbst Rechnernetze unterschiedlichster Konfiguration und Technologie lassen sich mit Hilfe sogenannter Switches oder Bridges Router und Gateways miteinander vernetzen. Der einzelne Rechner oder Benutzer gewinnt so den Eindruck er sei über ein einziges erdumspannendes Kommunikationsnetz mit anderen Rechnern oder Benutzern verbunden. Abbildung 3.2 skizziert eine Situation in der verschiedene Rechner und lokale Netze über ein Netz von Weitverkehrsnetzen miteinander verbunden sind. Abbildung 3.2: Weltweiter Verbund von Rechnernetzen.  
2; mod_longpage;185; 3; 2; 3.2 Überblick über Rechnernetze Kommunikation Protokolle und Schichtenarchitekturen Prozesse auf zwei unterschiedlichen Endsystemen kommunizieren miteinander durch den Austausch von Nachrichten über ein Computernetzwerk. Ein sendender Prozess erzeugt und sendet Nachrichten über das Netzwerk. Ein empfangender Prozess empfängt diese Nachrichten und antwortet möglicherweise indem er Nachrichten zurück schickt. Die Regeln und Konventionen auf denen diese Kommunikation basiert werden Protokolle genannt. Ein Protokoll ist eine Vereinbarung zwischen kommunizierenden Parteien über den Ablauf der Kommunikation. Ein Protokoll definiert die Nachrichtentypen (z.B. Anforderungen und Antworten) ihre Syntax (d.h. Felder und Feldabgrenzungen in den verschiedenen Nachrichtentypen) die Semantik der Nachrichtentypen und der Felder (die im übrigen nur für das Protokoll von Belang sind) und die Regeln wann und wie ein Prozess Nachrichten eines bestimmten Typs sendet bzw. wie er auf diese Nachrichten reagiert (beispielsweise durch das Senden von Antwortnachrichten oder Ausführen von Programmen wie z.B. Lesen oder Schreiben von Dateien). Vereinfacht kann man sagen: Protokolle sind Regeln die das Format den Inhalt von Feldern und seine Bedeutung sowie die Reihenfolge gesendeter Nachrichten festlegen. Um die Komplexität überschaubar zu halten organisieren Netzwerkdesigner Protokolle sowie die Netzwerkhardware und -software mit der die Protokolle implementiert werden in Schichten. Bei einer geschichteten Protokollarchitektur gehört jedes Protokoll zu einer bestimmten Schicht. Ein Protokoll der Schicht \(n\) definiert also genau die Regeln mit denen zwei Endsysteme Nachrichten auf der Schicht \(n\) austauschen. Tatsächlich werden Daten nicht direkt von Schicht \(n\) eines Rechners zu Schicht \(n\) eines anderen übertragen. Vielmehr nutzt eine Schicht den Dienst der direkt darunter liegenden Schicht den es über einen Dienstzugangspunkt erreicht der auch als Schnittstelle (Interface) bezeichnet wird. Nur in der untersten Schicht findet die eigentliche Kommunikation statt die Nachrichtenübertragung. Ein Dienst der der direkt darüber liegenden Schicht angeboten wird ist eine Menge von Programmen oder Operationen die Aktionen innerhalb der Schicht definieren und die wiederum Dienste der direkt darunter liegenden Schicht verwenden. Die Definition vom Dienst sagt nur was die Schicht leistet sie sagt aber nicht wie die direkt darüber liegenden Schicht auf den Dienst zugreift und wie die Schicht arbeitet. Die Schnittstelle zwischen zwei benachbarten Schichten definiert wie die obere Schicht auf den Dienst der unteren zugreift. Ein Protokoll einer Schicht implementiert einen Dienst der Schicht. Zur Verdeutlichung wollen wir die Kommunikation zwischen zwei Unternehmen per Brief als vereinfachte dreischichtige Architektur betrachten siehe Abbildung 3.3. Abbildung 3.3: Briefkommunikation als vereinfachte geschichtete Architektur. Die oberste Schicht bezeichnen wir als Schreib/Leseschicht die mittlere als Briefkommunikationsschicht und die untere als Posttransportschicht. Herr Müller möchte Frau Schmidt eine Nachricht zukommen lassen. Er schreibt sie auf ein Blatt Papier und legt es in den Postkorb seines Sekretärs und versieht ihn mit dem Empfänger Frau Schmidt. Dieser steckt das Blatt in einen Briefumschlag findet die Adresse von Frau Schmidt heraus und schreibt sie darauf. Dann frankiert er den Brief und übergibt ihn durch Einwurf in den gelben Postbriefkasten an die Posttransportschicht. Die Post sorgt für die Leerung des Postbriefkastens versieht dem Brief mit einem maschinenlesbaren Strichcode und transportiert ihn zum Hausbriefkasten des Empfängers. Die Sekretärin von Frau Schmidt öffnet den Brief und legt ihn in die Postmappe die Frau Schmidt zu bearbeiten hat. Die Dienstzugangspunkte sind hierbei der Postkorb und die Postmappe sowie der gelbe Postbriefkasten und der Hausbriefkasten. Auf der Briefkommunikationsschicht besteht der Dienst darin dass der Sekretär einen Brief zur Post bringt und die Sekretärin ihn an Frau Schmidt ausliefert. Wie der Sekretär die Adresse von Frau Schmidt findet wie er adressiert frankiert und den Brief zur Post bringt und wie die Sekretärin ihn aufmacht dazu macht der Dienst keine Angabe. Der Dienst auf der Posttransportschicht besteht darin einen adressierten und frankierten Brief bei seinem Empfänger abzuliefern. Er sagt aber nichts darüber wie der Brief befördert wird. Jede Schnittstelle definiert wie der Dienst der direkt darunter liegenden Schicht benutzt wird. Das Protokoll der Schreib/Leseschicht ist die geschriebene Sprache das Protokoll der Briefkommunikationsschicht regelt das Einstecken des Briefs in einen genormten Umschlag sowie die Adressierung und Frankierung beim Absender und das Öffnen beim Empfänger. Bei der Posttransportschicht regelt das Protokoll wie die Beförderung von Briefen organisiert wird. In unserem Beispiel sieht man dass jede Schicht die Nachrichten für die untergeordnete Schicht aufbereiten kann. Dabei muss allerdings sicher gestellt werden dass die Inhalte der Nachrichten nämlich die Daten die von der oberen Schicht übergeben wurden nicht verändert werden. Eine Schicht darf einzig die Daten die an ihrem Dienstzugangspunkt eintreffen mit Headern und Trailern versehen (sozusagen umklammern). Die Header und Trailer fügen der Nachricht Informationen für den Empfänger auf derselben Schicht hinzu. In unserem Beispiel sind Header und Trailer in der Briefkommunikationsschicht der Briefumschlag mit der Adresse des Empfängers und in der Posttransportschicht der Strichcode. Außerdem enthält der Header eine Briefmarke. Als Trailer könnte man die Absenderadresse interpretieren. Außer dem Hinzufügen von Headern und Trailern kann eine Schicht die ihr übergebenen Daten noch zu größeren Paketen zusammenfassen oder in kleinere Pakete aufteilen. Nehmen wir an dass in der Posttransportschicht nur Standardbriefe zu 20 g übertragen werden können. Das stellt die Posttransportschicht dadurch sicher dass zu schwere Briefe nicht ihren Dienstzugangspunkt passieren dürfen. Wenn nun der Briefkommunikationsschicht als Daten die Inhalte des Buches Computernetze übergeben werden so stellt das die Briefkommunikationsschicht vor ein Problem: das Buch wiegt mit 1295 g (in der Auflage von 2002) erwiesenermaßen mehr als 20 g. Was kann die Briefkommunikationsschicht nun tun damit sie das Buch doch über die Posttransportschicht versenden kann? Die Antwort ist unter diesen Bedingungen relativ simpel: Die Briefkommunikationsschicht (der Sekretär von Herrn Müller) teilt das Buch in Einheiten zu 20 g auf und schickt jede dieser 65 Einheiten einzeln auf den Weg. Die einzelnen Teile werden noch mit fortlaufenden Nummern versehen auf Basis derer die Empfängerseite d.h. die Sekretärin von Frau Schmidt das Buch wieder zusammensetzen kann. In unserem Beispiel sehen wir auch dass die Einzelheiten wie Briefe durch die Post gesammelt transportiert und zugestellt werden uninteressant für die Benutzer der Post sind. Das Konzept der Schichten vereinfacht die Realisierung eines großen und komplexen Systems denn es ist leichter die Realisierung des von einer Schicht bereitgestellten Dienstes zu ändern. Solange die Schicht den gleichen Dienst für die darüber liegende Schicht bereitstellt und die gleichen Dienste von der darunter liegenden Schicht benutzt bleibt der Rest des Systems unverändert. Wenn z.B. die Post ihre Transportmethoden ändert bleibt der Rest des Briefkommunikationssystems unverändert. Ein Dienst wird durch eine Menge von Programmen oder Operationen definiert die eine Schicht der darüber liegenden Schicht zur Verfügung stellt dieses Konzept nennen wir das Dienstmodell. Ein Dienst sagt aber nichts darüber wie diese Operationen implementiert (realisiert) werden. Die Implementierung der Dienste einer Schicht benutzt aber die Dienste der darunter liegenden Schicht. Im Beispiel wird in der Briefkommunikationsschicht der Dienst angeboten dass ein Brief aus einem Postkorb in einer Postmappe landet. In der Transportschicht wird der Brief vom gelben Briefkasten zum Hausbriefkasten befördert. Ein Protokoll ist eine Menge von Regeln die Format und Bedeutung der innerhalb einer Schicht ausgetauschten Nachrichten festlegt. Ein Protokoll dient der Erbringung eines gewünschten Dienstes auf der Basis der Nutzung der untergeordneten vorhandenen Dienste. Die Einheiten der kommunizierenden Seiten erbringen mit Protokollen ihre definierten Dienste z.B. erbringen die Quell- und Zielpostämter das Befördern der Briefe. Man kann die Protokolle verändern solange dies nichts an den für die Dienstbenutzer sichtbaren Diensten ändert. Ein Protokoll der Schicht \(n\) wird auf die Netzwerkeinheiten verteilt wir nennen diese Netzwerkeinheiten Quellhost und Zielhost. Die Prozesse die auf dem Quellhost und Zielhost laufen kommunizieren miteinander durch Austausch von Schicht-\(n\)-Nachrichten. Diese Nachrichten nennt man Protokolldateneinheiten (Protocol Data Units PDUs) der Schicht \(n\) bzw. \(n\)-PDUs. Das Schicht-\(n\)-Protokoll definiert den Inhalt und das Format einer \(n\)-PDU sowie die Art in der die \(n\)-PDUs zwischen den Netzwerkeinheiten ausgetauscht werden. Zusammengenommen werden die Protokolle der verschiedenen Schichten als Protokollstapel (Protocol Stack) bezeichnet da sie aufeinander aufbauen. Wir betrachten nun ein Beispiel eines Netzwerks das seine Kommunikationsprotokolle in vier Schichten organisiert. Abbildung 3.4 zeigt dass die Anwendung die auf der höchsten Schicht (hier also Schicht 4) läuft eine Nachricht \(M\) erzeugt. Jede auf dieser höchsten Schicht erzeugte Nachricht ist eine 4-PDU. Die Nachricht \(M\) selbst kann weiter strukturiert sein so wie eine Struktur in einer Programmiersprache verschiedene Felder enthalten kann z.B. ein Record in Pascal ein Struct in C oder eine Instanz einer Klasse in Java. Die Definition und Interpretation der Felder einer Nachricht sind Sache der Anwendung. Abbildung 3.4: Verschiedene PDUs auf unterschiedlichen Schichten der Protokollarchitektur. Im Quellhost wird die Nachricht \(M\) im Protokollstapel von der Schicht 4 nach unten an die Schicht 3 weitergegeben. Die Schicht 3 im Quellhost teilt die 4-PDU M in zwei Teile \(M_1\) und \(M_2\) auf und fügt dann zu \(M_1\) und \(M_2\) sogenannte Header1 hinzu um zwei vollständige 3-PDUs zu erzeugen. Die Header enthalten hier die zusätzlichen Informationen um es dem Empfänger in Schicht 3 zu ermöglichen bei Empfang von \(M_1\) und \(M_2\) diese wieder zu \(M\) zusammensetzen zu können.2 In der Quelle wird auf jeder Schicht ein weiterer Header hinzugefügt bis eine Menge von 1-PDUs fertig ist. Diese 1-PDUs werden vom Quellhost auf einer physikalischen Leitung abgeschickt. Am anderen Ende empfängt der Zielhost 1-PDUs und leitet sie im Protokollstapel nach oben weiter. Auf jeder Schicht wird der entsprechende Header entfernt und die Nachrichten falls notwendig zusammengefügt und an die obere Schicht als PDU dieser Schicht weitergereicht. Schließlich wird \(M\) aus \(M_1\) und \(M_2\) wieder zusammengesetzt und an die Anwendung weitergeleitet. Eine Nachricht kann während ihres Transportes in tieferen Schichten nicht nur zerlegt (disassembliert) und im Zielsystem wieder zusammengesetzt (reassembliert) werden. Ebenso können verschiedene Nachrichten oder Pakete verkettet und wieder getrennt werden. Ein Beispiel für das Zusammenfassen der Pakete finden wir auf einer Schicht unterhalb der Posttransportschicht. Wenn der Brief sich seinen Weg durch die Verteilzentren sucht so wird er meist anhand der Postleitzahl zu großen Einheiten in Form von Postsäcken zusammengefasst die dann gemeinsam durch eine unterliegende Schicht beispielsweise die Bahn transportiert werden. Die Rollen sind hier genau gegensätzlich zum vorangegangenen Beispiel: Der Sender fasst viele kleine ihm übergebene Pakete zu einem großen Paket zusammen. Dabei muss er darauf achten dass die einzelnen Teilpakete voneinander getrennt im großen Paket liegen. Der Sender übergibt das so entstandene Paket an die untergeordnete Schicht. Der Empfänger erhält von der untergeordneten Schicht das große Paket und trennt es wieder in die einzelnen Teilpakete auf. Aufbau eines Computernetzwerks am Beispiel des Internets Die Netzwerkanwendungen laufen auf den Endsystemen den sogenannten Hosts. Die Endsysteme sind über Kommunikationsleitungen verbunden. Als physische Medien für Kommunikationsleitungen kommen zum Beispiel geführte Medien wie Koaxialkabel Kupferkabel und Glasfaser oder ungeführte Medien wie Funkwellen im Raum zum Einsatz. Eine wichtige Kenngröße einer Leitung ist die Übertragungsrate die man auch als Bandbreite bezeichnet und in Bit/Sekunde (bps bits per second) misst. In diesem Zusammenhang verwenden wir das Dezimalsystem statt Binärsystem und es gilt: \( 1 Kbps = 10^{3} bps 1 Mbps = 10^{6} bps und 1 Gbps = 10^{9} bps \) da es um die Geschwindigkeit der Übertragung und nicht um Speichergrößen geht. In einem größeren Computernetzwerk sind die Endsysteme nicht direkt miteinander verbunden. Zwischen ihnen liegen weitere Datenverarbeitungsgeräte sogenannte Transitsysteme. Solch ein Transitsystem ist im Internet z.B. der Router. Ein Router nimmt Informationen von einer Eingangsleitung an und sendet sie über eine seiner ausgehenden Kommunikationsleitungen weiter. Das Internetprotokoll (Internet Protocol IP) spezifiziert das Format in dem im Internet Informationen zwischen Routern und Endsystemen ausgetauscht werden. Der Weg den die Information durch das Rechnernetz von einem Endgerät über verschiedene Transitsysteme zum anderen Endgerät nimmt bezeichnet man als Route oder Pfad. Das Internet ist ein Verbund aus privaten und öffentlichen Netzwerken. Deshalb wird es auch als Netzwerk aus Netzwerken bezeichnet. Jedes an das Internet angeschlossene Netzwerk muss das IP ausführen und bestimmte Namens- und Adresskonventionen einhalten genaueres dazu werden wir vor allem in Kurseinheit 4 kennen lernen. Das wichtigste ist dass das IP den Zusammenschluss von verschiedenen Netzwerken zu einem einzigen logischen Netzwerk dem sogenannten Internet macht. Das Internet verwendet eine hierarchische Topologie. Endsysteme sind über Zugangsnetzwerke an lokale Internet-Service-Provider (ISPs) angeschlossen siehe Abbildung 3.5. Ein Zugangsnetzwerk kann ein lokales Netzwerk (LAN) innerhalb eines Unternehmens oder z.B. einer Hochschule eine Wählverbindung über Telefonleitung (Modem ISDN oder DSL) oder ein Zugangsnetzwerk für mobile Endgeräte sein. Abbildung 3.5: Ein Ausschnitt aus dem Internet: einige Heimcomputer und ein Firmennetzwerk sind über ISPs an das Internet angeschlossen. Lokale ISPs sind ihrerseits mit regionalen ISPs verbunden die wiederum mit nationalen und internationalen ISPs verbunden sind. Die nationalen und internationalen ISPs sind untereinander auf der höchsten Ebene der Hierarchie verbunden. Neue Netzwerke können jederzeit an die bestehende Konstruktion angefügt werden. Die Basis des Internets sind Internet-Standards die von der IETF (Internet Engineering Task Force) entwickelt werden. Sie bilden die Grundlage dafür dass verschiedene Geräte und Netzwerke miteinander kommunizieren können. Die Standardisierungsdokumente nennt man RFCs (Request for Comments). Sie sind keine formellen Standards werden aber als solche betrachtet. Heute gibt es über 8000 RFCs. Alle RFCs lassen sich im Internet unter der Adresse http://www.ietf.org/rfc.html abrufen. Bei den Endsystemen d.h. den Geräten die am äußeren Rand des Internets angekoppelt sind unterscheidet man gerne Clients und Server. Mit Server bezeichnet man alle Rechner die Dienstleistungen für andere Rechner anbieten und mit Clients solche die diese Dienste in Anspruch nehmen. Oft sind Server leistungsstärkere Maschinen die an einem zentralen Standort aufgestellt und von einem Systemadministrator verwaltet werden während Clients eher auf den Schreibtischen oder in den Taschen der Benutzer anzutreffen sind. Diese Unterscheidung ist aber ganz und gar nicht eindeutig weil auch der schwächste Schreibtischrechner als Server einen Dienst anbieten kann und Server-Maschinen häufig als Clients auf die Dienste anderer Server zugreifen. Das Internet bietet verteilten Anwendungen auf den Endsystemen zwei Dienste realisiert durch die Protokolle TCP [1] beziehungsweise UDP [2]. Während IP zur Weiterleitung von Informationen zwischen Endsystemen und Routern dient leisten TCP oder UDP die Weiterleitung von Informationen zwischen einzelnen Prozessen die auf den Endsystemen laufen. TCP und UDP benutzen dazu das IP. Sie liegen also auf einer höheren Schicht als das IP. Betrachten wir nun den Netzwerkkern d.h. das Internet ohne die Endgeräte. Der Netzwerkkern besteht aus einer Menge verbundener Router welche die Endsysteme miteinander verbinden. Für den Aufbau eines Netzwerkkerns existieren zwei prinzipiell verschiedene Ansätze: Leitungsvermittlung und Paketvermittlung. In leitungsvermittelten Netzen wird für jede Kommunikationsanforderung eine dauerhafte Verbindung aufgebaut. Dazu werden die auf einem Pfad vom Sender zum Empfänger zur Kommunikation zwischen den Endsystemen benötigten Ressourcen (Puffer Leitungsbandbreite) für die Dauer der Sitzung reserviert d.h. für den Zeitraum in dem Sender und Empfänger Nachrichten austauschen wollen. Ein Beispiel dafür ist das alte analoge Telefonnetz hier wurde für jedes Telefongespräch eine Leitungsverbindung geschaltet die erst bei Beendigung des Gesprächs wieder abgebaut wurde. In paketvermittelten Netzen werden diese Ressourcen nicht reserviert. Jede Nachricht die in einer Sitzung gesendet wird verwendet die Ressourcen nach Bedarf. Es kann daher vorkommen dass Nachrichten oder Teile davon auf den Zugriff auf eine Verbindungsleitung warten müssen. Sie stehen dann z.B. in einer Warteschlange an. Das Internet ist ein paketvermitteltes Netz. Die Paketvermittlung erlaubt die gleichzeitige Nutzung von Pfaden oder Teilpfaden durch mehrere Endsysteme. In modernen paketvermittelten Netzwerken schickt der Quellhost auf dem der Sender-Prozess läuft lange Nachrichten als Folge kleiner Pakete. Dazu teilt er die Nachricht in Teile und gibt diese an die darunter liegende Schicht weiter (vgl.Abschnitt 3.5). Übungsaufgabe 3.1 Recherchieren Sie warum das Internet als ein paketvermitteltes Netz konzipiert worden ist. Aufgabe jetzt bearbeiten Warum aber werden die Nachrichten in kleine Pakete aufgeteilt und beim Empfänger wieder zusammengesetzt kann man nicht jede Nachricht gleich als Ganzes versenden? Diese besonders einfache Form der Paketvermittlung wird als Nachrichtenvermittlung bezeichnet sie hat allerdings gravierende Nachteile: Damit die Nachricht bei der Übertragung immer intakt bleibt muss jede Zwischenstation zuerst die komplette Nachricht empfangen und zwischenspeichern bis sie komplett vorhanden ist bevor sie sie an die nächste Station weiterleitet außerdem muss bei Übertragungsfehlern immer gleich die ganze Nachricht wiederholt übertragen werden. Wird sie aber in kleine Pakete zerlegt dann sind viele Zwischenstationen parallel mit der Übertragung beschäftigt und bei Übertragungsfehlern wird nur das fehlerhafte Paket wiederholt beides führt zu viel größerem Durchsatz. Dem gegenüber steht der Nachteil von kleinen Paketen dass der Umfang der Header-Information für ein Paket oder eine Nachricht etwa gleich ist (Identität von Sender und Empfänger Paket- oder Nachrichtenidentifizierung). Damit ergibt sich bei der Paketvermittlung gegenüber der Nachrichtenvermittlung ein höherer Header-Overhead3 pro Datenbyte. Paketvermittelte Netze werden in zwei Klassen unterteilt: Datagramm-Netzwerke und VC-Netzwerke (Virtual Channels virtuelle Kanäle). Sie unterscheiden sich dadurch dass sie Pakete entweder anhand von Hostzieladressen oder anhand von virtuellen Kanalnummern weiterleiten. Das Internet ist ein Datagramm-Netzwerk da das IP die Pakete anhand von Zieladressen weiterleitet. X.25 [3] Frame-Relay [4] und ATM (Asynchronous Transfer Mode) sind Beispiele für Paketvermittlungstechnologien die virtuelle Kanäle verwenden. In einem Datagramm-Netzwerk startet ein Paket in einem Sender-Host anschließend fließt es durch eine Reihe von Routern und endet in dem Empfänger-Host. Ein Router kann erst dann mit dem Versenden eines Pakets beginnen wenn er das ganze Paket empfangen hat dieses Verfahren nennt man Speichervermittlung (Store-and-Forward). Auf dem Weg von einem Router zum nächsten unterliegt ein Paket verschiedenen Arten von Verzögerungen: Verarbeitungsverzögerung: Die Zeit die ein Router für die Feststellung benötigt wohin das Paket weiterzuleiten ist ggfs.auch die Zeit um Bitfehler zu entdecken die durch Störungen auf der Leitung bei der Übertragung auftreten können. Warteschlangenverzögerung: Die Zeit die das Paket in einer Warteschlange eines Routers verbringt um auf die Übertragung auf einer ausgehenden Verbindungsleitung zu warten. allgindex Übertragungsverzögerung: Die Zeit die für die Einstellung aller Paketbits in das Übertragungsmedium erforderlich ist d.h. Größe des Pakets in Bit geteilt durch die Übertragungsrate der Kommunikationsschnittstelle zu dem physischen Medium in bps. allgindex Ausbreitungsverzögerung: Die Zeit die ein Bit braucht um sich von Router \(A\) bis zum nächsten Router \(B\) auszubreiten nachdem es auf die Verbindungsleitung befördert wurde. Die Ausbreitungsverzögerung von \(A\) nach \(B\) ist die Entfernung zwischen \(A\) und \(B\) geteilt durch die Ausbreitungsgeschwindigkeit die vom physischen Medium der Verbindungsleitung abhängt. Zusammen bilden diese Verzögerungen die Gesamtverzögerung des Routers für die Übertragung eines Pakets zum nächsten Router. Die Verzögerung auf einem Pfad errechnet sich als die Summe der Verzögerungen in allen Routern auf dem Pfad. Die Summe aus Verarbeitungs- Warteschlangen- Übertragungs- und Ausbreitungsverzögerungen eines Pakets auf dem Pfad von einem Sender zu einem Empfänger bezeichnet man als Ende-zu-Ende-Verzögerung des Pakets. Bei Überlastung eines Netzes kann es vorkommen dass für ein Paket kein Speicherplatz in der Warteschlange eines Routers vorhanden ist. Dann verwirft der Router das Paket. Das Paket geht damit verloren. Der Anteil verlorener Pakete erhöht sich mit zunehmender Verkehrsintensität. Aus diesem Grund wird die Leistung an einem Router oft nicht in Bezug auf die Verzögerung sondern in Bezug auf die Wahrscheinlichkeit von Paketverlusten gemessen. Übungsaufgabe 3.2 Man betrachte eine Nachricht die \(7 5\) Mbit groß ist. Angenommen zwischen Quelle und Ziel befinden sich zwei Router und drei Verbindungsleitungen und jede Verbindungsleitung hat eine Übertragungsrate von \(1 5\) Mbps. Weiter ignorieren wir die Verarbeitungs- Warteschlangen- und Ausbreitungsverzögerung. Welche Ende-zu-Ende-Verzögerung ergibt sich wenn die Nachricht mit der Nachrichtenvermittlung befördert wird also durch Versenden der \(7 5\) Mbit langen Nachricht im Ganzen? wenn die Nachricht in kleine Pakete mit einer Länge von je \(1 5\) Kbit übertragen wird? Aufgabe jetzt bearbeiten Das Internet-Schichtenmodell Der Internetprotokollstapel besteht aus fünf Schichten 4 siehe Abbildung 3.6. Die PDUs auf jeder Schicht haben einen eigenen Namen. Abbildung 3.6: Der Internet-Protokollstapel PDUs Protokolle und Kommunikationspartner. Die allgindex Anwendungsschicht ist zuständig für die Unterstützung von Netzwerkanwendungen. Sie beinhaltet viele Protokolle darunter HTTP [6][7] für das Web SMTP [8] für E-Mail und FTP [9] für Filetransfer. In der Internet-Terminologie wird sie auch als Verarbeitungsschicht bezeichnet. Die Transportschicht stellt einen Kommunikationsdienst zwischen zwei Anwendungsprozessen bereit und ermöglicht so eine Endpunkt-zu-Endpunkt-Kommunikation von Quell-Host zu Ziel-Host. Es gibt zwei Übertragungsprotokolle: TCP und UDP. TCP bietet einen zuverlässigen verbindungsorientierten Dienst über das ein Bytestrom von einem Rechner im Internet fehlerfrei einem anderen Rechner zugestellt wird. UDP bietet einen unzuverlässigen verbindungslosen Dienst. Auch die Vermittlungsschicht bietet einen Endpunkt-zu-Endpunkt-Kommunikationsdienst. Sie transportiert Datagramme vom Quellhost zum Zielhost. Die Vermittlungsschicht des Internets besteht aus zwei Hauptkomponenten: dem IP und vielen Routing-Protokollen. Das IP definiert das Format eines IP-Datagramms. Die Routing-Protokolle bestimmen welche Routen die Datagramme zwischen Quelle und Ziel nehmen. Innerhalb eines Netzwerks kann der Netzwerkadministrator jedes beliebige Routing-Protokoll benutzen. Die Vermittlungsschicht die das IP und viele Routing-Protokolle enthält wird auch als IP-Schicht oder Internet-Schicht bezeichnet. Die Sicherungsschicht ist für die Beförderung ganzer Rahmen von einem Knoten (Host oder Paket-Switch) zu einem benachbarten Knoten zuständig die physikalisch über einen Übertragungskanal z.B. ein Kabel eine Telefonleitung oder eine Punkt-zu-Punkt-Funkverbindung miteinander verbunden sind. Der auf der Sicherungsschicht bereitgestellte Dienst hängt von dem spezifischen Sicherungsschichtprotokoll ab. Ethernet [10] und PPP (Point-to-Point Protocol) [11][12] und in gewissem Umfang auch ATM und Frame-Relay sind Beispiele für die Sicherungsschicht. Demgegenüber überträgt die Bitübertragungsschicht die einzelnen Bits der 1-PDU von einem Router zum nächsten. Die Protokolle dieser Schicht hängen wesentlich von der Verbindungsleitung und vom Übertragungsmedium der Verbindungsleitung ab. Ethernet benutzt z.B. viele Protokolle für die Bitübertragungsschicht: eines für Twisted-Pair Kabel eines für Koaxialkabel eines für Glasfaser usw. Je nach Medium wird ein Bit unterschiedlich auf der Verbindungsleitung übertragen. Der Begriff Netzwerkschnittstellenschicht (Network Interface Layer) fasst die Bitübertagungsschicht und die Sicherungsschicht zusammen. Diese Schicht wird auch als Host-an-Netz-Schicht (Host-to-Net Layer) bezeichnet. Netzwerkeinheiten und Schichten im Zusammenspiel am Beispiel des Internets Endsysteme und Transitsysteme sind die wichtigsten Netzwerkeinheiten. Ein Beispiel für eine Art von Transitsystem im Internet sind die Router. Außerdem finden sich im Internet noch Switches oder Bridges. Beide Typen von Geräten sind Arten von Paket-Switches. Router und Switches (Bridges) unterscheiden sich von Hosts dadurch dass sie nicht alle Schichten des Protokollstapels implementieren: Switches und Bridges implementieren nur die Schichten 1 und 2 während Router die Schichten 1 bis 3 implementieren. Dagegen implementieren Hosts alle fünf Schichten. Die reduzierte Komplexität von Routern und Switches (Bridges) hängt mit dem Bestreben zusammen einen großen Teil der Komplexität der Internet-Architektur auf die Ränder des Netzwerks zu legen. Das Weiterleiten einer Nachricht von einem Endsystem zum anderen über eine Bridge und einen Router wird in Abbildung 3.7 illustriert. Abbildung 3.7: Informationsfluss zwischen zwei Endsystemen über Router und Bridge im Internet. Das ISO/OSI-Referenzmodell Von historischer Bedeutung ist das ISO/OSI-Referenzmodell [13][14]. Anfang der 80er Jahre entstand die internationale Norm ISO/IS 7498 mit dem Titel Information Processing Systems: Open Systems Interconnection - Basic Reference Model (ISO-RM). Hier wurde ein Rahmen geschaffen um die Kommunikation zwischen Computersystemen zu standardisieren. Das ISO/OSI-Referenzmodell definiert sieben Schichten. Abbildung 3.1 zeigt einen möglichen Informationsfluss und Protokollstapel mit Dienstzugangspunkten anhand dieses Modells. Das Referenzmodell schreibt keine Implementierung dieser Schichten vor sondern beschreibt zu jeder Schicht ihre allgemeinen Aufgaben ihre der nächsthöheren Schicht bereitzustellenden Dienste und die in ihr zu realisierenden Funktionen. Das Modell bildet damit ein Rahmenwerk zur Implementierung von Protokollen. Abbildung 3.1: Möglicher Informationsfluss zwischen zwei Endsystemen über zwei Transitsysteme gemäß dem ISO/OSI-Referenzmodell Die sieben Schichten lassen sich wie folgt charakterisieren: Schicht 1: Bitübertragungsschicht (physical layer): stellt physikalische Übertragungskanäle zur Verfügung die es gestatten beliebige Bitfolgen zu übertragen. Schicht 2: Sicherungsschicht (data link layer): stellt weitgehend sichere Übertragungskanäle für die Übertragung von Datenblöcken zur Verfügung. Hierzu werden Verfahren zur Fehlererkennung und Fehlerkorrektur eingesetzt. Schicht 3: Vermittlungsschicht (network layer): stellt logische Übertragungskanäle zwischen Endsystemen zur Verfügung. Ein Beispiel ist das IP im Internet. Schicht 4: Transportschicht (transport layer): stellt logische Übertragungskanäle zwischen den auf den Endsystemen aktiven miteinander kommunizierenden Anwendungsprozessen zur Verfügung. Beispiele sind die schon kurz angesprochenen Protokolle TCP und UDP im Internet. Schicht 5: Kommunikationssteuerungsschicht oder auch Sitzungsschicht (session layer): stellt Dienste zur Verfügung die es den kommunizierenden Prozessen erlauben ihren Dialog zu kontrollieren und zu synchronisieren. Schicht 6: Darstellungsschicht (presentation layer): stellt Dienste zur Verfügung die die Darstellung von Daten in unterschiedlichen Repräsentationen (z.B. zwischen verschiedenen Zeichencodierungen) umwandeln. Schicht 7: Anwendungsschicht (application layer): implementiert die eigentliche Anwendungsfunktionalität. Die Dienste der unteren drei Schichten ermöglichen den Transport von Daten zwischen Endsystemen während die Dienste der oberen vier Schichten für eine reibungslosen Verlauf des Dialogs zwischen den Anwendungsprozessen zuständig sind. Im Vergleich zu diesen sieben Schichten besteht das Internet nur aus fünf Schichten siehe Abbildung 3.6 und 3.7. Die Dienste und Funktionalitäten der Schichten 5 und 6 des ISO/OSI-Modells sind bereits so anwendungsspezifisch dass sie in der Regel von den Anwendungsprozessen implementiert werden. So findet sich die Funktionalität der Schicht 6 heutzutage am ehesten in Object Brokern.5 In diesem Kurs folgt die Darstellung der Konzepte der einzelnen Protokollschichten deshalb dem fünfschichtigen Internet-Schichtenmodell beginnend mit der Anwendungsschicht.  
2; mod_longpage;185; 3; 3; 3.3 Anwendungsschicht Rechnernetze dienen zur Realisierung verteilter Anwendungen. Allein durch den Anwendungszweck begründet sich die Notwendigkeit von Rechnernetzen. Die Anwendungsprozesse auf den unterschiedlichen Endsystemen kommunizieren miteinander durch den Austausch von Nachrichten über das Computernetzwerk. Die Protokolle auf der Anwendungsschicht legen das Format und die Reihenfolge der ausgetauschten Nachrichten fest und definieren die aus der Übertragung oder dem Empfang einer Nachricht resultierenden Konsequenzen. Bei verteilten Anwendungen muss man die Netzwerkanwendungen von den Protokollen auf der Anwendungsschicht unterscheiden. Die gebräuchlichste Netzwerkanwendung heutzutage ist wohl das World Wide Web (WWW).6 Diese Anwendung erlaubt das Holen von Dokumenten die auf anderen Endsystemen liegen. Um dieses zu leisten besteht das WWW aus mehreren Komponenten: Web-Browser als Benutzerschnittstelle z.B. Firefox Google Chrome Safari Microsoft Edge Opera etc. HTML (HyperText Markup Language) [17] einem Standard für Dokumentenformate damit der Browser die Dokumente anzeigen kann Web-Servern die die Dokumente bereithalten z.B. Apache- Microsoft- Netscape-Server und HTTP (HyperText Transfer Protocol [6][7]) einem Protokoll der Anwendungsschicht. HTTP definiert die Nachrichtentypen und die Art der Weiterleitung von Nachrichten zwischen Web-Browsern und Web-Servern. Es ist der Grund warum Browser und Server sich weltweit verstehen. Das Protokoll ist in den RFC 1945 und 2616 [6][18] standardisiert und öffentlich zugänglich: Jeder Web-Browser kann von jedem Web-Server Dokumente anfordern solange er sich an die Konventionen hält die in dem Protokoll festgelegt sind. Umgekehrt kann jeder Web-Server Dokumentanfragen korrekt beantworten solange er sich an die Festlegungen des Protokolls hält.7 Client/Server-Modell Für die Implementierung der Anwendungen brauchen wir eine Architektur die festlegt welche Komponenten es gibt und wie sie miteinander kommunizieren. Bei den modernen Architekturen für Netzwerkanwendungen gibt es das Client/Server-Modell und Peer-to-Peer-Modell. Im Client/Server-Modell werden Prozesse in zwei Gruppen eingeteilt. Es gibt einen Server-Prozess der einen bestimmten Dienst implementiert und bereitstellt. Es gibt viele Client-Prozesse die jweils einen Dienst von dem Server anfordern indem ihm eine Anforderung gesendet und dann auf die Antwort des Servers gewartet wird siehe Abbildung 3.8. Anders ausgedrückt: Der Client ist eine Anwendung die auf einer Maschine des Anwenders läuft Dienste beim Server anfordert und deren Ergebnisse vom Server zurück erhält. Zum Beispiel fordern Web-Clients in der Regel implementiert in Web-Browsern Dokumente beim Web-Server an und stellen diese für ihre Benutzer dar. Der Web-Server unterstützt das Abholen von Dokumenten. Abbildung 3.8: Client-Server Architektur. Der Server kann als fortwährend ablaufender Prozess charakterisiert werden der den folgenden Code ausführt: while true do \(\\) Warte bis Anforderung von einem Client C eintrifft Führe Anforderung aus Sende Antwort an Client C \(\\) Der Client ist üblicherweise in die Anwendung eingebettet. Ein Web-Browser implementiert beispielsweise die Funktionalität von HTTP die ein Client zur Kommunikation mit dem Web-Server benötigt die sogenannte Client-Seite von HTTP. Der Web-Server implementiert analog die Server-Seite von HTTP. Übungsaufgabe 3.3 Warum ist der Server im Client/Server-Modell ein kontinuierlich ablaufender Prozess? Aufgabe jetzt bearbeiten Viele heutige populäre Anwendungen wie z.B. Internet Telefonie IPTV und Filesharing verwenden die Peer-to-Peer-Architektur bei der es keine strenge Trennung von Server und Clients gibt. Die Teilnehmer sind Peers (Gleichgestellte Kollege) und jeder kann einen Dienst anbieten oder nutzen. Protokolle der Anwendungsschicht Dieser Abschnitt stellt exemplarisch verschiedene Protokolle der Anwendungsschicht vor um verschiedene Designprinzipien von Protokollen der Anwendungsschicht zu beleuchten. Wir verwenden dazu standardisierte öffentliche Protokolle 8 siehe Tabelle 3.1. Konzepte und Designentscheidungen existierender Protokolle zu kennen hilft bei der Beurteilung von Protokollen und ist nützlich für den Entwurf und die Implementierung eigener Protokolle. Anwendung Protokoll World Wide Web HTTP [6][18] E-Mail SMTP [8] IMAP [22] POP3 [23] File Transfer FTP [9] News NNTP [24] Remote Login Telnet [25] Streaming Multimedia HTTP RTP[26] RTMP Internet-Telephonie SIP [27] RTP[26] proprietär (z.B. Skype) Tabelle 3.1: Häufig benutzte verteilte Anwendungen mit zugehörigem Protokoll der Anwendungsschicht. Wesentliche Prinzipen von Protokollen der Anwendungsschicht lassen sich am eingangs erwähnten HTTP erklären. Dazu müssen wir zuerst verstehen dass im WWW Objekte d.h. zum Beispiel Webseiten oder Bilder eindeutige Bezeichner haben. Hierfür dient der URL (Uniform Resource Locator). Ein URL identifiziert eine Ressource (z.B. eine Webseite) auf eindeutige Weise. Ein URL besteht aus drei Teilen: dem Protokoll dem DNS-Namen oder der IP-Adresse des Rechners (den eindeutigen Namen des Hosts) auf dem sich die Seite befindet und einem lokalen eindeutigen Namen der Seite. Dazu wird das Namensschema < protocol id> :< protocol specific address> verwendet siehe Tabelle 3.2. Protokoll < protocol id> : < protocol specific address> HTTP http: //< IP adresse> /< lokaler Pfad> SMTP mailto: E-Mail-Adresse FTP ftp: //< IP adresse> /< lokaler Pfad> NTTP news: //< IP adresse> /< Newsgroup-Id> Telnet telnet: //< IP adresse> :< Port Nr.> Lokale Dateien file: < lokaler Pfad> Tabelle 3.2: Unterstützte Protokolle der Anwendungsschicht im Internet. URLs in diesen Formaten können in den meisten Web-Browsern direkt in das Adressfeld eingegeben werden. Übungsaufgabe 3.4 Um eine Webseite über das Internet zu holen muss sie identifiziert werden. Dafür benötigt man die folgenden drei Informationen: Wie heißt die Webseite? Wo befindet sie sich? Wie wird auf sie zugegriffen? Sind diese Informationen in einem URL der Form http:/// für einen Benutzer vorhanden? Aufgabe jetzt bearbeiten HTTP Zunächst betrachten wir nun die Nachrichtenformate von HTTP. HTTP definiert lediglich zwei Nachrichtentypen eine HTTP-Anfragenachricht und eine HTTP-Antwortnachricht. Hier ist eine typische Anfragenachricht: GET /ks/1801/ HTTP/1.1 Host: www.fernuni-hagen.de Connection: close User-agent: Mozilla/4.0 (extra carriage return line feed) Die Anfragenachricht ist ein ASCII-Text der von Menschen gelesen werden kann. Er genügt einem bestimmten Format. Die erste Zeile mit dem GET-Befehl heißt Anfragezeile und hat immer die drei Parameter: Methode (hier GET) Dateiname (hier: /ks/1801/) und HTTP-Version (hier: HTTP/1.1). Die GET-Methode wird benutzt um eine Datei anzufordern die durch /ks/1801/ identifiziert wird. Die drei weiteren Zeilen in der Anfragenachricht sind die Header-Zeilen. Die Header-Zeile Host gibt den Web-Server www.fernuni-hagen.de an auf dem die Datei /ks/1801/ liegt. Alle Zeilen sind durch Carriage Return und Line Feed (Zeilenende) getrennt. Am Ende kommt ein zusätzliches Carriage Return und Line Feed um das Ende der Nachricht zu markieren. Zum Verständnis von Anfragenachrichten ist noch eine Präzisierung bezüglich der Funktionsweise des Webs notwendig: Web-Seiten werden üblicherweise als Dokumente bezeichnet. Diese Seiten bestehen aus Objekten. Ein Objekt ist einfach eine Datei z.B. eine HTML- oder GIF-Datei die mit einem eigenen URL zugänglich ist. Viele Web-Seiten bestehen aus einer Basisdatei die Referenzen auf weitere Objekte z.B. Bilder enthält. Um eine Web-Seite aufzubauen muss ein Browser also gegebenenfalls mehr als ein Objekt von Servern anfordern. Für jedes dieser Objekte muss eine eigene HTTP-Anfragenachricht erzeugt und abgeschickt werden. In obigem Beispiel fordert der Client das Objekt 1801 im Verzeichnis /ks/ vom Web-Server www.fernuni-hagen.de an. Aus der Zeile die die Benutzer zur eindeutigen Identifizierung von Web-Seiten angeben werden für eine HTTP-Anfragenachricht zwei Parameter gewonnen: die Eingabe www.fernuni-hagen.de/ks/1801/ liefert für die Anfragezeile das Dokument /ks/1801/ und für die erste Header-Zeile den Hostnamen www.fernuni-hagen.de. Übungsaufgabe 3.5 Warum taucht im URL http://www.fernuni-hagen.de/ks/1801/ bei der Identifizierung der Webseite 1801 im Verzeichnis /ks/ vom Web-Server www.fernuni-hagen.de nicht die IP-Adresse auf sondern der Name www.fernuni-hagen.de? Aufgabe jetzt bearbeiten Der dritte Parameter HTTP/1.1 der Anfragezeile gibt an mit welcher Version von HTTP der Webbrowser arbeitet9 . Wie Tabelle 3.1 bereits vermuten ließ gibt es offenbar zwei Versionen von HTTP HTTP-Version 1.0 [6] und HTTP-Version 1.1 [18]. Beide Versionen von HTTP benutzen das Protokoll TCP der Transportschicht um Nachrichten auszutauschen. TCP stellt der Anwendung eine TCP-Verbindung zum Partnerprozess zur Verfügung. TCP garantiert für seine TCP-Verbindungen dass Nachrichten zuverlässig beim Empfänger ankommen mehr dazu in Abschnitt 3.4. HTTP-Version 1.0 arbeitet nur mit nicht persistenten Verbindungen. Nicht-persistente Verbindungen werden nach der Lieferung des angefragten Objektes sofort wieder abgebaut: Mit der Anfragenachricht fordert der Client ein bestimmtes Objekt an. Sobald der Server das angefragte Objekt in einer HTTP-Antwortnachricht verpackt also praktisch in den Briefumschlag gesteckt und abgeschickt hat befiehlt er dem TCP die Verbindung nach Ablieferung der Antwortnachricht zu schließen. HTTP-Version 1.1 unterstützt auch persistente Verbindungen.10 Bei persistenten Verbindungen überlässt der Server dem Client die Entscheidung wann die Verbindung wieder abgebaut werden soll. Bei komplexen Web-Seiten kann der Client also eine Verbindung dazu benutzen mehrere Objekte nacheinander von demselben Server zu laden ohne für jedes Objekt eine neue Verbindung aufbauen zu müssen. Der HTTP-Server schließt die Verbindung nur wenn sie für eine bestimmte Zeitdauer nicht benutzt wird. Diese Zeit ist konfigurierbar und heißt Timeout-Intervall. Persistente Verbindungen werden zudem entweder ohne oder mit Pipelining verwendet. Bei persistenten Verbindungen ohne Pipelining wartet der Client auf die Antwort einer Anfrage bevor er die nächste Anfrage an den Server schickt. Deshalb hat der Server für diesen Client keine Aufträge zu erfüllen während das unter Umständen große Antwortobjekt sich auf dem Weg vom Server zum Client befindet (vgl. Verzögerungen in Abschnitt 3.6). Um Server und Netzwerk besser auszulasten und auch um dem Benutzer eines Web-Browsers schnelleren Zugriff auf schon geladene Teile des Dokuments zu geben kann der Client sobald er auf eine Referenz eines Objekt stößt Anfragen an einen Server senden ohne auf vorherige Antworten zu warten. Diese Art der Nutzung der Verbindung nennt man persistente Verbindungen mit Pipelining. In obiger Anfragenachricht wünscht der Client durch die Angabe in der dritten Zeile connection: close ausdrücklich eine nicht persistente Verbindung obwohl HTTP-Version 1.1 persistente Verbindungen unterstützt. Die vierte Zeile gibt den Browser-Typ an in diesem Fall das Mozilla/4.0 ein Netscape-Browser. Dieses kann nützlich sein da manche Server unterschiedliche Versionen eines Objektes für unterschiedliche Browser bereithalten. Alle diese Versionen werden aber nur mit einem URL bezeichnet. Wird dem Server kein Browsertyp mitgeteilt liefert er die auf ihm konfigurierte Default-Version des angefragten Objektes. Eine mögliche Antwort auf obige Anfrage könnte wie folgt aussehen: HTTP/1.1 200 OK Connection: close Date: Thu 08 Aug 2004 12:00:15 GMT Server: Apache/2.0.35 (Unix) Last-Modified: Mon 24 Jun 2004 09:23:24 GMT Content-Length: 483 Content-Type: text/html ... Die erste Zeile die Statuszeile umfasst immer drei Felder: Version (hier HTTP-Version 1.1) Statuscode (200 bedeutet dass alles in Ordnung ist) und Phrase (hier OK: die Anfrage wurde gemäß den Anforderungen abgearbeitet). Danach folgen wieder Header-Zeilen die dem Web-Browser Details über die Lieferung geben: Zeile 3 enthält das Lieferdatum d.h. wann die Antwortnachricht generiert wurde Zeile 4 den Server und die Zeilen 5 bis 7 beschreiben das gelieferte Objekt selbst. Schließlich folgen im sogenannten Entity Body die eigentlichen Daten.11 Ergänzend möchten wir noch auf eine Besonderheit des World Wide Web eingehen nämlich die Web-Caches. Das sind Web-Server die von (lokalen) Organisationen eingesetzt werden um Kopien der zuletzt angeforderten Objekte zu speichern. Die Web-Browser der Benutzer werden so konfiguriert dass sie zunächst beim Web-Cache nach Objekten anfragen. Verfügt der Web-Cache über das Objekt so liefert er es direkt zurück. Andernfalls fungiert der Web-Cache als Web-Client und fordert das Objekt seinerseits vom dem Originalhost12 an hinterlegt eine Kopie und liefert dem Web-Browser das gewünschte Objekt. Web-Caches können also gleichzeitig als Server und Client operieren. Sie werden durch sogenannte Proxy-Server implementiert.13 Die drei häufigsten Gründe für die Einführung von Web-Caches sind: Die Reaktionszeit auf eine Client-Anfrage kann reduziert werden. Dieser Effekt wird noch verstärkt wenn eine Hochgeschwindigkeitsverbindung zwischen Web-Cache und Hosts besteht was in vielen Organisationen der Fall ist. Der Verkehr von einer Institution in das öffentliche Internet kann reduziert werden so dass die Institution mit weniger Bandbreite an das öffentliche Netz angeschlossen sein muss. Ein Host soll nicht im Internet als Client eines Web-Servers erkennbar sein. Er benutzt einen vertrauenswürdigen Proxy-Server so dass der Web-Server nicht erfährt von welchem Host aus die Anfrage wirklich kommt der Client bleibt also dem Server gegenüber anonym. Übungsaufgabe 3.6 Warum bleibt ein Client der über einen Proxy-Server einen Webserver kontaktiert dem Webserver gegenüber anonym? Aufgabe jetzt bearbeiten FTP Weitere Konzepte von Protokollen der Anwendungsschicht lassen sich an FTP dem File Transfer Protocol [9] erklären. Während HTTP alle Informationen die Client und Server zur Kommunikation benötigen über nur eine TCP-Verbindung sendet benutzt FTP zeitweise zwei Verbindungen für die Kommunikation zwischen Client und Server. Mit FTP können Benutzer Dateien von oder zu einem entfernten Host übertragen. Dazu muss der Benutzer sich dem entfernten Host bekannt machen und Zugang erhalten. Hierzu wird eine TCP-Verbindung als sogenannte Steuerverbindung aufgebaut. Die Steuerverbindung bleibt während der gesamten FTP-Sitzung des Benutzers erhalten. Über diese Verbindung kann der Benutzer durch Verzeichnisbäume navigieren und den Transfer von Dateien anstoßen. Wird der Transfer einer Datei angestoßen so wird die Datei aber nicht über die bisherige Verbindung übertragen sondern über eine zweite TCP-Verbindung die Datenverbindung. Sie wird nur für den Transfer dieser einen Datei eingerichtet und danach wieder abgebaut. Man sagt FTP sendet seine Steuerinformation out-of-Band. HTTP ist ein Beispiel für ein Protokoll das seine Steuerinformationen in-Band sendet. Bei HTTP stehen alle Steuerinformationen in den Headerfeldern der Anfrage- und Antwortnachrichten zusammen mit den nachfolgenden Daten. Ein FTP-Server muss zudem über den Zustand aller seiner Benutzersitzungen Buch führen. Insbesondere muss er für jeden Benutzer das aktuelle Verzeichnis verfolgen während die Benutzer durch die entfernten Verzeichnisbäume navigieren und die Benutzer den richtigen Steuerverbindungen zuordnen. Die Gesamtzahl der Sitzungen die ein FTP-Server gleichzeitig unterstützen kann ist hierdurch eingeschränkt. Web-Server hingegen speichern keine Informationen über die Clients die Dienste angefordert haben. HTTP ist deshalb ein sogenanntes zustandsloses Protokoll. E-Mail Das E-Mail-System des Internets basierte ursprünglich nur auf SMTP dem Simple Mail Transfer Protocol [8] das schon seit 1982 existiert und wesentlich älter ist als z.B. HTTP. Dieses SMTP ist ein sehr einfach gehaltenes Protokoll zum Versenden von E-Mails zwischen sogenannten Mail-Servern mittels TCP das nur besonderen Wert auf Zuverlässigkeit bei Ausfall von Leitungen oder Hosts legt an Authentifizierung und ähnlichen Luxus wurde damals jedoch nicht gedacht.14 Auf allen Mail-Servern die heute üblicherweise von ISPs betrieben werden läuft ein besonderer Prozess der Message Transfer Agent (MTA) der E-Mails annimmt (oder die Annahme verweigert) und entweder in lokalen Mailboxen für seine Benutzer abspeichert oder per SMTP an andere Mail-Server weiterleitet. Um es den Endbenutzern möglichst bequem zu machen sind heute weitere Protokolle im Einsatz. Zum Abholen der E-Mails vom Mail-Server auf den eigenen Rechner wird POP3 (Post Office Protocol Version 3 [23]) benutzt zum komfortablen Verwalten von Mailboxen die auf dem Server bleiben dient IMAP (Internet Message Access Protocol [22]). Der Benutzer hat auf seinem Rechner ein meist sehr mächtiges Programm den Mail User Agent (MUA z.B. Mozilla Thunderbird oder Microsoft Outlook) der einmal auf die verwendeten Protokolle und Benutzerdaten eingestellt werden muss. Zum Versenden der E-Mails wird auch vom MUA weiter SMTP benutzt seit einiger Zeit aber mit Authentifizierung. Übungsaufgabe 3.7 Recherchieren Sie welche Vorteile das Protokoll IMAP gegenüber dem Protokoll POP3 für das Abholen von E-Mails hat. Aufgabe jetzt bearbeiten Auch über HTTP also mittels Web-Browser kann man bei vielen ISPs seine E-Mails lesen und schreiben z.B. im Internet-Café und ist damit völlig unabhängig von einem eigenen Host oder MUA. Eine starke Einschränkung von SMTP ist dass die komplette Nachricht nur aus 7-Bit-ASCII-Zeichen bestehen soll dass also noch nicht einmal deutsche Umlaute und andere Sonderzeichen geschweige denn Schriftzeichen aus anderen Zeichensätzen erlaubt sind. Hierfür und für das Versenden von beliebigen Dateien als Mailanhang wurde ein Kodierungsstandard eingeführt an den sich alle MUAs halten sollen die Multipurpose Internet Mail Extensions (MIME siehe [28][29]). Dadurch wird sichergestellt dass zwischen SMTP-Servern nur 7-Bit-Nachrichten ausgetauscht werden der Benutzer aber in seinem MUA die Nachrichten mit allen verwendeten Schriftzeichen und Anhängen betrachten kann. Bemerkenswert bei E-Mail ist dass Mail-Server in der Regel gleichzeitig als Client und als Server agieren: sie senden und empfangen E-Mail für ihre Benutzer. Im Gegensatz zu HTTP ist SMTP ein Push-Protokoll d.h. der sendende Mail-Server schiebt die E-Mail unaufgefordert auf den empfangenden Mail-Server. Ein Web-Server dagegen sendet niemals unaufgefordert Daten. Bei HTTP zieht die Anfragenachricht die Information bei Bedarf vom Web-Server HTTP ist ein Pull-Protokoll. Schnittstelle zur Transportschicht Im vorherigen Abschnitt wurden bereits TCP-Verbindungen angesprochen. TCP ist ein Protokoll der Internet-Transportschicht mit dem Nachrichten zwischen Anwendungsprozessen ausgetauscht werden können. Wie wird dieser Dienst in Anspruch genommen? Hierzu existiert eine Schnittstelle zur Transportschicht die auch als API (Application Programming Interface) zwischen der Anwendung und der Transportschicht bezeichnet wird. Diese Schnittstelle bietet als Dienstzugangspunkt zu dem Dienst einen sogenannten Socket15 . Web-Clients und -Server kommunizieren also indem sie ihre HTTP-Nachrichten an einen Socket übergeben. Das Socket-Interface in einem Client ist die Tür zur TCP-Verbindung zu einem Server der ebenfalls über einen Socket eine Tür zu dieser Verbindung hat. Abbildung 3.9: Anwendungsprozesse Sockets und das zugrundeliegende Transportprotokoll. Neben TCP bietet die Transportschicht im Internet noch ein zweites Protokoll: UDP. UDP wird über eine andere Art von Socket in Anspruch genommen zu den Unterschieden mehr in Abschnitt 3.4. Für die Anwendungsschicht ist wichtig zu bedenken: Während der Anwendungsentwickler die volle Kontrolle über die Anwendungsschichtseite eines Sockets hat hat er kaum Kontrolle über die Transportschichtseite des Sockets siehe Abbildung 3.9. Dem Entwickler bleibt im wesentlichen die Wahl des Transportprotokolls und die Möglichkeit einige Parameter der Transportschicht zu setzen zum Beispiel die maximale Puffer- und Nachrichtengröße. Welche Dienste benötigt eine Anwendung überhaupt von der Transportschicht? Die Anforderungen an einen Dienst lassen sich grob in drei Gruppen einteilen. Sie betreffen die Zuverlässigkeit (Datenverlust) Bandbreite und die Zeit. Wir werden die Punkte kurz beleuchten. Eine vollständige Klassifizierung würde den Rahmen dieses Kurses jedoch sprengen. Anwendungen wie E-Mail Filetransfer oder das Laden von Dokumenten in einen Web-Bowser tolerieren keine Datenverluste. Eine E-Mail sollte beispielsweise vollständig sein wenn der Empfänger sie erhält. Im Gegensatz dazu sind Multimedia-Anwendungen die Daten in Echtzeit übertragen bis zu einem gewissen Grade verlusttolerant. Fehlen bei einer Live-Videoübertragung einige Bilder so kommt es vielleicht zu einigen Ruckeleffekten aber die Videoübertragung selbst ist immer noch nützlich für den Benutzer. Echtzeit Multimedia-Anwendungen benötigen jedoch in der Regel eine gewisse Bandbreite. Wird Sprache für die Internet-Telefonie in einer gewissen Rate kodiert so muss die entsprechende Bandbreite auch zur Verfügung stehen. Andernfalls kann man nicht mehr von einer Echtzeitanwendung sprechen. Ein Telefongespräch ist zum Beispiel nicht mehr möglich wenn die Interaktion der Gesprächteilnehmer zu stark gestört wird. Kann die Anwendung nicht auf platzsparendere Kodierungen für kleinere Bandbreiten ausweichen macht gegebenenfalls die gesamte Anwendung für den Benutzer keinen Sinn mehr. E-Mail Filetransfer und Web-Transfer sind dagegen elastische Anwendungen. Im ungünstigen Fall wird die Geduld der Benutzer strapaziert aber die intendierte Nutzung der übertragenen Daten wird prinzipiell nicht gefährdet. Die Anwendungen profitieren natürlich von mehr Bandbreite. Als dritte Gruppe der Dienstanforderungen lassen sich Anforderungen an die Zeit d.h. akzeptable Ende-zu-Ende-Verzögerungen stellen. Interaktive Echtzeitanwendungen wie die Internet-Telefonie erfordern Ende-zu-Ende-Verzögerungen in einer Größenordnung von ein paar hundert Millisekunden (ms) oder weniger sonst entstünden zu lange Pausen im Gesprächsverlauf. Tabelle 3.3 fasst die Anforderungen einiger Netzwerkanwendungen an die Transportschicht zusammen. Tabelle 3.3: Anforderungen ausgewählter Netzwerkanwendungen an die Transportschicht (basierend auf Abbildung 2.4 aus [30]). Anwendung Datenverlust Bandbreite zeitsensitiv Filetransfer Kein Verlust Elastisch nicht E-Mail Kein Verlust Elastisch nicht Web-Transfer Kein Verlust Elastisch nicht Echtzeitaudio Verlusttolerant wenige Kbps - 1 Mbps einige 100 ms Echtzeitvideo Verlusttolerant 10 Kbps - 5 Mbps einige 100 ms Konsum vongespeichertemAudio/Video Verlusttolerant gleich wieEchtzeitaudio/-video wenigeSekunden Adressierung von Prozessen Damit Prozesse auf verschiedenen Endsystemen miteinander kommunizieren können müssen sie sich gegenseitig kennen. Im Postdienstbeispiel muss der Briefschreiber auf der Briefkommunikationsschicht der Posttransportschicht die Adresse des Adressaten mitteilen. Die Adresse besteht aus dem Namen des Adressaten und einer Beschreibung seines Postkastens. Genauso müssen Netzwerkanwendungen auf der Anwendungsschicht der Transportschicht folgendes mitteilen: eine ID welche die Identität des empfangenden Prozesses auf dem Zielhost bezeichnet und einen Namen oder die Adresse des Hostrechners. Ein empfangender Prozess wird durch eine Portnummer eines Protokolls der Anwendungsschicht identifiziert wenn er gerade eine Anwendung ausführt die auch das Protokoll verwendet. Jedes Protokoll der Anwendungsschicht besitzt eine Portnummer. Sie besteht aus 16 Bit (d.h. eine Zahl von 0 bis 65535). Ein Host hat damit 65536 Ports oder mögliche ansprechbare Prozesse. Die Portnummern 0 bis 1023 sind für bekannte Anwendungsprotokolle reserviert. Der RFC 1700 [31] listet diese bekannten Portnummern (well-known port numbers) auf. HTTP benutzt zum Beispiel Portnummer 80 und FTP Portnummer 21 d.h. den Web-Serverprozess auf einem Host findet man am Port 80 den FTP-Serverprozess am Port 21. Stellt man einen eigenen Serverdienst einer verteilten Anwendung bereit so sollte man eine Portnummer zwischen 1024 und 65535 als Zugang festlegen und in der Benutzerinformation dokumentieren. Der bereitgestellte Dienst ist dann für potentielle Nutzer auffindbar und ansprechbar. Natürlich muss ein Sender-Prozess zur Identifizierung auch eine Portnummer besitzen der zwischen 1024 und 65535 liegt. Hosts werden im Internet durch sogenannte IP-Adressen identifiziert. IP-Adressen sind 32 Bit lang. Die 32 Bit werden als 4 Byte interpretiert die von links nach rechts gelesen eine hierarchische Struktur repräsentieren. IP-Adressen werden dezimal als vier durch Punkte getrennte Zahlen von 0 bis 255 geschrieben beispielsweise 132.176.71.2. Diese Notation heißt auch Punktdezimalnotation (dotted-decimal notation). IP-Adressen sind global eindeutig. Deshalb können diese Adressen nicht vom Host beliebig gewählt werden. Die 32 Bit werden in einen Netzwerk- und einen Host-Teil der IP-Adresse aufgeteilt. Eine häufige Aufteilung ist 16 dass die ersten 24 Bit das Netzwerk bestimmen in dem sich der Host befindet so wie der größte Teil unserer Postadresse durch den Staat den Wohnort und die Strasse feststeht. Lediglich die letzten 8 Bit sind dann der Host-Teil der IP-Adresse (entsprechend der Hausnummer). Das Domain Name System DNS Den meisten Lesern werden die mnemonischen Bezeichnungen für Internet-Hosts wie z.B. www.fernuni-hagen.de geläufiger sein. Diese alphanumerischen Hostnamen variabler Länge sind von Routern schwer zu verarbeiten. Router benutzen deshalb ausschließlich die IP-Adressen zum Weiterleiten von Nachrichten. Das Domain Name System (DNS) des Internets übernimmt die Aufgabe die mnemonische Hostnamen auf IP-Adressen abzubilden. Protokolle der Anwendungsschicht wie HTTP SMTP und FTP benutzen den DNS-Dienst um zu einem vom Benutzer eingegebenen Hostnamen die IP-Adresse zu ermitteln. Dabei ist DNS selbst ein Protokoll der Anwendungsschicht. Übungsaufgabe 3.8 Mit den Kommandos nslookup host oder dig können Sie die IP-Adresse eines Hosts anzeigen lassen. Versuchen Sie die IP-Adressen der folgenden Server zu finden: www.fernuni-hagen.de www.uni-hagen.de mailstore.fernuni-hagen.de Aufgabe jetzt bearbeiten Das DNS realisiert eine verteilte Datenbank die als eine Hierarchie von Name-Servern implementiert ist. Auf der untersten Stufe stehen die lokalen Name-Server die vom ISP betrieben und den Hosts im lokalen Netz jeweils beim Internet-Zugang zugewiesenen werden. Lokale Name-Server versuchen DNS-Anfragen für den anfragenden Host zu erledigen und die endgültige Antwort an ihn zurück zu liefern. Jede Institution die eine Second-Level-Domain wie fernuni-hagen.de betreibt stellt auch mindestens einen zuständigen autoritativen Name-Server zur Verfügung der alle Anfragen zu dieser Domain beantworten kann. Viele Name-Server fungieren gleichzeitig als lokale und autoritative Name-Server. Darüber stehen die Top-Level-Name-Server die für Domains wie .de zuständig sind und insbesondere den jeweils richtigen autoritativen Name-Server einer Institution kennen. Ganz oben in der Hierarchie des Internets gibt es etwa ein Dutzend Root-Name-Server die mindestens alle Top-Level-Name-Server kennen. Alle Name-Server arbeiten mit einem Cache der die Anfragen und Antworten der letzten Zeit speichert so dass die darüber liegenden Server entlastet werden. So wird der lokale Name-Server fast nie die Root-Name-Server konsultieren und auch die Top-Level-Server nur dann wenn nach einer bisher nicht bekannten Subdomain gefragt wird. Der lokale Name-Server arbeitet nach dem Prinzip der rekursiven Namens-Auflösung: er übernimmt die Anfrage eines Hosts erledigt alles Notwendige zur Beantwortung und liefert die endgültige Antwort zurück falls möglich. Die Arbeitsweise der Name-Server auf den höheren Stufen nennen wir dagegen iterative Namens-Auflösung: Sie antworten nur mit dem Verweis auf einen anderen (niedrigeren) Server der die Frage weiterbearbeiten kann. An diesen ist dann dieselbe Anfrage noch einmal zu richten. Das iterative Prinzip ist für die viel beschäftigten Name-Server vorteilhaft weil sie so ohne jede Verzögerung die Anfragen endgültig erledigen können. Abbildung 3.10 zeigt ein Beispiel für eine DNS-Anfrage: Host D benötigt die IP-Adresse von Host E (Schritt 1). Der lokale Name-Server L kann die Anfrage nicht sofort selbst beantworten fungiert deshalb als Client und wendet sich an den Root-Name-Server R der auf den zuständigen Top-Level-Name-Server T verweist. Dieser wiederum kennt den richtigen autoritativen Name-Server A. Also schickt L seine Anfrage ein letztes Mal los dieses Mal an A der endlich die gewünschte IP-Nummer von E liefert. Am Ende erhält D von L die gewünschte IP-Adresse von E (Schritt 8). Abbildung 3.10: Ein Beispiel für eine DNS-Anfrage: Host D benötigt die IP-Adresse von Host E. Hier ist der vollständige Weg über den Root-Name-Server R den Top-Level-Server T und den autoritativen Name-Server A dargestellt in den meisten Fällen ist ein Teil der Information schon im Cache vorhanden und wird deshalb nicht abgefragt. Beispiel einer Client/Server-Anwendung Die bisher betrachteten Konzepte möchten wir in der Praxis an einem Beispiel einer Client/Server-Anwendung demonstrieren. Das Beispiel zeigt wie ein Client- und ein Server-Prozess über TCP-Sockets miteinander kommunizieren. Wir verwenden hier Java weil Java sich als Programmiersprache für die Entwicklung von Netzwerkanwendungen im Internet etabliert hat. Sie müssen Java nicht kennen um das Programm zu verstehen betrachten Sie die Zeilen wie Pseudo-Code. Selbstverständlich kann man das Beispiel in jeder anderen Sprache programmieren in der ein API zu einer Transportschicht existiert. Das Beispielprogramm ist einer älteren Fassung des Buchs von Kurose und Ross [30] entnommen. Es hat folgende Struktur: Ein Client liest eine Zeile von seiner Standardeingabe (Tastatur) und sendet die Zeile über einen Socket an den Server. Der Server liest eine Zeile von seinem Verbindungssocket. Der Server konvertiert die Zeile in Großbuchstaben. Der Server sendet die modifizierte Zeile über seinen Verbindungssocket an den Client. Der Client liest die modifizierte Zeile von seinem Socket und gibt sie auf seiner Standardausgabe (Monitor) aus. // ConversionClient-Programm: // Liest eine Zeile von der Tastatur und sendet sie an den Server und // bekommt von ihm eine Zeile zurück. // Speichern als: ConversionClient.java // Kompilieren: javac ConversionClient.java // Starten: java ConversionClient // In Zeile 11 muss der richtige Servername stehen bzw. localhost // falls der Server auf demselben Rechner läuft. // Der Server muss natürlich vorher gestartet sein. 01 import java.io.\(\star\) 02 import java.net.\(\star \) 03 class ConversionClient \(\\) 04 public static void main(String[ ] argv) throws Exception 05 \(\\) 06 String sentence 07 String modifiedSentence 08 BufferedReader inFromUser = 09 new BufferedReader( 10 new InputStreamReader(System.in)) 11 Socket clientSocket = new Socket ( servername 6789) 12 DataOutputStream outToServer = 13 new DataOutputStream( 14 clientSocket.getOutputStream()) 15 BufferedReader inFromServer = 16 new BufferedReader(new InputStreamReader( 17 clientSocket.getInputStream())) 18 sentence = inFromUser.readLine() 19 outToServer.writeBytes(sentence + \sf\(\backslash n\)\sf) 20 modifiedSentence = inFromServer.readLine() 21 System.out.println( FROM SERVER: + 22 modifiedSentence) 23 clientSocket.close() 24 \(\\) 25 \(\\) In den ersten beiden Zeilen werden die Bibliotheken für die Benutzereingabe und -ausgabe (01) und die Nutzung des API der Internet-Transportprotokolle (02) geladen. Nach dem Programmkopf des Clientprogramms und der Deklaration zweier Variablen sentence und modifiedSentence wird in den Zeilen 08 bis 10 ein Zeichenstrom inFromUser für die Annahme von Benutzereingaben eingerichtet. Dann wird in Zeile 11 ein Socket clientSocket vom Typ TCP zu dem gewünschten Konvertierungsdienst geöffnet. Der Dienst wird auf dem Host servername an Port 6789 erwartet. Die Variablen outToServer und inFromServer werden mit den Datenströmen des Socket hin zum Server (12-14) und zurück vom Server an den Client (15-17) initialisiert. Sie sind das eigentliche Socket-Interface die Türen durch die die Informationen fließen. Die Benutzereingabe wird nun aus dem Zeichenstrom vom Benutzer eingelesen der Variablen sentence zugewiesen (18) und an den Server übergeben (19). Danach wird auf die Antwort des Servers gewartet und die ankommenden Zeichen vom Server fließen in die Zeichenkette modifiedSentence (20). Das Ergebnis des Dienstes wird für den Benutzer ausgegeben (21 und 22) und die TCP-Verbindung geschlossen (23). Die Benutzung des TCP-Dienstes geschieht in Java also durch die Anforderung eines Objektes vom Typ Socket (Zeile 11) und die nachfolgende Nutzung des Sockets zum Schreiben (19) und Lesen (20) von Daten. Dazu notwendige Datenströme müssen vorher initialisiert werden (12-14 und 15-17).17 Das Gegenstück zum Client ist das Server-Programm: // ConversionServer-Programm: // Speichern als: ConversionServer.java // Kompilieren: javac ConversionServer.java // Starten: java ConversionServer 01 import java.io.* 02 import java.net.* 03 class ConversionServer { 04 public static void main(String[ ] argv) throws Exception 05 { 06 String clientSentence 07 String capitalizedSentence 08 ServerSocket welcomeSocket = new ServerSocket (6789) 09 10 while (true) { 11 Socket connectionSocket = welcomeSocket.accept() 12 BufferedReader inFromClient = 13 new BufferedReader (new InputStreamReader ( 14 connectionSocket.getInputStream())) 15 DataOutputStream outToClient = 16 new DataOutputStream( 17 connectionSocket.getOutputStream()) 18 clientSentence = inFromClient.readLine() 19 capitalizedSentence = 20 clientSentence.toUpperCase() + \(\backslash n\) 21 outToClient.writeBytes(capitalizedSentence) 22 } 23 } 24 } In Zeile 08 wird der Dienst an Port 6789 bereit gestellt und läuft ab dann potenziell für immer (Zeile 10) (siehe Abschnitt 3.10 Charakterisierung von Serverprozessen). Der welcomeSocket ist eine Tür an der auf Clients gewartet wird. Fragt ein Client den Dienst nach wird ein neuer Socket connectionSocket erzeugt (Zeile 11) um den Kunden zu bedienen. Dazu werden Datenströme vom Client inFromClient (12-14) und zurück outToClient (15 - 17) erzeugt und so die Datenströme vom Client zu Server (12-14) und vom Server zu Client (15-17) miteinander verbunden. Die Variable inFromClient ist dabei das Gegenstück zu der Variable outToServer des Client-Programms und die Variable outToClient das Gegenstück zu der Variablen inFromServer des Client-Programms. Über den in den Server eingehenden Datenstrom wird der zu übersetzende Satz entgegen genommen (18) in Großbuchstaben konvertiert (19 und 20) und das Ergebnis an den Client zurückgeschickt (21). An dem Beispiel wird auch deutlich dass bei TCP die Struktur der Anwendungsnachricht und insbesondere die Erkennung des Nachrichtenendes Sache der Anwendung ist. Hier ist eine Nachricht durch genau eine Zeile (vgl. Zeile 18 im Server-Programm und Zeile 19 im Client-Programm) definiert. Übungsaufgabe 3.9 In Abbildung 3.5 ist zu sehen dass ein Router eines Firmennetzwerks eine Schnittstelle zwischen diesem privaten Netzwerk und dem Internet ist. Welche Aufgaben muss dieser Router erfüllen? Aufgabe jetzt bearbeiten  
2; mod_longpage;185; 3; 4; 3.4 Transportschicht Überblick Die Aufgabe der Transportschicht ist die Bereitstellung von Kommunikationsdiensten für die Anwendungsprozesse die auf den verschiedenen Endgeräten eines Computernetzwerks laufen. Hierbei geht es um die Unterstützung logischer Kommunikation zwischen Anwendungsprozessen auf verschiedenen Hosts die in der Regel nicht direkt sondern über eine Menge von dazwischenliegenden Routern und Kommunikationsverbindungen miteinander verbunden sind. Abbildung 3.11 illustriert die Nutzung der Transportschicht durch zwei Anwendungen auf verschiedenen Hosts die über das Rechnernetz verbunden sind. Transportprotokolle werden auf den Endgeräten (Hosts) implementiert nicht in Routern! Router dienen lediglich zum Transport von Paketen zwischen Hosts. Sie implementieren die Dienste der Schichten 3 bis 1 (Vermittlungsschicht bis Bitübertragungsschicht). Abbildung 3.11: Bereitstellung logischer Kommunikation zwischen zwei Anwendungen. Transportprotokolle nutzen jedoch die Dienste der Vermittlungsschicht zur Realisierung logischer Kommunikation zwischen Prozessen (vgl. hierzu auch das Beispiel der Briefkommunikation mittels der Posttransportschicht in Abschnitt 3.5). In der Regel laufen auf einem Host mehrere Anwendungsprozesse die über das Rechnernetz kommunizieren wollen. Da auf Schicht 3 nur die Kommunikation zwischen Hosts unterstützt wird muss ein Transportprotokoll Nachrichten verschiedener Prozesse über denselben Host senden und empfangen. Hierzu muss das Transportprotokoll den Nachrichtenstrom den es von der Vermittlungsschicht erhält auf die verschiedenen Empfängerprozesse verteilen. Dies nennt man Demultiplexen. Damit dies funktioniert muss das sendende Transportprotokoll geeignete Kontrollinformationen zu den Nachrichten die es vom Anwendungsprozess erhält hinzufügen bevor es sie an die Vermittlungsschicht zum Versenden übergibt. Dies nennt man Multiplexen. Das Multiplexen/Demultiplexen muss jedes Transportprotokoll leisten da Transportprotokolle gerade die Host-zu-Host-Übertragung der Vermittlungsschicht um die Prozess-zu-Prozess-Kommunikation erweitern. Die Anwendungen stellen aber noch andere Anforderungen an die Transportschicht wie wir in Abschnitt 3.12 gesehen haben. Das Internet bietet zwei unterschiedliche Transportprotokolle die sich hinsichtlich der ersten Anforderung der Zuverlässigkeit des Dienstes unterscheiden: Das UDP-Protokoll erlaubt das Senden einzelner Nachrichten zwischen Prozessen. Es macht dabei keine Garantien bezüglich der Ankunft (Datenverlust) und der Reihenfolge der Nachrichten. UDP funktioniert praktisch wie die gelbe Post: Der Sender baut keine Verbindung zum Empfänger auf sondern schickt die Nachrichten einfach los. Der Empfänger weiß also gar nicht dass Nachrichten an ihn gesendet werden.18 UDP wird deshalb als verbindungsloser Dienst bezeichnet. Das TCP-Protokoll garantiert dass die von einem Senderprozess zu einem Empfängerprozess übertragenen Nachrichten irgendwann in der richtigen Reihenfolge und vollständig beim Empfängerprozess ankommen. Der Sender baut vor dem Übertragen der Daten eine virtuelle Verbindung zum Empfänger auf ähnlich dem Telefonieren: Erst wenn der Empfänger den Hörer abnimmt kann der Sender dem Empfänger die Nachricht mitteilen. TCP wird deshalb als verbindungsorientierter Dienst bezeichnet. Für die anderen beiden Anforderungen von Anwendungen Bandbreite und Zeit gibt es im derzeitigen Internet keine Unterstützung. Die neue Version des Internets (IP Version 6) stellt hierfür neue Konzepte zur Verfügung die in der einschlägigen Literatur behandelt werden (z.B. [32][33]). Multiplexen und Demultiplexen von Anwendungen Jedes Transportprotokoll nutzt die darunter liegende Schicht zum Senden und Empfangen von Nachrichten. Solche Schicht-4-Nachrichten werden im folgenden als Segmente bezeichnet siehe Abbildung 3.6 auf . Im Internet unterstützt die Vermittlungsschicht den Nachrichtenaustausch zwischen Hosts. Jeder Host wird eindeutig durch seine IP-Adresse identifiziert. Ankommende Nachrichten (Segmente) die bei einem Host von der Vermittlungsschicht an die Transportschicht abgeliefert werden müssen nun an die richtigen Anwendungsprozesse weitergeleitet werden. Wie wir in Abschnitt 3.13 kennen gelernt haben werden Prozesse durch Portnummern identifiziert. Unter Verwendung von IP-Adressen und Portnummern funktioniert das Multiplexen und Demultiplexen wie folgt siehe Abbildung 3.12. Abbildung 3.12: Multiplexen und Demultiplexen von Anwendungen: Prozess 1 sendet eine Nachricht \(M\) an Prozess 2 und Prozess 4 die Nachricht \(M\) an Prozess 3. Das sendende Transportprotokoll fügt die zu sendenden Anwendungsdaten \(M\) mit einem Segmentheader \(H_4\) zu einem Segment zusammen. Im Segmentheader gibt es insbesondere zwei Felder die für die Identifizierung des Sender- und Empfängerprozesses verwendet werden: SourcePort und DestinationPort. Das Hinzufügen der Portnummern des Quellprozesses und des Zielprozesses gehört zum Multiplexen (Multiplexing). Das Segment wird dann an die Vermittlungsschicht zur Versendung weitergegeben. Das empfangende Transportprotokoll erhält ein Segment von der Vermittlungsschicht. Mittels der Felder SourcePort und DestinationPort im Segmentheader identifiziert das Transportprotokoll den korrekten Empfängerprozess und stellt das Segment zu. Diesen Schritt nennt man Demultiplexen (Demultiplexing). Zum Demultiplexen reicht insbesondere die Angabe der Portnummer des Zielprozesses alleine nicht aus. Da mehrere Anwendungsprozesse desselben Typs (die ja dieselbe Portnummer nutzen) auf einem Host laufen können wird auch die Angabe der Portnummer des Senderprozesses benötigt um die logische Kommunikationsverbindung eindeutig zu definieren siehe Abbildung 3.13. Abbildung 3.13: Nutzung von SourcePort- und DestinationPort-Nummern. Als Beispiel zu Multiplexen- und Demultiplexen betrachten wir einen Web-Server und mehrere Web-Clients die HTTP-Anfragen zum Server senden. Bevor ein Client eine HTTP-Anfrage sendet muss er eine Verbindung mit dem Server aufbauen wobei dort ein neuer Prozess mit Portnummer 80 erzeugt wird. Nun kann der Client eine Anfrage verpackt in Segmente zum Server senden. Die Portnummern von Quelle und Ziel im Header eines Segments ermöglichen das Demultiplexen.19 Wenn aber Web-Clients auf verschiedenen Hosts denselben SourcePort gewählt haben dann ermöglicht die eindeutige IP-Adresse im IP-Header des Datagramms die eindeutige Zuordnung des Segments zum korrekten Serverprozess. Dies wird in Abbildung 3.14 illustriert: Web-Server \(B\) kann die Anfragen mit SourcePort 12345 der Web-Clients \(A\) und \(C\) durch die Angabe der sourceIP von \(A\) und \(C\) unterscheiden. Abbildung 3.14: Kommunikation von Web-Clients unter Verwendung einer identischen SourcePort-Nummer mit einem Webserver. Verbindungslose Kommunikation mit UDP UDP (User Datagram Protocol) ist ein einfaches Transportprotokoll das in RFC 768 definiert ist. UDP ist ein verbindungsloser Dienst d.h. ein Senderprozess kann direkt UDP-Segmente verschicken ohne vorher eine Verbindung mit dem Empfängerprozess aufgebaut zu haben. Dies entspricht der Briefzustellung per Post bei der ein Absender ja auch einen Brief verschicken kann ohne dass der Empfänger etwas davon weiß. UDP realisiert einen unzuverlässigen Transportdienst d.h. Nachrichten können verloren gehen. Nachrichten die in einer Reihenfolge vom Senderprozess an den Empfängerprozess gesendet werden können in einer anderen Reihenfolge beim Empfängerprozess abgeliefert werden. UDP macht keine Aussage über die Übertragungsverzögerung bei der Nachrichtenübertragung. UDP wird bevorzugt für einmalige Abfragen und Anwendungen in Client/Server-Umgebungen benutzt in denen die Schnelligkeit der Zustellung wichtiger ist als die Genauigkeit z.B. die Übertragung von Sprache oder Video. Ein Beispiel für die Anwendung von UDP ist das Senden von DNS-Anfragen. Nach der DNS-Spezifikation kann DNS auch über TCP laufen aber es wird fast immer über UDP realisiert da eine DNS-Anfrage relativ klein ist und in ein UDP-Segment passt. So kann ein DNS-Server jede hereinkommende Anfrage gleich beantworten und sofort wieder vergessen. Wenn eine DNS-Anfrage erfolglos ist kann der DNS-Client die Anfrage bei einem anderen Name-Server versuchen oder die anfragende Anwendung informieren dass der Server nicht erreichbar ist. Abbildung 3.15 zeigt eine typische Client-Server-Anwendung die UDP zur Kommunikation verwendet. Ein Anwendungsprozess erzeugt zuerst einen Dienstzugangspunkt zum UDP-Dienst. Dies ist üblicherweise ein Socket vom Typ Datagramm-Socket der dem Anwendungsprozess eine eindeutige IP-Adresse und Portnummer zuordnet. Ein Beispiel für die Verwendung von Sockets für den TCP-Dienst haben wir in Abschnitt 3.15 kennen gelernt. Der Anwendungsprozess kann nun von dem Socket Nachrichten empfangen oder verschicken. Bei Ende des Anwendungsprozesses wird der Socket wieder freigegeben. Abbildung 3.15: Auf UDP basierende Client-Server-Anwendung. UDP-Segmente bestehen aus einem Header mit vier Feldern und einem Datenfeld mit den Anwendungsdaten. Im Header siehe Abbildung 3.16 stehen die SourcePortnummer (2 Byte) und die DestinationPortnummer (2 Byte) zur Identifikation von Sender- und Empfängerprozess. Hinzu kommen die Länge des Segmentes (inklusive Header) in Byte (2 Byte) und eine Prüfsumme (check sum) die zur Fehlererkennung bei fehlerhafter Übertragung des Headers bzw. des Datenfeldes dient. Abbildung 3.16: UDP-Segmentstruktur mit einem 8-Byte-Header. Übungsaufgabe 3.10 Was ist die maximale Länge eines UDP-Segments? Recherchieren Sie die maximale Länge eines Hostnamens und beantworten Sie die Frage warum eine DNS-Anfrage in ein UDP-Segment passt wie oben schon behauptet wurde. Aufgabe jetzt bearbeiten UDP führt eine simple Fehlererkennung ein da nicht alle Vermittlungsprotokolle eine Fehlererkennung oder -korrektur aufweisen. Falls UDP einen Fehler erkennt dann wird das Segment entweder verworfen oder mit einer Warnung an den Anwendungsprozess weitergereicht. Zur Fehlererkennung speichert UDP eine Prüfsumme (2 Byte) im Header des UDP-Segments. Die UDP-Prüfsumme wird berechnet als 1er-Komplement20 der 1er-Komplement-Summe21 aller 16-Bit-Wörter des Segments. Das UDP-Protokoll auf dem Empfänger-Host wird dann die gleiche Operation auf dem Segment durchführen aber inklusive der Prüfsumme hier sollte dann das Resultat \(1111111111111111\) herauskommen. Ist dagegen mindestens ein Bit gleich 0 dann ist ein Übertragungsfehler aufgetreten. Übungsaufgabe 3.11 Berechnen Sie die Prüfsumme der folgenden Wörter wobei hier zur Vereinfachung nur 8-Bit-Wörter (statt 16 Bit wie bei UDP) verwendet werden sollen. 01110101 11110000 00111111 11111000 11001001. Aufgabe jetzt bearbeiten Prinzipien zuverlässigen Datentransfers Eines der grundsätzlichen Probleme bei Rechnernetzen ist das Problem der zuverlässigen Datenübertragung. Grundsätzlich möchten Prozesse einen zuverlässigen Datenübertragungsdienst schließlich nimmt man Verluste bei Nachrichten nur ungern auf sich. Dieser Dienst soll also einen zuverlässigen Übertragungskanal für Nachrichten realisieren. Er soll garantieren dass die Daten unverändert (z.B. ohne Bitfehler) vollständig und in der Reihenfolge beim Empfängerprozess abgeliefert werden in der sie vom Senderprozess verschickt wurden. Das Bedürfnis nach einem zuverlässigen Datenübertragungsdienst kann prinzipiell auf jeder Schicht in einem Netzwerk auftreten. Ein solcher zuverlässiger Übertragungskanal für Nachrichten wird dann durch ein zuverlässiges Datenübertragungsprotokoll implementiert das einen darunter liegenden unzuverlässigen Übertragungskanal verwendet. Das Problem bei der Implementierung eines solchen Protokolls liegt in der potentiellen Unzuverlässigkeit der darunter liegenden Schicht. Im Beispiel des Internet realisiert z.B. das TCP-Protokoll einen zuverlässigen Übertragungskanal in der Transportschicht. Dazu baut TCP auf dem IP-Protokoll der Vermittlungsschicht auf das selbst wiederum ein unzuverlässiges Host-zu-Host-Protokoll ist. Im Allgemeinen kann die Schicht unterhalb der beiden zuverlässig kommunizierenden Endpunkte aus einer einzigen physikalischen Verbindung bestehen oder ein komplexes globales Rechnernetz sein. Man kann sich einen Dienst generell auch als abstrakten Datentyp (siehe Kurse Datenstrukturen I Datenstrukturen II Datenstrukturen) vorstellen. Der Dienst definiert dann eine Datenstruktur und die darauf arbeitenden Operationen. Ein Protokoll ist dann die Implementierung des Dienstes bzw.des abstrakten Datentyps der den Dienst spezifiziert. Ein zuverlässiger Datenübertragungsdienst kann auf einem unzuverlässigem Medium im Prinzip wie in Abbildung 3.17 implementiert werden. Abbildung 3.17: Implementierung eines zuverlässigen Datenübertragungsdienst. Die Pfeile drücken einen Datenfluss aus. Zuerst ruft ein Senderprozess mittels der Funktion rdt_send(daten) die Senderseite des zuverlässigen Datentransferprotokolls auf um Daten an den Empfängerprozess zu senden (rdt steht für reliable data transfer). Das zuverlässige Datentransferprotokoll verschickt die Daten mittels der darunter liegenden unzuverlässigen Netzwerkschicht an das auf der Empfängerseite laufende zuverlässige Datentransferprotokoll. Dazu verwendet es die von der unterliegenden unzuverlässigen Schicht bereitgestellte Funktion udt_send(paket) (udt steht für unreliable data transfer). Auf der Empfängerseite empfängt das Datentransferprotokoll die Pakete mittels der Funktion rdt_receive(paket). Bei Fehlern in der Datenübertragung ist weitere bilaterale Kommunikation von Daten und Kontrollinformation zwischen der Senderseite und der Empfängerseite des zuverlässigen Datenübertragungsprotokolls notwendig um die Fehler zu beheben. Wenn die Nachricht komplett übertragen wurde kann der Empfänger des zuverlässigen Datentransferprotokoll mittels der Funktion deliver_data(daten) die Nachricht an den Empfängerprozess liefern. Aus der Sicht von Senderprozess und Empfängerprozess handelt es sich um einen zuverlässigen Übertragungskanal der aber in Wirklichkeit vom zuverlässigen Datenübertragungsprotokoll mit Hilfe des unterlagerten unzuverlässigen Übertragungskanals realisiert wurde. Übungsaufgabe 3.12 Von welchen Schichten werden die Funktionen rdt_send() und udt_send() in Abbildung 3.17 zur Verfügung gestellt? Aufgabe jetzt bearbeiten Kommunikationsprotokolle werden in der Regel als Zustandsautomaten beschrieben. Hierbei beschreiben Knoten die Zustände und Pfeile die Übergänge zwischen Zuständen. An jedem Pfeil steht das Ereignis das den Übergang auslöst und die Operationen die beim Übergang ausgeführt werden. Ereignis und Operationen sind durch einen waagerechten Strich getrennt. Den einfachsten Fall eines Datenübertragungsprotokolls für einen zuverlässigen Übertragungskanal zeigt Abbildung 3.18:22 Hier gehen wir davon aus dass der genutzte Übertragungskanal auch zuverlässig ist d.h. rdt_receive() und udt_send() verlieren und verfälschen keine Daten. Abbildung 3.18: Datenübertragungsprotokoll für einen zuverlässigen Übertragungskanal. Bei der Übertragung von Daten über einen unzuverlässigen Übertragungskanal muss ein Transportprotokoll generell mit den folgenden Problemen umgehen: Einzelne Bits in einer Nachricht können fehlerhaft übertragen werden. Ganze Nachrichten können verloren gehen. Falls die Datenrate in der ein Empfänger Daten empfangen kann niedriger sein kann als die Rate in der ein Sender Daten senden kann dann muss zusätzlich das Problem der Überflutung des Empfängers mit Nachrichten gelöst werden. Im folgenden werden wir Lösungen für die ersten beiden Probleme kennen lernen. Um Überflutungen zu vermeiden sind Techniken der Flusskontrolle notwendig siehe Abschnitt 3.32 und [32]. Zur Vereinfachung der Darstellung betrachten wir im folgenden nur die unidirektionale Kommunikation von einem Senderprozess zu einem Empfängerprozess. Bidirektionale Kommunikation zwischen zwei Prozessen kann dann als Kombination von Sender- und Empfängerseite in einem Protokoll-Client implementiert werden. Zuverlässige Datenübertragung über einen verlustfreien Kanal mit Bitfehlern Betrachten wir zuerst den hypothetischen Fall dass Datenpakete über einen verlustfreien Kanal mit Bitfehlern d.h. ohne Verlust und in der richtigen Reihenfolge aber ggf. mit veränderten Bits übertragen werden. Es stellt sich also das Problem wie Bitfehler vom Protokoll zu behandeln sind. Wenn zwei Personen über einen solchen Kanal z.B. eine schlechte Telefonverbindung einen längeren Text diktieren dann würden sie wahrscheinlich das folgende Protokoll verwenden: der Zuhörer bestätigt nach jedem Satz den er gehört verstanden und aufgezeichnet hat den Empfang indem er OK sagt. Falls der Empfänger einen Satz nicht versteht z.B. wegen einer Störung dann wird er den Sprecher um Wiederholung bitten. In diesem Protokoll werden sowohl positive Bestätigungen (Acknowledgements-ACK) als auch negative Bestätigungen (Negative Acknowledgements-NACK) verwendet. In der Literatur heißen zuverlässige Datenübertragungsprotokolle die auf Wiederholungen basieren auch ARQ-Protokolle (Automatic Repeat reQuest). Grundsätzlich werden in einem ARQ-Protokoll drei Fähigkeiten benötigt um Bitfehler zu behandeln: Fehlererkennung: Um Bitfehler zu erkennen müssen zusätzliche Informationen in einem Paket übertragen werden (z.B. die Prüfsumme). Rückmeldung vom Empfänger: Da Senderprozess und Empfängerprozess in der Regel auf verschiedenen Hosts laufen kann der Sender nur mittels expliziter Nachrichten etwas über den Zustand des Empfängers erfahren. In obigem Beispiel müssen die positiven (ACK) und negativen (NACK) Bestätigungen vom Empfänger zum Sender kommuniziert werden. Wiederholung: Ein fehlerhaft empfangenes Paket muss vom Sender noch einmal übertragen werden. Die Senderseite eines Protokolls für einen verlustfreien Kanal mit Bitfehlern hat zwei Zustände siehe Abbildung 3.19. In einem Zustand wartet das Protokoll auf Daten die übertragen werden sollen in dem anderen Zustand wartet das Protokoll auf eine Empfangsbestätigung des Empfängers und das Ereignis rdt_send(data) existiert nicht. Falls ein NACK empfangen wird dann sendet das Protokoll das letzte Paket noch einmal und bleibt im Wartezustand auf die Bestätigung für dieses Paket. Falls ein ACK empfangen wird dann wurde das letzte Paket korrekt empfangen und das Protokoll kehrt in den Zustand des Wartens auf neue Daten des Senderprozesses zurück.23 Abbildung 3.19: Senderseite eines Stop-and-Wait- Protokolls für einen verlustfreien Kanal mit Bitfehlern. In dem oben beschriebenen Protokoll wartet der Sender mit der Übertragung des nächsten Pakets solange bis eine Bestätigung für das aktuelle Paket eingetroffen ist. Deshalb heißt dieses Protokoll auch Stop-and-Wait-Protokoll. Die Empfängerseite des Stop-and-Wait-Protokolls hat nur einen Zustand in dem das Protokoll auf die Ankunft eines neuen Pakets wartet siehe Abbildung 3.20. Das Protokoll prüft dann z.B. anhand der Prüfsumme ob das Datenpaket einen Bitfehler aufweist. Liegt ein Bitfehler vor dann wird ein NACK verschickt ansonsten werden die Anwendungsdaten extrahiert an den Empfängerprozess weitergereicht und ein ACK an den Sender zurückgeschickt. Abbildung 3.20: Empfängerseite eines Stop-and-Wait-Protokolls für einen verlustfreien Kanal mit Bitfehlern. Unglücklicherweise ist das oben skizzierte Protokoll nicht in der Lage mit Bitfehlern in den Bestätigungen umzugehen. Dieses Problem kann prinzipiell auf zwei Arten gelöst werden: Durch Hinzufügen einer genügend langen Prüfsumme kann der Sender nicht nur einen Bitfehler in einer Bestätigungsnachricht erkennen sondern diesen auch beheben. Dies löst das Problem eines fehlerhaften Kanals der keine Pakete verliert. Bei Empfang eines verfälschten ACK oder NACK kann der Sender nicht entscheiden ob der Empfänger ein ACK oder NACK senden wollte. In diesem Fall kann der Sender einfach das aktuelle Datenpaket erneut senden. Dies führt aber zu duplizierten Paketen im Nachrichtenstrom vom Sender zum Empfänger. Der Empfänger hat nun das Problem dass er nicht weiß ob das letzte gesendete ACK/NACK korrekt vom Sender empfangen wurde (und ob daher das aktuelle Paket eine Wiederholung ist oder nicht). Üblicherweise wird dieses Problem durch Hinzufügen eines neuen Feldes Sequenznummer im Paketheader gelöst. Der Sender nummeriert nun alle gesendeten Pakete und trägt in dieses neue Feld die Sequenznummer des Pakets ein. Damit kann der Empfänger einfach prüfen ob ein empfangenes Paket eine Wiederholung mit alter Sequenznummer oder ein neues Paket ist. Für den einfachen Fall unseres Stop-and-Wait-Protokolls genügt ein einzelnes Bit zur Nummerierung um die Pakete zu unterscheiden. Da wir annehmen dass der Kanal keine Pakete verliert müssen ACK/NACK-Pakete nicht explizit die Sequenznummer des Pakets enthalten auf das sie sich beziehen. Der Sender kann annehmen dass sich jedes ACK/NACK ob verfälscht oder nicht immer auf das letzte übertragene Paket bezieht. Abbildung 3.21 zeigt das Zustandsübergangsdiagramm der Senderseite eines Stop-and-Wait-Protokolls mit Sequenznummern. Das Protokoll kommt ohne negative Bestätigungen (NACK) aus. Wenn der Empfänger ein Paket nicht korrekt empfangen hat sendet der Empfänger einfach ein ACK für das letzte korrekt empfangene Paket. Wenn der Sender nun zwei ACKs für dasselbe Paket \(P\) erhält weiß er dass der Empfänger das nachfolgende Paket von \(P\) nicht korrekt empfangen hat. Unser verbessertes Protokoll hat nun doppelt so viele Zustände wie das einfache Protokoll in Abbildung 3.19 und 3.20. Der Grund hierfür ist dass der Zustand nun speichern muss ob das nächste zu sendende Paket die Sequenznummer 0 oder 1 haben soll. Abbildung 3.21: Senderseite eines Stop-and-Wait-Protokolls mit Sequenznummern die zwischen 0 und 1 wechseln. Abbildung 3.22 zeigt das Zustandsübergangsdiagramm der Empfängerseite eines Stop-and-Wait-Protokolls mit Sequenznummern. Abbildung 3.22: Empfängerseite eines Stop-and-Wait-Protokolls mit Sequenznummern. Zuverlässige Datenübertragung über einen verlustbehafteten Kanal mit Bitfehlern Nehmen wir nun an dass der Kanal Pakete in der richtigen Reihenfolge überträgt aber sowohl Bitfehler verursachen als auch Pakete verlieren kann. Dies tritt bei den heute verwendeten Netzwerktechnologien durchaus häufiger auf. Ein zuverlässiges Datenübertragungsprotokoll muss nun zwei neue Aufgaben lösen: Erkennung von Paketverlusten Behandlung von Paketverlusten. Mit Hilfe der uns schon bekannten Techniken wie z.B. Prüfsummen Sequenznummern Bestätigungsnachrichten (ACK) und Wiederholungen können wir Paketverluste behandeln. Aber wie kann ein Paketverlust bemerkt werden? In diesem Abschnitt werden wir eine Lösung vorstellen bei welcher der Sender die Aufgabe der Erkennung und Behandlung von Paketverlusten löst. Nehmen wir an der Sender überträgt ein Paket und entweder dieses Paket oder das dazugehörige ACK geht verloren. In beiden Fällen kommt keine Antwort des Empfängers beim Sender an. Falls der Sender nun bereit ist lang genug zu warten so dass er sicher sein kann dass ein Paket verloren wurde dann kann der Sender nach Ablauf der Wartezeit einfach das Paket erneut übertragen. Dieses Protokoll löst unser Problem. Die Frage ist nun: Wie lange muss der Sender warten um sicher zu sein dass ein Paket verloren gegangen ist? Im Prinzip müsste der Sender mindestens die Zeit abwarten die ein Paket vom Sender zum Empfänger und zurück benötigt. Diese Zeit beinhaltet auch Zeiten in denen Pakete im Internet in Routern oder Gateways24 gepuffert werden und die Zeit der Verarbeitung des Pakets bei der Empfängerseite des Protokolls. Diese worst-case Verzögerung lässt sich in der Praxis kaum schätzen. Hinzu kommt dass unser Protokoll möglichst rasch einen Paketverlust behandeln sollte anstatt sehr lange darauf zu warten dass ein Paketverlust tatsächlich sehr sicher aufgetreten ist. Deshalb wird in der Praxis eine Zeitschranke gewählt deren Überschreitung einen Paketverlust anzeigt obwohl eventuell gar keiner aufgetreten ist. Sobald der Sender innerhalb der Zeitschranke kein ACK für das Paket erhält wird er das Paket wiederholen auch wenn tatsächlich gar kein Paket oder ACK verlorengegangen sein mag z.B. wegen einer Überlastung des Internets. Dies wird in den Fällen in denen das Paket gar nicht verloren gegangen ist zu Duplikaten führen. Solche Duplikate kann der Empfänger aber anhand der Sequenznummern erkennen bestätigen und verwerfen. Damit der Sender das Überschreiten der Zeitschranke erkennen kann benötigt er einen Zeitgeber (Countdown-Timer). Der Sender muss in der Lage sein den Zeitgeber bei jedem gesendeten Paket zu setzen bei Ablauf des Zeitgebers das verlorengegangene Paket zu wiederholen und bei Ankunft einer Bestätigungsnachricht (ACK) den Zeitgeber zu stoppen. Die Zeit vom Starten des Countdown-Timers bis zum Ablauf nennt man auch Timeout-Intervall. Die Existenz von Duplikaten der Datenpakete oder der Bestätigungsnachrichten kompliziert für den Sender die Behandlung von Bestätigungsnachrichten. Um festzustellen zu welchem Paket eine Bestätigungsnachricht gehört muss die Sequenznummer des bestätigten Pakets vom Empfänger in ein neues Feld (Bestätigungsfeld) des ACK-Pakets geschrieben werden. Anhand dieser Sequenznummer kann der Sender feststellen welches Paket vom Empfänger bestätigt wurde. Abbildung 3.23: Senderseite des Alternating-Bit-Protokolls. Abbildung 3.23 zeigt die Senderseite des oben beschriebenen Protokolls. Der aufmerksame Leser wird feststellen dass in Abbildung 3.23 keine Zeitgeber angehalten werden z.B. wenn ein Acknowledgement empfangen wird. Dies ist in einem Zustandsübergangsdiagramm auch nicht notwendig da ja das Protokoll in einen neuen Zustand übergeht in dem das Ablaufen des Zeitgebers einfach ignoriert wird. Deshalb muss der Zeitgeber in dieser Darstellung auch nicht explizit angehalten werden. Das in Abbildung 3.23 gezeigte Protokoll erfüllt nun alle Anforderungen an ein Protokoll für die zuverlässige Datenübertragung über einen verlustbehafteten Kanal mit Bitfehlern. In der Literatur wird dieses Protokoll auch als Alternating-Bit-Protokoll bezeichnet da die Sequenznummern der Pakete immer zwischen 0 und 1 alternieren. Damit ein solches Protokoll mit Timeout effizient funktioniert muss die Länge des Timeout-Intervalls sorgfältig gewählt werden wie die nächste Aufgabe zeigt. Übungsaufgabe 3.13 Wir betrachten das Alternating-Bit-Protokoll siehe Abbildung 3.23. Wir verwenden ein extrem kurzes Timeout-Intervall. Funktioniert das Protokoll überhaupt noch? Wenn ja welche Nachteile ergeben sich? Jetzt setzen wir auf ein sehr (bzw. zu) langes Timeout-Intervall. Das Protokoll funktioniert aber unter welchen Umständen ist dies nachteilig? Welche Informationen braucht man um die richtige Länge für das Timeout-Intervall festzulegen? Aufgabe jetzt bearbeiten Abbildung 3.24 zeigt in einem sogenannten Zeitdiagramm einige typische Situationen bei Ablauf des Protokolls. Häufig werden diese Situationen in solchen Zeitdiagrammen dargestellt. Die Zeit verläuft in solchen Diagrammen von oben nach unten. Die Spalten entsprechen hierbei den Aktionen der Akteure (Senderseite Empfängerseite). Zwischen den beiden Akteuren stellen Pfeile den Nachrichtenfluss dar und die Pfeilbeschriftungen identifizieren die gesendeten Nachrichten. Die eckigen Klammern von send paket1 zu timeout deuten an wann der Zeitgeber abgelaufen ist. Abbildung 3.24: Typische Situationen beim Alternating-Bit-Protokoll. Leider ist die Leistung des Alternating-Bit-Protokolls in der Praxis kaum ausreichend da es ein Stop-and-Wait-Protokoll ist. Nach jeder Übertragung eines Pakets wartet das Protokoll auf die Bestätigung. In der Praxis werden deshalb Protokolle eingesetzt die mehr als ein Paket senden dürfen bevor eine Bestätigung des Empfängers eintreffen muss. Hierzu ist es notwendig Pakete durchzunummerieren. Die Fenstergröße legt fest wie viele Pakete gesendet werden dürfen ohne dass die vorhergehenden Pakete bestätigt werden müssen. Ein sogenanntes Sendefenster enthält zu jedem Zeitpunkt sämtliche Paketnummern die der Sender aktuell beim Versenden von Paketen benutzen darf siehe auch Abbildung 3.25. Der Empfänger bestätigt in der Regel nicht den Empfang jedes einzelnen Paketes sondern den Empfang einer korrekt empfangenen Paketfolge (kumulative Bestätigung). Abbildung 3.25: Sequenznummern im go-back-\(n\) aus Sendersicht. Die Sequenznummern der Pakete beim Sender werden in vier Gruppen eingeteilt: Die Pakete bis Nummer \(\rm nextseqnum - 1\) wurden übertragen davon wurden die bis \(\rm base - 1\) schon bestätigt. Pakete mit Sequenznummern von \(\rm nextseqnum\) bis \(\rm base + n - 1\) dürfen noch gesendet werden. fig3/gbn Dazu sendet er die Nummer des letzten in der Folge korrekt empfangenen Pakets.25 So arbeitende Protokolle heißen Fenster-Protokolle. In diesem Sinne kann z.B. das Alternating-Bit-Protokoll als Fensterprotokoll mit der Fenstergröße 1 bezeichnet werden. Für Protokolle mit einer Fenstergröße größer als 1 sind aber weitere Vorkehrungen für Fehlerbehandlungen zu treffen: Werden Nachrichten oder Quittungen fehlerhaft übertragen oder gehen sie verloren so kommen zusätzlich zu Zeitüberwachungen die jetzt individuell pro Paket erfolgen müssen zwei aktive Eingriffsmöglichkeiten des Empfängers zum Einsatz: Er kann den Sender auffordern das fehlerhafte Paket und alle danach bereits gesendeten \(n-1\) Folgepakete erneut zu senden (go-back-n auch Sliding-Window-Protokoll genannt). Er kann den Sender auffordern nur das fehlerhafte Paket erneut zu senden (selective-reject/repeat). In diesem Kurs werden diese Ansätze nicht weiter behandelt. Wir gehen nur insoweit darauf ein dass das im folgenden vorgestellte Protokoll aus der Praxis also TCP Konzepte der Fensterprotokolle wie Sequenznummern und kumulative Bestätigungen benutzt. Zur weiteren Vertiefung sei auf die Literatur [32][34] oder auf weiterführende Kurse (z.B. Kommunikations- und Rechnernetze) verwiesen. Verbindungsorientierte Kommunikation mit TCP Nachdem wir in den vorangegangenen Abschnitten die Grundlagen von Transportprotokollen und den verbindungslosen Transportdienst UDP im Internet kennen gelernt haben wollen wir uns nun mit dem zweiten Transportdienst im Internet dem verbindungsorientierten Transportprotokoll TCP (Transmission Control Protocol [1]) beschäftigen. TCP realisiert einen zuverlässigen Datentransfer zwischen zwei Anwendungsprozessen. TCP arbeitet immer Punkt-zu-Punkt d.h. zwischen genau zwei Anwendungsprozessen. TCP unterstützt Vollduplex-Datenübertragung (full-duplex) zwischen genau einem Sender und einem Empfänger. Das heißt beide Anwendungsprozesse können über eine Verbindung gleichzeitig Daten an die andere Seite schicken.26 Anwendungsprozesse nutzen TCP indem sie zuerst eine Verbindung zwischen Sender und Empfänger aufbauen. Nachdem die Verbindung aufgebaut ist kann der Senderprozess beliebige Bytefolgen über die Verbindung an den Empfänger schicken. TCP garantiert dass die identische Bytefolge beim Empfänger ankommt. Insofern funktioniert TCP wie eine Datei in die der Sender Daten schreibt und aus welcher der Empfänger Daten liest (analog zu einer Pipe in Unix). TCP vermittelt nicht wie UDP einzelne Nachrichten zwischen den Anwendungsprozessen sondern einen Datenstrom. Wenn die Anwendungsprozesse den Datenstrom zur Übertragung von Nachrichten also Blöcken von Informationen nutzen wollen so müssen sie Beginn und Ende von Nachrichten selbst im Datenstrom codieren so dass der Empfänger Beginn und Ende einer Nachricht erkennen kann. Wir konnten dieses Vorgehen schon am Anwendungsbeispiel in Abschnitt 3.15 sehen wo das Ende einer Nachricht durch ein \sf\(\backslash n\)\sf (Zeilenende-Zeichen) angezeigt wird. Auch bei HTTP in Abschnitt 3.11 lässt sich dies erkennen. Damit der HTTP-Server die eingehende GET-Anfragenachricht bearbeiten konnte muss er zuerst die gesamte Nachricht vom Socket lesen. Das Ende der Nachricht erkennt der Server an der Zeichenfolge CR LF CR LF also zweimal das Paar Carriage Return Line Feed hintereinander. Damit ist klar wann eine Nachricht zu Ende ist und eine neue Nachricht beginnt. Wenn ein Anwendungsprozess die Kommunikation beenden will dann baut er die TCP-Verbindung wieder ab. Sobald die Verbindung abgebaut ist können keine Daten mehr über die Verbindung übertragen werden. Aufgrund der zentralen Bedeutung der Verbindung heißt TCP auch verbindungsorientiertes Protokoll. Verbindungsmanagement Bevor Daten auf einer TCP-Verbindung übertragen werden können muss diese Verbindung zwischen Sender-Anwendungsprozess und Empfänger-Anwendungsprozess aufgebaut werden. Analog zu UDP wird auch hier ein Socket verwendet um eine Verbindung von einem Client-Prozess zu einem Server-Prozess zu erzeugen. Dabei gibt der Client-Prozess die IP-Adresse und die Portnummer des Server-Prozesses an. Mit dieser Information kann das TCP auf dem Client-Host (im folgenden Client-TCP genannt) eine Verbindung zum TCP auf dem Server-Host (im folgenden Server-TCP) aufbauen. Dabei geht der Client in drei Schritten vor siehe Abbildung 3.25 die den Verbindungsaufbau in einem Zeitdiagramm darstellt. \pstexfigZeitdiagramm des TCP-Verbindungsaufbaus. Das Client-TCP sendet ein sogenanntes SYN-Segment27 an das Server-TCP. Dieses Segment enthält keine Anwendungsdaten aber es hat ein spezielles Header-Feld das SYN-Bit auf 1 gesetzt. Zusätzlich enthält das Sequenznummer-Feld des SYN-Segments eine initiale Sequenznummer (client_isn) die das Client-TCP gewählt hat. Sobald das SYN-Segment beim Server-TCP ankommt erzeugt es eine neue Verbindung und weist ihr entsprechende Puffer und Variablen zu. Dann schickt das Server-TCP ein SYNACK-Segment \footnotemark[\valuefootnote]an das Client-TCP zurück. Dieses Segment enthält noch keine Anwendungsdaten aber es enthält drei wichtige Informationen im Segment-Header: das SYN-Bit ist auf 1 gesetzt das Acknowledgement-Feld ist auf die gerade empfangene Sequenznummer client_isn+1 gesetzt 28 und das Sequenznummer-Feld ist auf eine initiale Sequenznummer des Servers (server_isn) gesetzt die das Server-TCP gewählt hat. Dieses SYNACK-Segment bestätigt dem Client-TCP dass das Server-TCP die Verbindung mit der initialen Sequenznummer client_isn des Client-TCP akzeptiert hat und dass die initiale Sequenznummer des Servers server_isn ist. Sobald das Client-TCP das SYNACK-Segment empfängt erzeugt es ebenfalls eine entsprechende Verbindung und weist ihr entsprechende Puffer und Variablen zu. Dann schickt das Client-TCP eine Bestätigungsnachricht. In diesem Segment ist das Acknowledgement-Feld auf server_isn+1 und das SYN-Bit auf 0 gesetzt. Sobald diese drei Nachrichten erfolgreich ausgetauscht wurden ist die Verbindung etabliert und die Anwendungsprozesse können Daten über die Verbindung übertragen. Weil beim Verbindungsaufbau genau drei Nachrichten ausgetauscht werden heißt dieses Verfahren auch Drei-Wege-Handschlag (three-way handshake). Wenn der Client die Verbindung schließen will dann sendet er ein close Kommando an das Client-TCP. Er geht dann wie folgt vor: Das Client-TCP sendet ein sogenanntes FIN-Segment29 an das Server-TCP. Dieses Segment enthält keine Anwendungsdaten aber es hat ein spezielles Header-Feld das FIN-Bit auf 1 gesetzt. Wenn das Server-TCP ein FIN-Segment empfängt dann schickt es eine Bestätigung ein ACK-Segment an das Client-TCP. Dann schickt das Server-TCP ein eigenes FIN-Segment mit FIN-Bit auf 1 gesetzt an das Client-TCP. Wenn das Client-TCP das FIN-Segment empfängt dann schickt es eine Bestätigung ein ACK-Segment an das Server-TCP. Nach einer bestimmten Wartezeit wird die Verbindung gelöscht und die zugewiesenen Ressourcen (Variablen Puffer) werden freigegeben. Abbildung 3.26 fasst die Zustandsübergänge die Client-TCP und Server-TCP typischerweise durchlaufen zusammen. Abbildung 3.26: Zustandsübergänge von Client-TCP und Server-TCP. Nachdem das Client-TCP das letzte FIN des Server-TCP empfangen und bestätigt hat wartet es noch eine gewisse Zeit (die implementationsabhängig zwischen 30 Sekunden und 2 Minuten liegt) und gibt dann die Verbindungsressourcen frei. Diese Wartezeit ist notwendig damit das Client-TCP die letzte Bestätigung wiederholen kann falls sie verloren geht. Die obige Diskussion des Verbindungsmanagements von TCP hat den normalen Ablauf des Verbindungsaufbaus und -abbaus geschildert. Sonderfälle wie z.B. den gleichzeitigen Wunsch von Client-TCP und Server-TCP die Verbindung abzubauen wurden nicht behandelt. Interessierte Leser werden dazu auf die gängige Literatur verwiesen (z.B. Stevens [35]). Übungsaufgabe 3.14 Beim Aufbau einer TCP-Verbindung zwischen einem Client und einem Server wird ein Drei-Wege-Handschlag durchgeführt d.h. drei Segmente werden ausgetauscht wobei auch die initialen Sequenznummern festgelegt werden siehe Abbildung 3.25. Warum genügt nicht ein Zwei-Wege-Handschlag d.h. warum kann der Client nicht einfach an den Server ein SYN-Segment schicken um den Aufbau einer Verbindung anzufordern und der Server bestätigt mit einem SYNACK-Segment? Warum ist die Angabe der initialen Sequenznummern beim Aufbau der TCP-Verbindung erforderlich? Aufgabe jetzt bearbeiten Datentransfer in TCP Sobald eine TCP-Verbindung aufgebaut ist können sich die beiden Anwendungsprozesse gegenseitig Daten schicken. In dem Programmbeispiel in Abschnitt 3.15 entspricht der Verbindungsaufbau der Konstruktion des Sockets in Zeile 11 des Client-Programms auf . Sobald der Client-Anwendungsprozess Daten in den Socket schreibt (Zeile 19) werden diese vom Client-TCP behandelt siehe Abbildung 3.27. Das Client-TCP leitet diese Daten direkt in den Sendepuffer der Verbindung weiter der beim Verbindungsaufbau dieser Verbindung zugeordnet wurde. Abbildung 3.27: TCP Sende- und Empfangspuffer. TCP nimmt eine Datenmenge aus dem Sendepuffer und fügt sie in ein neues Segment ein. Ein TCP-Segment besteht aus einem 20-Byte-Header-Feld einem Optionsfeld und Anwendungsdaten siehe Abbildung 3.28. Die maximale Anzahl von Bytes die in einem Segment übertragen werden können ist implementationsabhängig und sollte passend zur Datagrammgröße der Vermittlungsschicht (IP-Protokoll) gewählt werden. Übliche maximale Segmentgrößen (Maximum Segment Size MSS) sind z.B. 1460 Byte 536 Byte oder 512 Byte - jedoch maximal 64 KB in IP Version 4. Abbildung 3.28: Struktur eines TCP-Segments. Die Sequenznummer und die Bestätigungsnummer sind 32 Bit lang. Die 16-Bit-Fenstergröße wird für die Flusskontrolle benutzt. Die Bits RST SYN und FIN werden zur Signalisierung von Kommandos für den Verbindungsauf- und -abbau genutzt. Das IP-Protokoll überträgt das TCP-Segment als IP-Datagramme und liefert die gesendeten Daten an das Server-TCP. Das Server-TCP speichert die empfangenen Daten im Empfangspuffer der Verbindung. Aus diesem Empfangspuffer kann der Server-Anwendungsprozess Daten lesen (im Programmierbeispiel entspricht dies der Zeile 18 im Server-Programm auf ). Da TCP bidirektionale Kommunikation unterstützt hat jede Seite der Verbindung eigene Sende- und Empfangspuffer die Variablen inFromClient outToClient outToServer inFromServer in Abschnitt 3.15. Übungsaufgabe 3.15 Die Sequenznummer eines TCP-Segments ist 32 Bit lang und deshalb durch \(2^{32}\) begrenzt siehe auch Abbildung 3.28. Warum können trotzdem theoretisch beliebig viele Segmente über eine TCP-Verbindung gesendet werden? Aufgabe jetzt bearbeiten Sequenz- und Bestätigungsnummern Da das IP-Protokoll ein unzuverlässiges Datenübertragungsprotokoll ist muss TCP zusätzlich Datenverluste erkennen und beheben und die gleichbleibende Reihenfolge der übertragenen Daten garantieren vergleiche die Situationen in Abbildung 3.17 auf . TCP realisiert einen Datenstrom von Bytes zwischen Sender und Empfänger. Jedes Byte in diesem Bytestrom wird nummeriert diese Nummer nennen wir die Bytestromnummer. Die Sequenznummer eines Segments ist definiert als die Bytestromnummer des ersten Bytes im Segment. Betrachten wir das Beispiel eines Anwendungsprozesses der über eine TCP-Verbindung eine Datei von 10.000 Byte an den Empfängerprozess verschicken will wobei die maximale Segmentgröße MSS 1.000 Byte Anwendungsdaten beträgt. Der Anwendungsprozesses schreibt dann die 10.000 Byte in den Socket der mit der Verbindung assoziiert ist. Das Client-TCP konstruiert dann 10 Segmente und befüllt deren Datenfelder mit den Anwendungsdaten. Wenn das erste Byte mit \(i\) nummeriert wird (\(i=client\_isn+1\) siehe Abschnitt 3.27) enthält das erste Segment die Bytes \(i\) bis \(i+999\) das zweite Segment die Bytes \(i+1000\) bis \(i+1999\) usw. Damit erhält das erste Segment die Sequenznummer \(i\) das zweite Segment die Sequenznummer \(i+1000\) usw. die im Header des entsprechenden TCP-Segments eingetragen werden. TCP verwendet Bestätigungsnummern (Acknowledgements) um zu signalisieren welches Byte als nächstes vom Sender erwartet wird. Wenn also das Server-TCP die ersten beiden Segmente korrekt empfangen hat und das dritte anfordern möchte dann erwartet es als nächstes Byte dasjenige mit der Bytestromnummer \(i+2000\). Deshalb gibt das Server-TCP die Nummer \(i+2000\) als Bestätigungsnummer im nächsten Segment an welches das Server-TCP an das Client-TCP verschickt. Falls das Server-TCP Segmente empfängt die Daten enthalten die erst später im Datenstrom kommen (z.B. passiert dies wenn ein vorheriges Segment verlorengegangen ist oder später ankommen wird) dann verschickt das Server-TCP als Bestätigungsnummer trotzdem nur die Nummer des nächsten noch fehlenden Bytes im Bytestrom. TCP wendet also die kumulative Bestätigung von bis dahin korrekt empfangenen Bytes an. Was passiert jedoch mit den Daten der Segmente die erst nach der Lücke kommen? Implementationsabhängig kann das Server-TCP diese Daten puffern und darauf warten dass die fehlenden Daten ankommen und dann bis zum dann fehlenden nächsten Byte bestätigen oder aber verwerfen und damit das Client-TCP später zur Wiederholung zwingen. Übungsaufgabe 3.16 Was passiert wenn im Netzwerk noch verspätete Pakete einer mittlerweile schon abgebauten Verbindung ankommen? Wann könnte das ein Problem sein? Aufgabe jetzt bearbeiten Fehlerkorrektur TCP benutzt zur Übertragung seiner Segmente den unzuverlässigen Übertragungsdienst des IP-Protokolls. Insbesondere garantiert IP nicht dass Datensegmente unverfälscht ankommen. Deshalb fügt TCP eine eigene Prüfsumme zu jedem Segment hinzu siehe Abbildung 3.28 und überträgt verfälschte Daten erneut siehe Abschnitt 3.31. Der Algorithmus für die Bildung der Prüfsumme ist einfach: Sie ist genau das 1er-Komplement der 1er-Komplement-Summe aller 16-Bit-Wörter des Segments. Zur Kontrolle addiert der Empfänger alle 16-Bit-Wörter einschließlich des Headers. Wenn sich dabei die Binärzahl \(1111111111111111\) bestehend aus 16 gesetzten Bits ergibt dann ist wahrscheinlich kein Fehler bei der Übertragung aufgetreten siehe auch Abschnitt 3.18. Zuverlässiger Datentransfer in TCP TCP realisiert einen zuverlässigen Datenübertragungsdienst d.h. Daten werden unverändert vollständig und in der Reihenfolge beim Empfängerprozess abgeliefert in der sie vom Senderprozess versendet wurden. Um festzustellen ob die Daten in einem TCP-Segment verändert wurden berechnet der Empfänger die 1er-Komplement-Summe siehe Abschnitt 3.18 auf dem empfangenen Segment inklusive der Prüfsumme im TCP-Segment-Header siehe auch Abbildung 3.28. Die Prinzipien die wir in Abschnitt 3.19 kennen gelernt haben gelten auch hier. Zur Bestimmung eines Paketverlusts und des korrekten Empfangs wird bei der Übertragung ein einziger Timer und insbesondere die kumulative Bestätigung wie beim Sliding-Window-Protokoll verwendet siehe . Für die richtige Reihenfolge besitzt jedes Segment eine Sequenznummer die genau die erste Bytenummer des Segments ist. Die Bestätigungsnummer im TCP-Header ist immer die Sequenznummer des Segments das der Empfänger als nächstes erwartet. Als Beispiel schauen wir ein vereinfachtes TCP-Senderprogramm an das keine Flusskontrolle (siehe Abschnitt 3.32) realisiert das Daten vom Anwendungsprozess erhält die kleiner als die maximale Segmentgröße sind und das nur unidirektionalen Datentransfer vom Sender zum Empfänger durchführt. Das Programm muss die drei folgenden Ereignisse bewältigen: TCP erhält Daten vom Anwendungsprozess. Ein Timer ist abgelaufen. Ein ACK-Segment ist angekommen. Programm Sender while true do { Falls das TCP Daten vom Anwendungsprozess erhält dann Erzeuge ein TCP-Segment mit der Sequenznummer nextseqnum und den Anwendungsdaten als Inhalt Starte den Timer falls der Timer nicht läuft Übergebe das Segment an das IP-Protokoll zur Übertragung nextseqnum := nextseqnum + Länge (Anwendungsdaten) Falls der Timer einen Timeout auslöst dann Wiederhole die Übertragung des noch nicht bestätigten Segments mit der kleinsten Sequenznummer Starte Timer Falls eine Bestätigung (ACK) mit der Bestätigungsnummer30 \(N\) empfangen wird dann \(sendbase := N\) Starte erneut den Timer falls \( sendbase < nextseqnum\) \(/^\ast\) es gibt noch nicht bestätigte Segmente. \(^\ast /\) Falls \(N< sendbase\) dann wird die Bestätigung ignoriert Sonst \(/^\ast\) \(N= sendbase\) der Empfänger erwartet das Segment mit der Sequenznummer sendbase. \(^\ast /\) Erhöhe Anzahl der empfangenen duplizierten ACK für das Segment mit Sequenznummer \(N\) Falls Anzahl der empfangenen duplizierten ACK gleich drei ist wiederhole die Übertragung des Segments und setze die Anzahl zurück auf 0 \(/^\ast\) Fast Retransmit \(^\ast /\) } end-of-while Das Programm zeigt wie der TCP-Senderprozess die drei Ereignisse behandelt: Er empfängt Daten vom Anwendungsprozess verpackt diese in Segmente und übergibt sie zur Übertragung an die Vermittlungsschicht. Der Timer wird gestartet falls er momentan nicht aktiv ist. Dies kann der Fall sein wenn beispielsweise alle bisher gesendeten Segmente schon bestätigt wurden. Wenn der Timer abläuft dann wiederholt der Senderprozess die Übertragung des unbestätigten Segments mit der kleinsten Sequenznummer. Wenn ein Segment mit Bestätigungsnummer \(N\) im ACK-Feld ankommt dann prüft der Sender zuerst ob es sich um eine Bestätigung für ein bis jetzt noch nicht bestätigtes Segment handelt. Dies ist genau dann der Fall wenn \(N\) größer ist als die Sequenznummer des letzten vom Empfänger bestätigt Byte. Der Sender weiß dann dass der Empfänger alle Daten korrekt empfangen hat die vor der Sequenznummer \(N\) liegen. Deshalb kann der Sender seine Zustandsvariable sendbase die die Sequenznummer des letzten vom Empfänger noch nicht bestätigten Bytes enthält auf den neuen Wert \(N\) setzen.Falls aber vorher schon einmal eine Bestätigung mit demselben Wert \(N\) im ACK-Feld empfangen wurde (d.h. \(N \le\) sendbase) dann handelt es sich um ein sogenanntes Duplicate ACK. In diesem Fall ignoriert der Sender die Bestätigung falls \(N< \) sendbase ist. Falls \(N=\) sendbase weiß der Sender dass der Empfänger das Segment mit der Sequenznummer \(N\) nicht korrekt empfangen hat siehe auch Abbildung 3.25. Der Sender erhöht deshalb einen Zähler für die Anzahl der Duplicate ACK für das betroffene Segment. Tatsächlich entspricht ein Duplicate ACK einer negativen Bestätigung. Wenn der Sender drei Duplicate ACKs mit dem Wert \(N=\) sendbase erhalten hat dann nimmt er an dass das Segment mit Sequenznummer \(N\) verloren gegangen ist und überträgt es erneut - auch wenn der Timer noch nicht abgelaufen sein sollte. Der Zähler für die Anzahl der Duplicate ACKs für das wieder übertragene Segment wird auf Null zurückgesetzt. Man spricht deshalb auch von einem Fast Retransmit. Übungsaufgabe 3.17 Im TCP-Senderprogramm wird erst eine schnelle Wiederholung der Übertragung des Segments \(N\) ( Fast Retransmit) durchgeführt nachdem drei duplizierte ACK empfangen wurden. Warum wird nicht nach dem ersten Empfang des Duplikat-ACKs für ein Segment schon ein Fast-Retransmit ausgeführt? Aufgabe jetzt bearbeiten Flusskontrolle Das bis jetzt diskutierte TCP-Protokoll weist noch ein Problem auf. Jeder TCP-Prozess hat für jede Verbindung einen Empfangspuffer in den die korrekt angekommenen Anwendungsdaten in der richtigen Reihenfolge eingefügt werden. Aus diesem Puffer liest der Anwendungsprozess Daten und macht dadurch wieder Platz für neu ankommende Daten. Falls aber der Senderprozess schneller Daten sendet als sie vom Empfängerprozess gelesen werden wird ein endlich großer Puffer nach einiger Zeit überlaufen. Dieses Problem kann häufig auftreten da der Empfängerprozess durch andere Aktivitäten am Lesen der Daten gehindert werden kann oder sogar gerade gar nicht aktiv sein kann (weil z.B. das Betriebssystem den Prozess gerade nicht ausführt). Um dieses Problem zu lösen bietet TCP einen Flusskontrolldienst (Flow Control Service). Mit diesem Dienst kann der Empfänger die Senderate des Senders so verringern dass der Empfangspuffer nicht überläuft. Dazu hat jeder TCP-Prozess eine Variable die man Empfangsfenster (receive window) nennt siehe Abbildung 3.29. Dieses Empfangsfenster zeigt an wie viel freier Pufferplatz noch beim Empfänger vorhanden ist. Wann immer der Sender Daten sendet zieht er den zur Speicherung notwendigen Platz vom Empfangsfenster ab. Falls kein Patz mehr da ist sendet der Sender keine weiteren Daten. Damit dies funktioniert teilt der Empfänger dem Sender in jedem Segment das er an den Sender schickt in dem WINDOW-Feld des TCP-Headers mit wie viel freien Platz er im Puffer hat. Nun kann der Sender jeweils maximal so viele Daten senden wie noch Pufferplatz übrig ist. Wenn der Empfängerpuffer voll ist wartet der Sender bis er vom Empfänger ein Segment erhält in dem wieder neuer freier Platz gemeldet wird. Damit dies auch dann funktioniert wenn der Empfänger gerade keine Anwendungsdaten an den Sender zu schicken hat muss der Empfänger in diesem Fall ein leeres Segment ohne Dateninhalt an den Sender schicken in dem er den freien Empfangspufferplatz mitteilt. Sonst wäre der Sender blockiert und könnte trotz freien Empfangspuffers keine Daten mehr senden. Abbildung 3.29: Empfangsfenster. Es gibt noch einen weiteren Fall in dem die Verringerung der Senderate des Senders sinnvoll ist. Falls das Netzwerk überlastet ist macht es Sinn die Senderate zu verringern. Dies nennt man auch Überlastkontrolle (Congestion Control). Obwohl die Lösung des Überlastproblems auch die Reduzierung der Senderate erfordert handelt es sich um ein unterschiedliches Problem da es durch andere Ursachen entsteht. Vergleich UDP-TCP Die Transportsschicht des Internet bietet mit UDP und TCP zwei verschiedene Transportdienste an. TCP bietet einen zuverlässigen Datenübertragungsservice während UDP nur einen unzuverlässigen Datenübertragungsservice bietet. Wann sollte ein Anwendungsentwickler nun welches Protokoll benutzen? UDP weist neben der eingeschränkten Zuverlässigkeit auch eine Reihe von Vorteilen auf: UDP ist ein verbindungsloses Protokoll und kommt daher ohne Verbindungsaufbau und Verbindungsabbau aus UDP speichert keinen Verbindungszustand und kommt daher mit einer einfacheren Implementierung aus. Ein Serverprozess der mit UDP arbeitet kann daher deutlich mehr Clients bedienen UDP kommt mit einem Header von nur 8 Byte aus. Dies verringert den administrativen Aufwand gegenüber TCP das einen 20 Byte langen Header aufweist. Pro Segment kann UDP 12 Byte mehr übertragen UDP erlaubt Anwendungen so viele Daten pro Zeiteinheit zu versenden wie sie Daten generieren (abhängig von Algorithmus CPU etc.) und über die Netzwerkkarte in das Internet einspeisen können. Dies bedeutet jedoch nicht dass alle diese Daten auch beim Empfänger ankommen. Insbesondere kann es aufgrund von Überlast zu Nachrichtenverlusten kommen. Übungsaufgabe 3.18 Warum kann es bei Nutzung von UDP zu Nachrichtenverlusten kommen wenn das Netzwerk überlastet ist? Aufgabe jetzt bearbeiten Aufgrund obiger Vorteile wird UDP z.B. von den folgenden Internetanwendungen benutzt: Remote file server (NFS: Network File System) Streaming Multimedia Anwendungen die einen schnellen Datentransfer brauchen und mit Nachrichtenverlusten leben können (z.B. Video-Übertragungen) Internet Telephony Anwendungen die bei Nachrichtenverlusten eine etwas schlechtere (aber oft noch verständliche) Sprachqualität liefern können Network Management Anwendungen (SNMP: Simple Network Management Protocol [36]) kontrollieren Netzwerke die bei Überlastung des Netzes noch am ehesten UDP-Kommunikation durchführen können Das Routing Protocol (RIP: Routing Internet Protocol [37] [38]) versendet periodisch neue Routing-Tabellen (siehe Kurseinheit 4) die in der Vermittlungsschicht des Internet zur Steuerung des Paketflusses eingesetzt werden. Bei Verlust einer neuen Routing-Tabelle kommt das nächste Update schnell. Das Domain Name System (DNS [39][40]) übersetzt symbolische Namen in IP-Adressen siehe Abschnitt 3.13. Bei Verlust einer Anfrage wird die kurze Anfrage erneut gestellt oder an einen anderen Server gerichtet. TCP realisiert einen zuverlässigen bidirektionalen Datenübertragungsdienst mit Flusskontrolle. Deshalb entlastet TCP den Anwendungsprogrammierer von der expliziten Behandlung von Bit-Fehlern Paketverlusten Paketvertauschungen und Pufferüberläufen im Anwendungsprogramm. Mit diesen angenehmen Leistungen sind aber auch ein paar Nachteile verbunden: TCP ist ein verbindungsorientiertes Protokoll. Damit einher geht eine initiale Verzögerung bei der Datenübertragung (verursacht durch das 3-Wege-Handschlag-Verfahren beim Verbindungsaufbau). Dies ist eine der Hauptursachen für die Wartezeiten beim WWW denn bei HTTP Version 1.0 wird für jede Anforderungsnachricht eine neue Verbindung aufgebaut. TCP speichert den Verbindungszustand in der TCP-Implementierung auf Sender- und Empfängerseite. Diese Informationen (Puffer Zustandsvariable) werden zur Verwaltung der Verbindung und zur Erbringung der Diensteigenschaften benötigt. Daher kann ein Server der mit TCP arbeitet weniger Clients bedienen als ein gleich ausgestatteter Server der mit UDP arbeitet. Der TCP Header ist 20 Byte lang gegenüber 8 Byte bei UDP. TCP verbraucht also mehr Bandbreite für Verwaltungsinformation als UDP. TCP bietet eine Flusskontrolle die den Senderprozess am Verschicken zu vieler Nachrichten hindert. Dies ist dann ein Nachteil wenn eine Echtzeitanwendung zwar Paketverluste tolerieren kann aber immer eine minimale Senderate haben muss (z.B. bei der Übertragung von Live Video). TCP wird z.B. von den folgenden Internetanwendungen benutzt: E-Mail (SMTP [8]) Remote Terminal Access (Telnet [25] SSH) WWW (HTTP [18] HTTPS) File Transfer (FTP [9])  
2; mod_longpage;215; 4; 0; Kurstext Kurseinheit 4 Anja Haake Jörg M. Haake Christian Icking Lihong Ma 4 Vermittlung und Übertragung Diese Kurseinheit behandelt die tieferen Schichten des Schichtenmodells und Aspekte des Netzwerkkerns. Wir behandeln die Prinzipien und Konzepte der Vermittlungsschicht (Abschnitt 4.1 bis 4.4) und geben einen kurzen Überblick über die Sicherungsschicht (Abschnitt 4.5). Um die Behandlung der Schichten des Protokollstapels abzuschließen gehen wir in Abschnitt 4.6 kurz auf die unterste Schicht ein die Bitübertragungsschicht. In Abschnitt 4.7 weisen wir interessierte Leser auf mögliche Vertiefungen des Themas Rechnernetze hin.  
2; mod_longpage;215; 4; 1; 4.1 Aufgaben der Vermittlungsschicht In Abschnitt 3.4 haben wir gesehen wie die Transportschicht Kommunikationsdienste zwischen zwei Anwendungsprozessen bereitstellt die auf zwei verschiedenen Endsystemen laufen. Hierbei benutzt die Transportschicht die Dienste der Vermittlungsschicht. Die Aufgabe der Vermittlungsschicht (network layer) ist der Transport von Transportschicht-Nachrichten auch Segmente genannt von einem sendenden Host zu einem empfangenden Host (Host-zu-Host-Vermittlung): Aus Sicht der Transportschicht kommunizieren die beiden Hosts auf denen die beiden Anwendungsprozesse (Sender und Empfänger) laufen über die Vermittlungsschicht explizit miteinander. Die Vermittlung eines Segments in der Vermittlungsschicht von Host zu Host umfasst jedoch alle Transitsysteme (Paket-Switches) die auf dem Pfad liegen der die beiden Hosts verbindet. Dazu muss die Vermittlungsschicht die folgenden Teilaufgaben lösen: die Pfadermittlung (Routing): Hierbei muss die Vermittlungsschicht die Topologie der Router kennen und einen geeigneten Pfad bestimmen den ein Paket in dem ein Segment verpackt wurde vom Senderhost zum Empfängerhost nehmen soll. Die Vermittlung von Paketen: Hier muss für Pakete die bei einem Transitsystem ankommen zuerst entschieden werden über welche ausgehende Verbindung diese weitergesendet werden sollen. Call-Setup: in VC-Netzwerken (z.B. ATM oder X.25 siehe Abschnitt 3.2.2) müssen alle Transitsysteme auf einem Pfad bei Aufbau eines virtuellen Kanals richtig konfiguriert werden. Das Internet ist aber ein Datagramm-Netzwerk d.h. ein Host muss jedes einzelne Paket mit der Adresse des Zielhosts versehen und es dann ins Netzwerk einspeisen. Also wird hier kein Call-Setup benötigt. Deshalb wird diese Teilaufgabe hier nicht weiter behandelt. Das Dienstmodell auf der Vermittlungsschicht beschreibt die Eigenschaften die der Kommunikationsdienst der Vermittlungsschicht den nutzenden Prozessen bietet. Bei der Datagramm-Vermittlungsschicht des Internets wird ein verbindungsloser Best-Effort-Dienst zum Transport von Paketen zwischen Sender und Empfänger angeboten. Dieser Dienst weist die folgenden Eigenschaften auf: Pakete können verloren gehen Pakete können in einer anderen Reihenfolge empfangen werden als sie gesendet wurden über die Dauer des Transports eines Pakets bis zur Ankunft beim Empfänger wird keine Aussage gemacht es werden keine Zusagen über die Bandbreite zwischen Sender und Empfänger gemacht und es werden keine Informationen bzgl. einer Überlastung des Netzwerks an den Sender- oder Empfängerprozess weitergegeben. In einem VC-Netzwerk (z.B. ATM oder X.25) wird ein verbindungsorientiertes Dienstmodell realisiert. Bei einem verbindungsorientierten Dienst können zusätzliche Leistungen bzw. Eigenschaften für den Transport von Paketen zugesichert werden. Beispiele hierfür sind bei ATM-Netzwerken vier ATM-Dienstmodelle die bezüglich der oben erwähnten Eigenschaften weitergehende Garantien abgeben. So garantiert z.B. bei ATM das Constant-Bit-Rate-Dienstmodell eine angeforderte Bandbreite den vollständigen Transfer sämtlicher Pakete in der richtigen Reihenfolge und die Aufrechterhaltung der zeitlichen Abstände zwischen den Paketen. Übungsaufgabe 4.1 Warum kann das Internet nur einen Best-Effort-Dienst anbieten? Aufgabe jetzt bearbeiten In Abschnitt 4.2 behandeln wir zuerst die allgemeinen Grundprinzipien des Routing und danach in Abschnitt 4.3 als Konkretisierung die Vermittlungsschicht im Internet. Schließlich erwähnen wir in Abschnitt 4.4 noch die Erweiterungen welche die neue Version IPv6 in der Vermittlungsschicht bietet.  
2; mod_longpage;215; 4; 2; 4.2 Prinzipien der Pfadermittlung In jedem Datagramm- oder VC-Netzwerk muss die Vermittlungsschicht den Pfad (die Route siehe Abschnitt 3.2.2) für die Pakete festlegen die diese vom Sender zum Empfänger nehmen sollen. Die Berechnung dieses Pfades ist die Aufgabe des Routing-Protokolls. Im Routing-Protokoll legt der Routing-Algorithmus fest welchen Pfad ein Paket nehmen soll. Hierbei soll der Routing-Algorithmus einen guten Pfad aus der Menge aller möglichen Pfade zwischen dem Senderhost und dem Empfängerhost auswählen. Üblicherweise wendet man hier ein Kostenmodell an das jedem Pfad Gesamtkosten zuweist. Der Algorithmus soll dann den billigsten Pfad berechnen wobei in der Praxis auch noch weitere Kriterien zu berücksichtigen sind: z.B. sollen durch ein Firmennetzwerk keine Pakete aus den Netzwerken von konkurrierenden Firmen weitergeleitet werden. Grundlage zur Formulierung von Routing-Algorithmen sind gewichtete Graphen die das Netzwerk modellieren. Ein Graph besteht aus einer Menge von Knoten die durch Kanten miteinander verbunden sind. In gewichteten Graphen ist zusätzlich jeder Kante eine Zahl zugeordnet die ausdrückt welche Kosten entstehen wenn man diese Kante nutzt um zum direkten Nachbarknoten zu gelangen. Bei der Nutzung von Graphen für die Formulierung von Routing-Algorithmen stellen die Knoten die Transitsysteme dar in denen die Routing-Entscheidungen getroffen werden. Die Kanten repräsentieren die Kommunikationsverbindungen zwischen den Transitsystemen. Die Gewichte an den Kanten geben die Kosten der Nutzung dieser Verbindung an z.B. die aktuelle Wartezeit bei Nutzung der Verbindung oder die Länge der Verbindung (interkontinentale Verbindungen können teurer als lokale Verbindungen sein). Die Aufgabe eines Routing-Algorithmus ist nun die Ermittlung des billigsten Pfades zwischen einem Quellknoten und einem Zielknoten. In der Literatur wird diese Aufgabe auch oft als Wegauswahl bezeichnet man spricht dann auch von Wegauswahlverfahren. Die Kosten eines Pfads sind die Summe der Kosten der einzelnen Verbindungen auf dem Pfad. Die billigsten Kosten von einem Quellknoten zu einem Zielknoten sind das Minimum aller Kosten über alle möglichen Pfade zwischen der Quelle und dem Ziel. Abbildung 4.1 zeigt ein Beispiel für die Modellierung eines Netzwerks aus 6 Transitsystemen von \(A\) bis \(F\) und 10 Kommunikationsverbindungen. Der billigste Pfad zwischen \(A\) und \(C\) hat die Kosten 3 und führt von \(A\) über \(D\) und \(E\) zu \(C\). Abbildung 4.1: Modellierung eines simplen Netzwerks. Übungsaufgabe 4.2 Wie lautet der billigste Pfad von \(A\) zu \(F\) in Abbildung 4.1? Aufgabe jetzt bearbeiten Wie haben Sie diesen Pfad ermittelt? Meistens probiert man ein paar Pfade von \(A\) nach \(F\) aus und wählt dann den billigsten. Aber haben Sie auch alle 17 möglichen Wege systematisch ausprobiert? Dabei ist diese Berechnung ein Beispiel für einen zentralisierten Routing-Algorithmus: der Algorithmus wurde an einer Stelle durchgeführt nämlich durch Sie und hatte komplette Information über das Netzwerk nämlich Abbildung 4.1. Wir können Routing-Algorithmen anhand der folgenden Unterscheidungsmerkmale klassifizieren: Global - dezentral: Ein globaler Routing-Algorithmus besitzt vollständiges Wissen über das Netzwerk. Er kennt also alle Verbindungen und alle Kosten. Diese Informationen muss der Routing-Algorithmus irgendwie sammeln bevor ein Pfad berechnet werden kann. Die eigentliche Berechnung kann sowohl zentral an einer Stelle als auch verteilt an mehreren Stellen stattfinden. Solche Algorithmen mit globalen Zustandinformationen heißen deshalb Link-State-Algorithmen. Ein dezentraler Routing-Algorithmus berechnet einen Pfad mit einem iterativen verteilten Verfahren ohne dass jeder Knoten die Verbindungskosten des gesamten Netzwerks kennen muss. Stattdessen kennt jeder Knoten nur die Informationen über seine direkten Verbindungen. Mittels Informationsaustausch mit seinen Nachbarn im Graphen kann der Knoten sukzessive den billigsten Pfad zu einem Zielknoten berechnen. Ein Beispiel für solch einen Algorithmus ist der Distanzvektor-Algorithmus bei dem jeder Knoten nur weiß zu welchem Nachbarn ein Paket weitergeleitet werden muss um einen speziellen Zielknoten zu erreichen und was die Kosten des Pfades von ihm selbst zum Zielknoten sind. Statisch - dynamisch: Ein statischer Routing-Algorithmus ändert seine Routing-Entscheidungen nur aufgrund manueller Eingaben z.B. dem Editieren von Routing-Tabellen in Routern. Dadurch kann ein statischer Routing-Algorithmus nur langsam an veränderte Netzwerksituationen angepasst werden z.B. bei Ausfall von Verbindungen. Ein dynamischer Routing-Algorithmus passt sich automatisch an veränderte Netzwerksituationen an wie z.B. Netzwerkauslastung oder Ausfall von Verbindungen. Die Anpassung kann hierbei entweder periodisch durchgeführt oder durch Zustandsveränderungen im Netzwerk ausgelöst werden. Im Internet sind nur zwei Verfahren gebräuchlich: ein dynamischer globaler Link-State-Algorithmus siehe Abschnitt 4.8 und ein dynamischer dezentraler Distanzvektor-Algorithmus siehe Abschnitt 4.9. Routing-Algorithmen für andere Netzwerktypen werden hier nicht weiter besprochen. Sehen Sie hierzu bei Bedarf in der Fachliteratur nach z.B. Kapitel 4 bei Kurose und Ross [1]. Globaler Link-State-Algorithmus Voraussetzung für den Algorithmus ist dass alle Knoten alle Informationen über die Netzwerktopologie und die Verbindungskosten vorliegen haben. Dies wird in der Praxis dadurch erreicht dass jeder Knoten die Informationen über die Kosten der an ihn angeschlossenen Verbindungen in sogenannten Link-State-Broadcast-Nachrichten an alle anderen Knoten verschickt (Broadcast). Wenn alle Knoten alle ihre Link-State-Broadcast-Nachrichten verschickt haben dann hat jeder Knoten die komplette Information über die Netzwerktopologie und die Verbindungskosten vorliegen. Damit kann nun jeder Knoten die billigsten Pfade zu allen anderen Knoten berechnen. Dijkstras iterativer Algorithmus berechnet den billigsten Pfad von einem gegebenen Knoten (der Quelle \(Q\)) zu allen anderen Knoten im Netzwerk. Nach der \(k\)-ten Iteration wurden die billigsten Pfade zu \(k\) Zielknoten berechnet. Diese \(k\) Pfade haben die \(k\) billigsten Kosten in der Menge aller billigsten Pfade zu den \(k\) Zielknoten. Es gelte die folgende Notation: \(c(i j)\ge 0\) bezeichnet die Kosten der direkten Verbindung von Knoten \(i\) zu Knoten \(j\). Falls keine direkte Verbindung existiert dann ist \(c(i j) =\infty\). Außerdem setzen wir die Symmetrie-Eigenschaft voraus d.h. \(c(i j) = c(j i)\). \(d_i\) bezeichnet die Kosten des Pfades vom Quellknoten \(Q\) zum Zielknoten \(i\) der in dieser Iteration die billigsten Kosten hat. \(P_i\) bezeichnet den Vorgängerknoten (einen Nachbarn von Knoten \(i\)) auf dem gegenwärtig billigsten Pfad vom Quellknoten zum Zielknoten \(i\). \(S\) bezeichnet die Menge der Knoten bei denen die billigsten Pfade vom Quellknoten bereits bekannt sind. Der Algorithmus besteht aus einer Initialisierung und der Iteration (d.h. einer Schleife). Nach der Beendigung des Algorithmus sind die billigsten Pfade vom Quellknoten zu allen anderen Knoten bekannt. Initialisierung: \(S = \{Q\}\) \(/^\ast\) \(Q\) ist der Quellknoten an dem die Berechnung ausgeführt wird \(^\ast /\) Für alle Knoten \(i\) des Netzwerkes begin Falls \(i\) direkter Nachbar von \(Q\) ist dann \(d_i = c(Q i) \) \(P_i=Q\) sonst \( d_i = \infty\). end Iteration: Wiederhole solange bis alle Knoten des Netzwerkes in \(S\) sind begin Finde ein \(i\notin S\) mit \(d_i=\min\limits_j\notin S \{d_j\}\) Füge \(i\) zu \(S\) hinzu Update \(d_j\) für alle direkten Nachbarn \(j\notin S\) von \(i\) Falls \(d_i+c(i j)< d_j\) ist dann \( d_j = d_i+c(i j) \) \(P_j=i\) \(/^\ast\) die neuen Kosten von \(j\) sind entweder die alten Kosten oder die bekannten billigsten Kosten nach \(i\) plus den Kosten von \(i\) nach \(j\) \(^\ast/\) end Wir betrachten nun das Netzwerk in Abbildung 4.1 und berechnen die billigsten Pfade vom Quellknoten \(A\) zu allen möglichen Zielknoten.1 Die Ergebnisse für jeden Iterationsschritt sind in Tabelle 4.1 dargestellt: Tabelle 4.1: Zwischenergebnisse des Dijkstra-Algorithmus für Abbildung 4.1. Schritt \(S\) \(B\) \(P_B\) \(C\) \(P_C\) \(D\) \(P_D\) \(E\) \(P_E\) \(F\) \(P_F\) 1 \(A\) 2 \(A\) 5 \(A\) 1 \(A\) \(\infty\) \(\infty\) 2 \(A D\) 2 \(A\) 4 \(D\) 2 \(D\) \(\infty\) 3 \(A D E\) 2 \(A\) 3 \(E\) 4 \(E\) 4 \(A D E B\) 3 \(E\) 4 \(E\) 5 \(A D E B C\) 4 \(E\) 6 \(A D E B C F\) Bei Terminierung des Algorithmus liegt für jeden Knoten sein Vorgänger auf dem billigsten Pfad vom Quellknoten vor. Für jeden Vorgänger kennen wir außerdem dessen Vorgänger. So können wir die gesamten Pfade vom Quellknoten zu allen Zielknoten rekonstruieren wir erhalten jetzt einen Graphen ohne geschlossene Pfade (Zyklen) siehe Abbildung 4.2. So einen Graphen bezeichnen wir als Baum. Abbildung 4.2: Die kürzesten Pfade von \(A\) zu allen Knoten bilden einen Baum mit der Wurzel \(A\) ein Teilgraph des Netzwerks aus Abbildung 4.1. Die Zeitkomplexität2 dieses Algorithmus ist \(O(n^2)\) für \(n\) Knoten um den billigsten Pfad von einem Quellknoten zu allen \(n-1\) anderen Knoten zu berechnen. Der Algorithmus muss von außen neu angestoßen werden um neue Pfade zu berechnen wenn sich zum Beispiel die Auslastung (Kosten) von Verbindungen oder die Netzwerktopologie geändert hat. Dezentraler Distanzvektor-Algorithmus Der Distanzvektor-Algorithmus ist ein dezentrales iteratives und asynchrones Verfahren zur Berechnung der billigsten Pfade: Dezentral: Jeder Knoten empfängt Informationen von seinen direkten Nachbarn führt eine Berechnung durch und sendet gegebenenfalls das Ergebnis an seine direkten Nachbarn zurück. Iterativ: Das Verfahren läuft so lange wie noch Informationen zwischen Nachbarn ausgetauscht werden müssen. Asynchron: Die Knoten tauschen Informationen aus wenn sie es müssen d.h. wenn sich bei ihnen eine Änderung der billigsten Pfade ergibt. Andere Knoten müssen auf diese Informationen nicht innerhalb einer bestimmten Zeit reagieren. Grundlegende Datenstruktur des Algorithmus ist die Distanztabelle die an jedem Knoten gespeichert ist. Die Distanztabelle enthält eine Zeile für jeden Zielknoten und eine Spalte für jeden seiner direkten Nachbarn. Jede Zelle \(d_X(Z N)\) der Matrix gibt an zu welchen Kosten der Zielknoten \(Z\) vom aktuellen Knoten \(X\) über den Nachbarknoten \(N\) erreichbar ist. Diese Kosten entsprechen den Kosten \(c(X N)\) der direkten Verbindung von Knoten \(X\) zum Nachbarknoten \(N\) plus den billigsten Kosten über einen Pfad vom Nachbarknoten \(N\) zum Zielknoten \(Z\) also \( d_X(Z N) = c(X N) + \min \limits_W \{d_N(Z W)\} \) wobei das Minimum über alle Nachbarknoten \(W\) von \(N\) genommen wird. Jeder Knoten soll die Kosten des billigsten Pfads jedes seiner Nachbarn zu jedem Ziel kennen. Trägt ein Knoten neue billigste Kosten in seiner Distanztabelle ein so muss er deshalb seine direkten Nachbarn darüber informieren. Ein Beispiel soll die Bedeutung der Distanztabelle verdeutlichen. In Abbildung 4.3 ist ein Netzwerkmodell und die dazugehörige Distanztabelle für den Knoten \(E\) angegeben. Betrachten wir zuerst die Zeile für Zielknoten \(A\): Der billigste Pfad von \(E\) nach \(A\) geht über die direkte Verbindung zu \(A\): die Kosten sind 1. Deshalb ist \(d_E(A A) = 1\). Die Kosten des billigsten Pfades von \(E\) nach \(A\) direkt über den Nachbarknoten \(D\) ergibt sich zu 5: die Kosten von \(E\) nach \(D\) sind 2 plus den billigsten Weg von \(D\) nach \(A\) (nämlich zurück zu \(E\) und weiter zu \(A\)) mit den Kosten 3. Die Schleife die denselben Knoten (hier \(E\)) mehrfach benutzt ist die Quelle einiger Probleme wie wir noch sehen werden. Der billigste Pfad von \(E\) nach \(A\) direkt über den Nachbarknoten \(B\) ergibt sich zu 14. Abbildung 4.3: Die Distanztabelle für Knoten \(E\). Die billigsten Kosten zum entsprechenden Ziel über einen entsprechenden Nachbarn sind fett und unterstrichen dargestellt. Übungsaufgabe 4.3 Warum ist 14 das Ergebnis für den billigsten Pfad von \(E\) nach \(A\) direkt über den Nachbarn \(B\) in Abbildung 4.3 und nicht \(15=8+7\)? Aufgabe jetzt bearbeiten In der Distanztabelle in Abbildung 4.3 sind die billigsten Pfade von \(E\) zu jedem Zielknoten fett markiert. Aus seiner Distanztabelle kann also jeder Knoten seine Routing-Tabelle erstellen d.h. die Minima jeder Zeile. Diese Tabelle gibt an über welchen Nachbarn die Pakete an ein bestimmtes Ziel weitergeleitet werden sollen. Damit die Routing-Tabellen verteilt berechnet werden können also ohne Kenntnis des gesamten Graphen tauschen beim Distanzvektor-Algorithmus nur die direkten Nachbarn Informationen aus. Hierzu benutzen sie den sogenannten verteilten Bellman-Ford-Algorithmus der an jedem Knoten \(X\) ausgeführt wird. Er besteht aus einer Initialisierungsphase und einer Iterationsphase. Initialisierung: 01 Für alle benachbarten Knoten \(V\) tue: 02 begin 03 \(d_X(\ast V)=\infty\) \(/^\ast\) der \(\ast\)-Operator bedeutet für alle Zeilen \(^\ast/\) 04 \(d_X(V V) = c(X V)\) 05 end 06 Für alle Zielknoten \(Z\) tue: 07 begin 08 Sende \(\min\limits_W \{d_X(Z W)\}\) an jeden direkten Nachbarn 09 end Iterationsphase: 01 Wiederhole für immer 02 begin 03 Warte bis entweder eine Kostenveränderung der Verbindung zum 04 direkten Nachbarn \(V\) bemerkt wird oder bis ein Update der 05 Verbindungskosten vom Nachbarn \(V\) eintrifft. 06 Falls \(c(X V)\) sich um die Differenz \(\delta\) geändert hat 07 begin 08 Für alle Zielknoten \(Z\) tue: 09 begin 10 \(/^\ast\) addiere die positive oder negative Differenz zu allen 11 Kosten der Wege die über \(V\) führen \(^\ast/\) 12 \(d_X(Z V) = d_X(Z V) + \delta\) 13 end 14 end 15 Sonst falls ein Update von Nachbar \(V\) bezüglich Zielknoten \(Z\) 16 empfangen wird 17 \(/^\ast\) d.h. der billigste Pfad von \(V\) nach \(Z\) hat sich geändert: \(V\) hat 18 einen neuen Wert \(neuerWert = \min\limits_W \{d_V(Z W)\}\) geschickt. \(^\ast/\) 19 begin 20 \( d_X(Z V) = c(X V) + neuerWert\) 21 end 22 Falls ein neues \(\min\limits_W \{d_X(Z W)\}\) für einen Zielknoten \(Z\) existiert 23 begin 24 sende den neuen minimalen Kostenwert \(\min\limits_W \{d_X(Z W)\}\) für \(Z\) 25 an alle direkten Nachbarn. 26 end 27 end Der Knoten \(X\) verändert seine Distanztabelle wenn er eine Kostenveränderung einer seiner abgehenden Verbindungen bemerkt (Zeilen 6-14) oder wenn er ein Update von einem direkten Nachbarn erhält (Zeilen 15-21). Wenn ein neuer billigster Pfad ermittelt wurde schickt der Knoten diesen Wert an alle direkten Nachbarn (Zeilen 22-26). Der aktuelle Knoten kennt die Werte \(\min\limits_W\{d_V(Z W)\}\) ausschließlich aufgrund der Updates die er vom Nachbarknoten \(v\) erhalten hat. Sobald keine neuen minimalen Kosten mehr ermittelt werden werden auch keine Update-Nachrichten mehr verschickt: jeder Knoten wartet bei der Warte-Anweisung in der Schleife (Zeilen 3-5) bis sich die Kosten einer Verbindung ändern. Wenn sich Verbindungskosten ändern dann aktualisiert der Knoten seine Distanztabelle und benachrichtigt seine Nachbarn falls ein neuer billigster Pfad berechnet wurde. Diese Propagation von neuen Werten kommt üblicherweise relativ rasch zum Stillstand. Abbildung 4.4 zeigt ein einfaches Netzwerk und den Aufbau der zugehörigen Routing-Tabellen der bereits nach der zweiten Iteration konvergiert: Nach dem ersten Austausch von Routing-Informationen ändern sich nur noch die Werte für \(d_B\) 3 woraus sich aber keine neuen Minima-Informationen ergeben die weitergeschickt werden müssten. Abbildung 4.4: Verteilter Bellman-Ford-Algorithmus auf einem Netzwerk:\brkcptindt (a) Distanztabellen nach der Initialisierung\brkcptindt (b) nach der ersten Iteration\brkcptindt (c) nach der zweiten Iteration Übungsaufgabe 4.4 Wir betrachten den Graphen in Abbildung 4.3 und wollen einen Schritt des dezentralen Distanzvektor-Algorithmus ablaufen lassen. Knoten \(A\) und \(B\) haben ihre Distanztabellen schon wie folgt initialisiert: Wie sehen die aktualisierten Distanztabellen aus nachdem Knoten \(B\) und \(A\) ihre Informationen ausgetauscht haben? Bitte füllen Sie die vorgegebenen Tabellen aus: Aufgabe jetzt bearbeiten Ein typisches Problem kann auftreten wenn sich die Kosten einer Verbindung erhöhen und sich dadurch eine Routing-Schleife bildet siehe Abbildung 4.5: Wir nehmen an dass sich die Verbindungskosten der Verbindung von \(B\) nach \(A\) plötzlich auf 60 erhöhen siehe Abbildung 4.5 (1). \(B\) erkennt nun dass der Pfad nach \(A\) über \(C\) billiger ist nämlich 6 als der direkte Weg zu \(A\). Gemäß des verteilten Bellman-Ford-Algorithmus informiert \(B\) nun \(C\) über dieses neue Minimum. Knoten \(C\) wird seine Kosten für den Weg zu \(A\) über \(B\) jedoch nur auf 7 erhöhen siehe Abbildung 4.5 (1): Aus der Sicht von \(C\) gelangt man am besten zu \(A\) über \(B\) indem man direkt zu \(B\) gelangt also 1 plus die Kosten die \(B\) braucht um zu \(A\) zu gelangen nämlich 6. Knoten \(C\) schickt also weiterhin Pakete an \(A\) über den Knoten \(B\) da ja die Verbindungskosten von 7 immer noch billiger sind als die direkte Verbindung von \(C\) nach \(A\) in Höhe von 50. In diesem Zustand der Routing-Tabellen siehe Abbildung 4.5 (1) werden die Pakete von \(C\) nach \(A\) endlos zwischen \(C\) und \(B\) hin und her geschickt. Diese Situation bezeichnet man auch als Routing-Schleife. Solange die Routing-Tabellen nicht die global richtige Situation widerspiegeln werden die Pakete niemals ihr Ziel erreichen! Die Situation verbessert sich erst schrittweise durch die Aktualisierung der Tabellen gemäß des verteilten Bellman-Ford-Algorithmus. Abbildung 4.5: Das Entstehen einer Routing-Schleife und das Count-to-Infinity-Problem. Tatsächlich erhöhen sich die Verbindungskosten in unserem Beispiel nur jeweils um den Wert 1 bei jedem Update da jeweils nur die Kosten der direkten Verbindung zwischen \(C\) und \(B\) in Höhe von 1 auf die Kosten der Verbindung von \(C\) nach \(A\) addiert werden siehe Abbildung 4.5 (2) (3). Insgesamt müssen 45 Updates zwischen \(B\) und \(C\) ausgetauscht werden bevor \(C\) letztendlich feststellt dass die Verbindung zu \(A\) über \(B\) teurer ist als seine direkte Verbindung zu \(A\). Im Allgemeinen gilt: erhöhen sich die Kosten einer Verbindung so wird diese schlechte Neuigkeit erst langsam durch das Netzwerk propagiert. Die anderen Knoten werden solange noch Pakete weiter über die nun schlechte (da teure) Verbindung schicken bis bei ihnen neue Werte angekommen sind die schlechter als andere alternative Pfade sind. Im Extremfall erhöht sich der propagierte neue Wert pro Iteration aber nur um 1 siehe Abbildung 4.5. Es braucht also entsprechend viele Iterationen bis ein anderer kostengünstigerer Pfad berechnet wird. Bis dahin wird der Verkehr noch über die schlechte Verbindung abgewickelt. Dieses Problem heißt in der Literatur auch Count-to-Infinity-Problem. Um das Problem der Routing-Schleife zu vermeiden kann die sogenannte Poisoned-Reverse-Strategie angewendet werden: Wenn der Knoten \(C\) Pakete zum Zielknoten \(A\) über einen anderen Knoten hier \(B\) leitet obwohl er selbst eine direkte Verbindung zum Zielknoten hat dann teilt er \(B\) einfach mit dass die Kosten der direkten Verbindung von \(C\) zum Zielknoten \(A\) bei \(\infty\) liegen siehe Abbildung 4.6 (1). Abbildung 4.6: Knoten \(C\) und \(B\) wenden die Poisoned-Reverse-Strategie an. So wird \(B\) niemals Nachrichten an \(A\) über den Knoten \(C\) schicken! Sobald die Verbindungskosten über \(B\) teurer werden als die Kosten der direkten Verbindung zum Zielknoten zum Beispiel wenn der Verbindung von \(B\) nach \(A\) plötzlich die Kosten 60 zugewiesen werden dann wird der Knoten \(C\) die wirklichen Kosten seiner Verbindung zu \(A\) an \(B\) schicken nämlich hier den Wert 50 siehe Abbildung 4.6 (1). Knoten \(B\) wird dann seinen Verkehr zum Zielknoten \(A\) über den Knoten \(C\) abwickeln da \(A\) dann über \(C\) zu den Kosten von 51 erreichbar ist und das billiger als die eigenen direkten Verbindungskosten von \(B\) nach \(A\) von 60 ist siehe Abbildung 4.6 (2). Schleifen die sich aus mehr als zwei Knoten zusammensetzen werden durch diese Strategie jedoch nicht bemerkt. Zudem löst diese Strategie auch nicht immer das Count-to-Infinity-Problem. Dies wird an folgendem Beispiel deutlich: Ein Netzwerk besteht aus vier Knoten \(A\) \(B\) \(C\) und \(D\) und den Verbindungen \(A\)-\(B\) \(B\)-\(C\) und \(C\)-\(D\) mit Kosten von jeweils \(1\) und \(A\)-\(D\) mit Kosten 50 siehe auch Abbildung 4.7. Die Knoten kennen die folgenden Informationen über die Kosten um zu Knoten \(A\) zu gelangen: Knoten \(B\) erreicht \(A\) direkt mit den minimalen Kosten \(1\). Knoten \(C\) erreicht \(A\) über \(B\) mit den minimalen Kosten \(2\). Knoten \(D\) erreicht \(A\) über \(C\) mit den minimalen Kosten \(3\). Knoten \(D\) verwendet nun die Poisoned-Reverse-Strategie und teilt \(C\) mit dass \(A\) über ihn mit den Kosten \(\infty\) erreicht wird da \(D\) weiß dass er \(A\) mit Kosten 3 über \(C\) viel billiger als die direkte Verbindung erreichen kann. Abbildung 4.7: Die Distanztabellen von Knoten \(B\) \(C\) und \(D\) um Zielknoten \(A\) zu erreichen. Knoten \(D\) verwendet die Poisoned-Reverse-Strategie. Wenn nun die Verbindung \(A\)-\(B\) unterbrochen wird siehe Abbildung 4.8 dann teilt Knoten \(B\) Knoten \(C\) mit dass \(A\) über ihn mit Kosten \(3\) erreichbar ist siehe Schritt \((1)\) in Abbildung 4.8. \(C\) aktualisiert die Kosten nach \(A\) mit dem neuen Wert \(4\) in Schritt (2). Knoten \(C\) teilt \(B\) und \(D\) mit dass \(A\) über ihn mit Kosten \(4\) erreichbar ist. Knoten \(B\) und \(D\) erhöhen die Kosten nach \(A\) jeweils in Schritt (3) um 1. Jetzt haben wir das Count-to-Infinity-Problem. Abbildung 4.8: Ein Count-to-Infinity-Problem entsteht obwohl Knoten \(D\) die Poisoned-Reverse-Strategie verwendet. Vergleich Link-State- versus Distanzvektor-Algorithmus Wir vergleichen die beiden Algorithmen bezüglich ihrer Komplexität Konvergenzgeschwindigkeit d.h. der Geschwindigkeit in der die Algorithmen zu einer stabilen Lösung führen und Robustheit. Dabei gehen wir von einem Netzwerk mit \(n\) Knoten und \(L\) Verbindungen aus. Komplexität bzgl. Nachrichtenmenge: Link-State-Algorithmus: Das Senden von \(O(nL)\) Nachrichten sind notwendig damit jeder Knoten den gesamten Graphen mit Kosten kennt. Bei der Verbreitung der Nachricht wird die Fluten-Methode verwendet. Fluten funktioniert wie folgt: Zuerst wird ein Nachricht-Paket mit der Identität (IP-Adresse) des Knotens mit Kosten zu direkten Nachbarn und mit einer Sequenznummer für das Paket an die Nachbarn gesendet. Die Übertragung kann zuverlässig gemacht werden indem der Sender und der Empfänger den Bestätigungs- und Neuübertragungsmechnismus verwenden. Wenn ein weiterer Knoten das Paket empfängt speichert er das Paket falls er noch nie so ein Paket mit der Identität eines Knotens erhalten hat. Verfügt er bereits über eine Kopie vergleicht er die beiden Sequenznummern. Nur wenn das neue Paket eine größere Sequenznummer besitzt ersetzt er die Kopie durch das neue Paket da er vermutet dass das neu angekommene Paket wirklich neu sein muss. Ist das empfangene Paket das neue sendet der Knoten jetzt eine Kopie an alle seine Nachbarn außer an den von dem er das Paket vorher empfangen hat. Also wird ein Link-State-Paket nie zu dem Knoten zurückgesendet von dem es empfangen wurde. Das garantiert dass das Fluten terminiert. Distanzvektor-Algorithmus: In jeder Iteration müssen Nachrichten nur dann geschickt werden wenn sich ein anderer billigster Weg zu einem Knoten ergibt und zwar nur an die direkten Nachbarn. Konvergenzgeschwindigkeit: Link-State-Algorithmus:Bei jedem Knoten wird der Dijkstra-Algorithmus mit \(O(n^2)\) Operationen ausgeführt um den billigsten Pfad von ihm zu allen \(n-1\) anderen Knoten zu berechnen. Der Algorithmus terminiert und hat eine Laufzeit \(O(n^2)\). Insbesondere berechnet jeder Knoten seine Routing-Tabelle mit Dijkstra-Algorithmus einmal dann bleibt die Tabelle stabil erhalten bis die nächste Veränderung passiert. Es gibt beim Linkstate-Algorithmus keine Konvergenzphase. Distanzvektor-Algorithmus: Beim Distanzvektor-Algorithmus findet eine Konvergenzphase statt in der Updates immer wieder durchgeführt werden müssen. Während der Konvergenzphase bleibt die Routing-Tabelle nicht stabil und es ist nicht zu sehen wie lange die Veränderung der Tabelle dauert. Während der Konvergenzphase können Routing-Schleifen entstehen die durch das Count-to-Infinity-Problem verursacht werden.Der Algorithmus reagiert schnell auf gute Nachrichten und konvergiert aber langsam bei schlechten Nachrichten. Robustheit: Wenn ein Knoten ausfällt oder fehlerhaft ist dann werden seine Nachbarknoten es merken. Link-State-Algorithmus: Jeder Nachbar wird eine neue Link-State-Nachricht im ganzen Netz verbreiten d.h. dieser fehlerhafte Knoten existiert nicht mehr im Netz. Danach berechnet jeder Knoten seine eigene neue Routing-Tabelle. Hierdurch entsteht ein gewisses Ausmaß von Robustheit des verteilten Systems. Der Algorithmus wird in der Praxis recht häufig verwendet. Distanzvektor-Algorithmus: Jeder Nachbarknoten des ausgefallenen Knotens wird seine Routing-Tabelle aktualisieren und die neuen Kosten seinen Nachbarn mitteilen die wiederum ihre Routing-Tabellen aktualisieren und die neuen Kosten an ihre Nachbarn weitergeben. In dieser Zeit können Routing-Schleifen entstehen bei denen Pakete im Kreis herum geschickt werden dadurch kann das ganze Netz zusammenbrechen. Der Link-State-Algorithmus ist besser als der Distanzvektor-Algorithmus wenn man nur die Komplexität Robustheit und Konvergenzgeschwindigkeit betrachtet. Leider kann man so einen Algorithmus nicht für ein großes Netzwerk verwenden da jeder Knoten den gesamten Graphen des Netzwerks kennen und speichern muss. Wenn man einen Graphen als Adjazenzmatrix speichert dann benötigt jeder Knoten \(O(n^2)\) Speicherplatz. Wenn man einen Graphen in Form von Adjazenzlisten speichert d.h. jeder Knoten im Graphen hat eine Liste von Nachbarn dann hat jeder Knoten \(O(nL)\) Speicherbedarf. Hierarchisches Routing In der bisherigen Diskussion haben wir das Netzwerk als Menge verbundener Transitsysteme angesehen. Jedes Transitsystem wendet denselben Routing-Algorithmus an um Pfade durch das gesamte Netzwerk zu berechnen. In der Praxis funktioniert diese Sicht aus zwei Gründen nicht: Aufgrund der schieren Größe realer Netzwerke z.B. des Internets (Millionen von Routern) ist der Aufwand für die Berechnung Speicherung und Weiterleitung von Routing-Tabellen-Informationen nicht mehr tragbar. Ein Distanzvektor-Algorithmus mit Iterationen durch Millionen von Routern würde wohl kaum konvergieren. Deshalb muss die Komplexität der Pfadberechnung in großen Netzwerken reduziert werden. Organisationen wollen ihre Netzwerke so betreiben wie sie es wollen. Sie wollen z.B. einen Routing-Algorithmus auswählen der auf ihre Bedürfnisse am besten passt. Trotzdem sollte das Netzwerk mit anderen außenstehenden Netzwerken verbunden werden können. Beide Probleme können durch das Konzept der autonomen Systeme gelöst werden. In einem autonomen System (AS) wird eine Menge von Transitsystemen zusammengefasst die alle denselben Routing-Algorithmus benutzen der auch Intra-AS-Routing genannt wird. Sogenannte Gateways oder Gateway-Router verbinden verschiedene autonome Systeme. Gateways sind spezielle Transitsysteme in einem AS die Pakete an Zielknoten außerhalb des autonomen Systems weiterleiten. Ein Inter-AS-Routing-Protokoll wird für Gateways benötigt um Pakete von einem autonomen System an ein anderes weiterzuleiten. Dieser Ansatz begrenzt die Komplexität der Pfadberechnung da innerhalb eines autonomen Systems nur die Größe des eigenen homogenen Netzwerks bewältigt werden muss. Zwischen autonomen Systemen müssen nur die Gateways mit der Anzahl verbundener anderer autonomer Systeme fertig werden. Die Betreiber von autonomen Systemen sind frei in der Wahl ihres Intra-AS-Routing-Protokolls solange die Inter-AS-Routing-Protokolle auf den Gateways mit den Gateways der verbundenen anderen autonomen Systeme kompatibel sind. Ein Gateway-Router muss daher sowohl das Intra-AS-Routing-Protokoll ausführen damit es im autonomen System erreichbar ist als auch das Inter-AS-Routing-Protokoll zur Kommunikation mit den Gateways der verbundenen anderen autonomen Systeme. Wenn ein Paket an eine Adresse außerhalb des eigenen autonomen Systems gesendet werden soll dann muss das Paket lediglich innerhalb des autonomen Systems an ein passendes Gateway geschickt werden. Dieses Gateway wendet dann sein Inter-AS-Routing-Protokoll an und schickt das Paket zu einem anderen Gateway. Dieses schickt das Paket dann mit seinem Intra-AS-Routing-Protokoll im eigenen autonomen System weiter. Es ist wichtig anzumerken dass ein Paket durchaus durch mehrere autonome Systeme geschleust werden kann bevor es das autonome System erreicht in dem sein Zielknoten liegt. Wegen der so entstehenden Hierarchie von Routing (Intra-AS-Routing als Grundlage und darüber die Schicht des Inter-AS-Routing) nennt man diesen Ansatz auch hierarchisches Routing.  
2; mod_longpage;215; 4; 3; 4.3 Die Vermittlungsschicht im Internet In diesem Abschnitt betrachten wir die aktuelle Vermittlungsschicht des Internets die IP-Schicht. Sie bietet einen verbindungslosen Datagramm-Dienst. Nach Übergabe eines Segments von der Transportschicht an die Vermittlungsschicht des Senders wird es in ein oder mehrere IP-Datagramme verpackt und mit den Adressen des Senders und Empfängers versehen. Jedes Datagramm wird an den ersten Router auf dem Pfad zum Empfänger geschickt. Die Vermittlungsschicht im Internet besteht aus drei Komponenten die wir in den folgenden Abschnitten genauer behandeln werden: Das Internetprotokoll der derzeit noch oft benutzten Version IPv4 [2] definiert die Adressierung in der Vermittlungsschicht die Felder eines Datagrammes und die Aktionen die durch Router und Endsysteme auf einem Datagramm in Abhängigkeit von den Inhalten seiner Felder ausgeführt werden. Die Pfadbestimmungskomponente bestimmt den Pfad den ein Datagramm auf dem Weg von dem Sender zum Empfänger durch das Internet nimmt. Das Protokoll ICMP (Internet Control Message Protocol [3]) ist die Komponente des Internets die es erlaubt Fehler in Datagrammen anzuzeigen und Informationen über die Vermittlungsschicht abzufragen. Das IP-Protokoll IPv4 Um die Paketvermittlung im Internet zu verstehen betrachten wir zuerst die Adressierung im Internet. Danach gehen wir darauf ein wie Endgeräte und Router im Internet ein Datagramm transportieren. Zum Abschluss behandeln wir dann wie ein IP-Datagramm auf seiner Reise von Router zu Router je nach Bedarf fragmentiert und wieder zusammengesetzt werden kann. Adressierung in IPv4 Ein Endgerät besitzt typischerweise genau eine Verbindung zum Netzwerk. Die Grenze zwischen dem Endgerät und der physischen Verbindung wird Schnittstelle (Interface) genannt. Im Gegensatz zu einem Endgerät verfügt ein Router über mehrere physische Verbindungen. Jede Grenze zwischen dem Router und einer seiner physischen Verbindungen wird ebenso Interface genannt. Jedes Interface benötigt eine global eindeutige IP-Adresse um IP-Datagramme senden und empfangen zu können. IP-Adressen sind also eher Interfaces zugeordnet als den Hosts oder Routern die das Interface enthalten. Es gibt 5 Klassen von IP-Adressen in IPv4. Jede IP-Adresse ist 4 Byte lang entsprechend 32 Bit siehe Abbildung 4.9. Es gibt also theoretisch \(2^{32}\) eindeutige IP-Adressen. Diese Adressen werden üblicherweise in der Punktdezimal-Notation geschrieben bei der jedes Byte der Adresse als Dezimalzahl - getrennt durch Punkte - dargestellt wird siehe Abschnitt 3.3.4. Um effizientes Routing zu ermöglichen folgen die Adressen einem Schema ( Netzwerk Host) das in der IP-Adresse sowohl die Kennung des Netzwerks als auch die eindeutige Kennung eines Hosts in diesem Netzwerk kodiert. Die IP-Adressen aller Hosts in einem Netzwerk teilen sich einen gemeinsamen Präfix Netzwerk. Abbildung 4.9: Klassen von IP-Adressen in IPv4. Übungsaufgabe 4.5 Wie viele Hosts kann es in einem Netzwerk der Klasse A hochstens geben? Aufgabe jetzt bearbeiten Abbildung 4.10 zeigt ein Beispiel für die Interface-Adressen in drei miteinander verbundenen einfachen IP-Netzwerken wobei das Netzwerkpräfix (network prefix) der Interfaces jeweils die ersten drei Byte sind. Das letzte Byte ist der Host-Teil.4 Das einfache IP-Netzwerk links in der Abbildung 4.10 hat die Adresse \(220.1.1.0/24\) wobei \(220.1.1.0\) die Basisadresse des Netzwerks ist. Die Notation \(/24\) drückt aus dass die ersten 24 Bit der Adresse die Netzwerkadresse definieren. \(/24\) heißt auch manchmal Netzwerkmaske (network mask subnet mask) die auch in der Form \(255.255.255.0\) dargestellt wird. Jedes Interface das zu einem Netzwerk hinzugefügt werden soll muss diesem Adressierungsschema folgen. Ein neues Interface im Netzwerk \(220.1.1.0/24\) muss also eine IP-Adresse der Form \(220.1.1.x\) mit \(x \in[0 255]\) haben. Abbildung 4.10: Interface-Adressen für ein Netzwerk aus drei einfachen Netzwerken. Die Adressaufteilung in Abbildung 4.9 erwies sich aber als zu unflexibel da in vielen Netzwerken große Mengen von Adressen ungenutzt blieben. Deshalb ist es mit dem CIDR-Standard (Classless Interdomain Routing Standard) der IETF [4] nun möglich die 32 Bit der Adresse beliebig aufzuteilen: Die Notation \(a.b.c.d/x\) meint dass die ersten \(x\) der 32 Bit der Adresse das Netzwerkpräfix beschreiben. Einem Netzwerk mit z.B. weniger als 2000 aber mehr als 256 Hosts wären nach der alten Schema ein Netzwerkpräfix der Klasse B mit \(2^{16}=65536\) Host-Adressen zugeteilt worden nun kann es eine Adresse \(a.b.c.d/21\) bekommen verwendet also nur die ersten 21 Bit als Netzwerkpräfix und hat \(2^{11}=2048\) interne Adressen zur Verfügung. Damit bleiben gegenüber früher etwa 63.000 IP-Adressen frei die anderen Organisationen zugeteilt werden können. Ein Host kann auf mehreren Wegen zu einer IP-Adresse beziehungsweise zum Host-Teil hinter dem ansonsten festliegenden Netzwerkpräfix kommen. Eine Möglichkeit ist die manuelle Konfiguration durch den Systemadministrator der die IP-Adresse üblicherweise in eine Datei schreibt. Eine andere oft genutzte Möglichkeit ist die Nutzung des DHCP-Protokolls (Dynamic Host Configuration Protocol [5]). In einem Netzwerk kann ein Host eine Anfrage an einen DHCP-Server stellen der dann dem Host eine freie IP-Adresse dynamisch zuordnet. An eine Netzwerkadresse also das Netzwerkpräfix zu kommen ist schon erheblich schwieriger. Ein Netzwerkadministrator bekommt eine Netzwerkadresse in der Regel von dem Internet-Provider der Organisation. Jeder Internet-Provider hat üblicherweise einen größeren Adressraum für seine Kunden reserviert. Diesen Adressraum kann der Provider unter seinen Kunden aufteilen. Internet-Provider müssen ihre Adressräume wiederum bei der ICANN (Internet Corporation for Assigned Names and Numbers [6]) bzw. bei den von ihr lizenzierten regionalen Internet Registries beantragen und dafür natürlich auch bezahlen. Übungsaufgabe 4.6 Eine Universität hat weniger als \(2^{5}=32\) aber mehr als \(2^{4}=16\) Fakultäten und jede besitzt mehr als \(2^{10}=1024\) aber weniger als \(2^{11}-2=2046\) Rechner. Das Netzwerk der Universität hat einen Hauptrouter der mit einem ISP oder einem regionalen Netzwerk verbunden ist siehe Abbildung 4.11. Der Universität sind \(2^{16}=65536\) IP-Nummern der Klasse B mit der Adresse \(135.50.0.0/16\) zugeteilt worden. Abbildung 4.11: Das Netzwerk der Universität. Wie können diese IP-Adressen \(135.50.0.0/16\) auf die Fakultäten so verteilt werden dass jede ein einfaches Teilnetzwerk innerhalb des Uni-Netzwerks mit einer sogenannten Teilnetzwerkmaske bildet? Geben Sie für die 8 Fakultäten aus Abbildung 4.11 die erste zugewiesene IP-Adresse sowie die Teilnetzwerkmaske in der Notation \(a.b.c.d/x\) an. Aufgabe jetzt bearbeiten Routing von IP-Datagrammen Jedes IP-Datagramm besitzt ein Feld für die IP-Adresse des Senders und ein Feld für die IP-Adresse des Empfängers. Der Sender trägt seine eigene IP-Adresse in das Sender-Adressfeld und die Empfänger-IP-Adresse in das Empfänger-Adressfeld ein. In das Datenfeld kommt üblicherweise ein TCP- oder UDP-Segment. Das Längenfeld gibt an wie viele Bytes tatsächlich genutzt sind. Die weiteren Felder und ihre Bedeutung können Sie bei Bedarf in der Literatur [1][7] nachlesen. Wir wollen den Transport eines IP-Datagramms durch die Vermittlungsschicht anhand eines Beispiels diskutieren siehe Abbildung 4.12. Abbildung 4.12: Routing-Tabellen in Host 1 und im Router. Wenn Host 1 ein IP-Datagramm an Host 3 schickt der in demselben einfachen IP-Netzwerk liegt dann passiert das folgende: Zuerst sucht das IP-Protokoll in der Routing-Tabelle von Host 1 nach einem Eintrag dessen Netzwerkadresse zu der IP-Adresse von Host 3 passt. Das heißt die Adresse \(220.1.1.3\) von Host 3 wird der Reihe nach mit den Einträgen in der Routing-Tabelle verglichen und zwar nur in den führenden Bits der Netzwerkadresse bis eine Übereinstimmung gefunden wird. In diesem Beispiel passt schon der erste Eintrag \(220.1.1.0/24\). Jetzt weiß Host 1 dass Host 3 zu demselben Netzwerk gehört. Die Distanzen in der Routing-Tabelle geben zusätzlich an wie viele Netzwerke einschließlich des Zielnetzwerks die Pakete vom Sender bis zum Ziel überqueren müssen.5 Man spricht auch von der Anzahl der Hops. Der Eintrag für das Netzwerk \(220.1.1.0/24\) zeigt dass die Distanz zu Host 3 genau 1 ist. Host 1 weiß jedenfalls dass er Host 3 direkt über seine ausgehende Schnittstelle erreichen kann ohne weitere Router zu Hilfe nehmen zu müssen. Host 1 gibt das IP-Datagramm damit an die Sicherungsschicht weiter die es dann direkt zu Host 3 transportiert. Wenn Host 1 ein IP-Datagramm an Host 5 sendet der zu einem anderen Netzwerk gehört dann wird der passende Eintrag \(220.1.3.0/24\) an der dritten Stelle der Routing-Tabelle gefunden. Jetzt sieht Host 1 dass er das IP-Datagramm an die IP-Adresse \(220.1.1.4\) der Schnittstelle des Routers schicken muss. Der Eintrag \(220.1.3.0/24\) zeigt noch dass die Distanz zum Zielknoten 2 Hops beträgt also liegt der Zielknoten in einem anderen Netzwerk das nur über einen dazwischenliegenden Router erreicht werden kann. Die Sicherungsschicht überträgt das in einem Sicherungsschichtrahmen verpackte IP-Datagramm ohne dass die Empfänger-IP-Adresse im IP-Datagramm verändert wird! Im Router angekommen konsultiert der Router seine eigene Routing-Tabelle und verschickt das IP-Datagramm über seine Sicherungsschicht an den Empfänger. Der Eintrag in der Routing-Tabelle im Router hat hier durch die eingetragene Distanz 1 angezeigt dass der Empfängerknoten direkt über das ausgehende Interface mit der angegebenen IP-Adresse \(220.1.3.27\) erreichbar ist. Würde der Empfängerknoten in einem weiter entfernten Netzwerk liegen dann würde in der Routing-Tabelle des Routers ein Eintrag mit einer größeren Distanzzahl existieren und das Interface zum nächsten Router auf dem Pfad spezifizieren. Unser Router würde dann das IP-Datagramm an den nächsten Router weiterreichen usw. Aus dieser Diskussion wird deutlich dass die Routing-Tabellen eine zentrale Rolle im Internet spielen. Es ist die Aufgabe der Routing-Algorithmen möglichst gute Pfade in den Routing-Tabellen abzulegen siehe auch Abschnitt 4.8 und 4.9. Fragmentierung und Reassemblierung in IPv4 Sicherungsprotokolle die die Vermittlungsschicht zum Transport der Datagramme über die physische Leitung benutzen müssen unterscheiden sich unter anderem durch die maximale Paketgröße und damit die maximale Anzahl an Datenbytes MTU (maximum transfer unit). Im Ethernet werden z.B. Pakete mit maximal 1500 Byte an Dateninhalt unterstützt. Dagegen unterstützen viele Verbindungen in Weitverkehrsnetzen Pakete mit höchstens 576 Byte Dateninhalt. Wenn ein IP-Datagramm über einen Pfad mit Verbindungen geleitet wird die unterschiedliche MTUs unterstützen dann tritt ein Problem auf: Was passiert mit einem Paket in der Sicherungsschicht welches ein IP-Datagramm der Vermittlungsschicht enthält wenn es plötzlich über eine Verbindung übertragen werden soll die eine kleinere MTU hat? Die Lösung dieses Problems liegt in der Fragmentierung und Reassemblierung von IP-Datagrammen: wann immer ein IP-Datagramm zu groß ist wird es auf dem Router in mehrere kleinere IP-Datagramme zerlegt. Diese Fragmente können dann über die Verbindung mit der kleineren MTU übertragen werden. Bevor das ursprüngliche IP-Datagramm des Senderknotens an den Empfängerknoten übergeben werden kann muss es aus den Fragment-IP-Datagrammen wieder zusammengesetzt werden. Dieses Zusammensetzen des ursprünglichen IP-Datagramms aus den Fragmenten passiert in der Vermittlungsschicht in den Endsystemen des Internets - nicht in den Routern. Damit dies funktioniert muss ein Endsystem anhand der empfangenen IP-Datagramme erkennen ob die empfangenen IP-Datagramme Fragmente eines größeren IP-Datagramms waren. Dazu dienen in IPv4 bestimmte Felder im Header der Datagramme: Identification (ID) Flag und Fragmentation Offset. Bei Erzeugung eines IP-Datagramms versieht der Sender dieses mit einer neuen ID durch Inkrementieren der ID des letzten gesendeten Datagramms. Wenn das IP-Protokoll eines Router ein IP-Datagramm fragmentiert dann behält jedes Fragment dieselbe ID des Datagramms. Alle Fragmente haben das Flag-Feld auf 1 gesetzt bis auf das letzte Fragment. Hieran erkennt das Endsystem dass das letzte Fragment eines größeren IP-Datagramme empfangen wurde. Anhand des Fragmentation Offset kann das Endsystem die Fragmente in die richtige Reihenfolge bringen und Lücken erkennen. Das Feld Fragmentation Offset gibt an an welche Stelle im aktuellen Datagramm das Fragment gehört. Falls ein oder mehrere Fragmente verloren gegangen sind dann wird das gesamte ursprüngliche IP-Datagramm verworfen. Im Falle von TCP muss das Transportprotokoll dann eine Wiederholung veranlassen. Die Fragmentierung verlangsamt die Verarbeitungsprozesse im Router so wie die Reassemblierung die Verarbeitung auf dem Endsystem verlangsamt. Deshalb möchte man Fragmentierung vermeiden. Sofern man die Größe eines IP-Datagramms auf den kleinsten gemeinsamen Nenner verringert kann man die Fragmentierung ganz verhindern. Im Internet sollen die eingesetzten Sicherungsschichtprotokolle MTUs von mindestens 576 Byte unterstützen. Deshalb kann eine Beschränkung des Dateninhalts auf 536 Byte pro IP-Datagramm sinnvoll sein. Zusammen mit dem 20 Byte IP-Header und 20 Byte für den Header des TCP-Segments liegt man dann unter der magischen Grenze von 576 Byte. Pfadbestimmung im Internet In diesem Abschnitt betrachten wir die Routing-Protokolle des Internets. Das Internet besteht aus einem Verbund von Netzwerken die von regionalen Organisationen betrieben werden. In Abschnitt 4.11 wurde definiert dass eine Menge von Routern die von derselben Organisation betrieben werden und die dasselbe Routing-Protokoll benutzen als autonomes System (AS) bezeichnet wird. Jedes autonome System besteht selbst wiederum aus einer Menge von einfachen IP-Netzwerken. Wir unterscheiden im Internet zwei Fälle des Routing: In einem autonomen System: Intra-AS-Routing Zwischen autonomen Systemen: Inter-AS-Routing Intra-AS-Routing Ein Routing-Protokoll für Intra-AS-Routing konfiguriert die Routing-Tabellen in allen Routern eines autonomen Systems. In der Praxis kommen im Internet vor allem drei Protokolle zum Einsatz: RIP (Routing Information Protocol [8][9]) OSPF (Open Shortest Path First [10]) und EIGRP (Enhanced Interior Gateway Routing Protocol ein proprietäres Protokoll von Cisco). Wir behandeln hier nur RIP. RIP ist eines der frühesten Intra-AS-Routing-Protokolle das heute noch viel genutzt wird. Es nutzt einen Distanzvektor-Algorithmus ähnlich zu dem in Abschnitt 4.9 beschriebenen. Die RIP Version 1 [8] verwendet als Kosten die Anzahl der Hops die bei der Übertragung von Datenpaketen auf dem Weg zum Ziel durchlaufen werden müssen siehe auch Abschnitt 4.16. Außerdem sind die maximalen Kosten eines Pfades auf den Wert 15 beschränkt. Deshalb können autonome Systeme mit RIP nur einen maximalen Durchmesser von 15 Routern haben. Nachbarknoten tauschen in RIP etwa alle 30 Sekunden Routing-Informationen mit sogenannten RIP Response Messages aus indem sie sich ihre Routing-Tabelleneinträge für höchstens 25 Zielnetzwerke im autonomen System verschicken. Betrachten wir nun den Ausschnitt eines autonomen Systems in Abbildung 4.13. Tabellen 4.2 und 4.4 zeigen die Entwicklung der Routing-Tabelle von Router \(B\) aufgrund des Empfangs einer der RIP Response Message von Router \(A\) die in Tabelle 4.3 dargestellt ist. Abbildung 4.13: Ausschnitt aus einem autonomen System. Gestrichelte Linien zeigen Verbindungen zu weiteren Routern an Kantenbezeichnungen \(w\) \(x\) \(y\) \(z\) bezeichnen Netzwerke. Tabelle 4.2: Ausschnitt aus der Routing-Tabelle von Router \(B\) vor dem Empfang der RIP Response Message von Router \(A\). Zielnetzwerk Nächster Router Distanz zum Ziel \(w\) \(A\) 2 \(y\) \(D\) 2 \(z\) \(D\) 7 \(x\) - 1 ... ... ... Die Routing- Tabelle 4.2 ist folgendermaßen zu lesen: wenn Router \(B\) Pakete an das Netzwerk \(w\) verschicken will (siehe erste Zeile) dann versendet er diese über den Router \(A\). Nachdem Router \(A\) das Update (exakt seine Routing-Tabelle siehe Tabelle 4.3) an \(B\) verschickt hat erfährt \(B\) dass das Netzwerk \(z\) nur 5 Schritte über Router \(A\) entfernt ist. Bisher war \(z\) aber über Router \(D\) mit der Distanz 7 erreichbar. Deshalb modifiziert \(B\) seinen Tabelleneintrag für \(z\) entsprechend (5 Schritte von \(A\) nach \(z\) plus 1 Schritt über die Verbindung \(x\) nach \(A\)) und sendet Pakete nach \(z\) nun über \(A\). Wie kommt es in der Praxis zu einem Wechsel der Distanz zu einem Netzwerk? Entweder war der Algorithmus noch in der Konvergenzphase oder es wurden neue Verbindungen oder Router zum autonomen System hinzugefügt. Tabelle 4.3: Ausschnitt aus der Routing-Tabelle (bzw. der RIP Response Message) von Router \(A\). Zielnetzwerk Nächster Router Distanz zum Ziel \(z\) \(C\) 5 \(w\) - 1 \(x\) - 1 ... ... ... Tabelle 4.4: Ausschnitt aus der Routing-Tabelle von Router \(B\) nach dem Empfang der RIP Response Message von Router \(A\). Zielnetzwerk Nächster Router Distanz zum Ziel \(w\) \(A\) 2 \(y\) \(D\) 2 \(z\) \(A\) 6 \(x\) - 1 ... ... ... Wenn RIP auf einem Router mehr als 180 Sekunden kein Update von einem Nachbarn erhält betrachtet es den Nachbarn als unerreichbar. RIP modifiziert dann die lokale Routing-Tabelle und schickt Updates zu seinen Nachbarn. Ebenso können Router über RIP von Nachbarn deren Kosten zu einem Zielknoten abfragen. RIP benutzt zur Kommunikation einzelne UDP-Datagramme über den Standard-Port 520. Es benutzt also ein Transportschichtprotokoll UDP um eine Funktion der Vermittlungsschicht zu realisieren siehe auch Abschnitt 3.4.6. Der Grund hierfür liegt in der typischen Implementierung von RIP auf UNIX-Systemen als Anwendungsprozess (sogenannter routed daemon). Der routed Anwendungsprozess hat Zugang zu den Routing-Tabellen des Hosts und kommuniziert über einen Socket mittels des Standard-Transportprotokolls UDP. RIP ist daher ein Protokoll auf der Anwendungsschicht. Inter-AS-Routing BGP4 (Border Gateway Protocol Version 4 [11][12]) ist der De-Facto-Standard für Inter-AS-Routing im Internet. Autonome Systeme werden im Internet als administrative Domänen (Domains) bezeichnet. BGP basiert auf der Idee des Distanzvektor-Algorithmus siehe Abschnitt 4.9. Allerdings ist BGP ein Pfadvektor-Protokoll da BGP in einem Router keine Kosteninformationen (z.B. die Anzahl der Hops bis zum Zielknoten) ablegt sondern Pfadinformationen nämlich wie die Reihenfolge der autonomen Systeme auf einem Pfad vom Quell-AS zum Ziel-AS ist. BGP legt nicht fest wie aus mehreren möglichen Pfaden zum Ziel-AS ein Pfad ausgewählt werden soll. Dies ist eine Richtlinienentscheidung (Policy Decision) des Domänenadministrators. Jede Domäne kann ihre eigenen Richtlinienentscheidungen treffen ohne dass dies den anderen Domänen mitgeteilt würde. BGP betrachtet das Internet als Graph aus autonomen Systemen die durch Nummern bezeichnet werden. Zu einem bestimmten Zeitpunkt kann ein autonomes System \(X\) einen Pfad über mehrere AS zum autonomen System \(Y\) kennen - oder auch nicht. Angenommen \(X\) hat den Pfad \(X-Z_1-Z_2-Z_3-Y\) zum autonomen System \(Y\) in der BGP-Routing-Tabelle. BGP kennt daher nicht nur den ersten Router auf dem Pfad sondern alle Stationen. Wenn \(X\) ein Update zu seinen Nachbarn schickt dann wird tatsächlich die gesamte Pfadinformation aus der Routing-Tabelle verschickt. Wenn ein Nachbar \(W\) von \(X\) nun dieses Update empfängt dann kann er neue Einträge \(W-X-Z_1-Z_2-Z_3-Y\) zu seiner Tabelle hinzufügen - oder er kann diesen Update ignorieren z.B. wenn er bereits einen besseren Pfad zu \(Y\) besitzt oder der neue Eintrag eine Routing-Schleife erzeugen würde. BGP-Informationen werden durch das Netzwerk propagiert indem die unmittelbaren Nachbarn im Graphen BGP-Nachrichten austauschen. Das BGP-Protokoll definiert vier Nachrichtentypen: Open-Nachricht: BGP benutzt TCP als Transportprotokoll auf dem Port 179. Beim Verbindungsaufbau sendet BGP zuerst eine Open-Nachricht um sich beim Nachbarn zu authentifizieren6 und Informationen abzugleichen z.B. Timer. Akzeptiert der Nachbar die Verbindung sendet er eine KeepAlive-Nachricht zurück. Update-Nachricht: Mit Update-Nachrichten werden Pfade an Nachbarn gemeldet oder zurückgezogen. KeepAlive-Nachricht: Diese Nachricht dient zur Akzeptierung eines Verbindungsaufbaus durch eine Open-Nachricht oder als Lebenszeichen wenn der Sender keine neuen Informationen zu übertragen hat. Notification-Nachricht: Diese Nachricht dient der Signalisierung eines Fehlers in der vorherigen Nachricht oder zur Anzeige eines beabsichtigten Verbindungsabbaus. In der Praxis hat ein autonomes System oft mehrere Gateway-Router um Verbindungen zu anderen autonomen Systemen zu halten. Zwischen diesen Gateway-Routern innerhalb desselben autonomen Systems kann BGP verwendet werden um Updates auszutauschen. Übungsaufgabe 4.7 BGP benutzt den Distanzvektor-Algorithmus um die Routing-Aufgabe zwischen autonomen Systemen zu lösen. Kann das Count-to-Infinity-Problem beim BGP entstehen? Aufgabe jetzt bearbeiten Fehlerbehandlung und Abfragen im Internet Im Internet benutzen Hosts Router und Gateways das ICMP-Protokoll (Internet Control Message Protocol [3]) für den Austausch von Vermittlungsschicht-Informationen wie z.B. Fehlermeldungen oder Informationen über benutzte Netzwerkpfade. ICMP benutzt zur Kommunikation IP-Datagramme d.h. ICMP-Nachrichten werden von ICMP direkt in IP-Datagramme verpackt. Konzeptionell ist ICMP daher ein Protokoll der über IP liegenden Schicht.7 ICMP-Nachrichten haben einen Typ und ein Feld namens Code um die Bedeutung der Nachricht zu spezifizieren. Zusätzlich enthält eine ICMP-Nachricht die ersten 8 Byte des IP-Datagramms welches zur Erzeugung der ICMP-Nachricht führte siehe Tabelle 4.5 die einige ICMP-Nachrichten zeigt. Dies hilft dem Sender den vorliegenden Fehler zu erkennen. Tabelle 4.5: Einige ICMP-Nachrichtentypen. ICMP-Typ Code Bedeutung 0 0 Echo-Antwort (auf eine Ping-Nachricht) 3 0 Zielnetzwerk nicht erreichbar 3 1 Zielhost nicht erreichbar 3 2 Ziel-Protokoll nicht erreichbar 3 3 Ziel-Port nicht erreichbar 3 6 Zielnetzwerk unbekannt 3 7 Zielhost unbekannt 4 0 Quelle reduzieren (zur Überlastkontrolle) 8 0 Echo-Anforderung (Ping-Nachricht) 9 0 Router-Bekanntmachung 10 0 Router-Entdeckung 11 0 Time-To-Live (TTL) abgelaufen 12 0 Fehlerhafter IP-Header ICMP wird oft für die Kommunikation von Fehlermeldungen benutzt: Falls ein Router feststellt dass er keinen Weg zum Zielhost kennt verwirft der Router das IP-Datagramm und erzeugt statt dessen eine ICMP-Fehlernachricht (z.B. Typ 3 mit Code 7 für Zielhost unbekannt). Wenn der Senderhost die ICMP-Fehlernachricht empfängt reicht er diese als Fehlercode an das Transportprotokoll des Senders (z.B. TCP) zurück das diesen Fehler dann an die Anwendung weiterreicht. Ein Beispiel für die Verwendung von ICMP ist das Programm ping. Es sendet eine ICMP-Nachricht vom Typ 8 mit Code 0 an den Zielhost. Der Zielhost empfängt die Echo-Anforderung und sendet eine Echo-Antwort (ICMP-Nachricht Typ 0 mit Code 0) zurück. Ein anderes Beispiel für den Einsatz von ICMP ist das Programm traceroute. Es ermittelt den Pfad den IP-Datagramme zu einem angegebenen Zielhost nehmen. Dazu sendet das Programm eine Reihe gewöhnlicher IP-Datagramme an den Zielhost. Dabei erhöht es bei jedem neuen IP-Datagramm das Feld TTL (Time-To-Live) um eins. Router die das IP-Datagramm übertragen verringern das TTL-Feld bei jeder Übertragung um 1. Wenn das TTL-Feld auf Null gesetzt wird wird das IP-Datagramm verworfen.8 In diesem Fall aber sendet der Router eine Warnung (ICMP-Nachricht mit Typ 11 und Code 0) an den Senderhost die auch die IP-Adresse des Routers und den Zeitstempel der Paketerzeugung enthält. Mit diesen Informationen kann ICMP dann den Pfad und die Umlaufzeit der Pakete auf dem Pfad berechnen und anzeigen.  
2; mod_longpage;215; 4; 4; 4.4 Neuere Entwicklungen für die Vermittlungsschicht Seit den frühen neunziger Jahren entwickelt die Internet Engineering Task Force (IETF) das IPv6-Protokoll [13][14][15] als Reaktion auf verschiedene Probleme mit IPv4: Aufgrund des rapiden Wachstums des Internets war absehbar dass der Adressraum von IPv4 in relativ naher Zukunft erschöpft sein würde. Deshalb werden in IPv6 128 Bit lange Adressen (statt 32 Bit in IPv4) verwendet. Zusätzlich zu Unicast- und Multicast-Adressen werden sogenannte Anycast-Adressen eingeführt mit denen ein Datagramm an irgendeinen Host aus einer Anycast-Gruppe weitergeleitet werden kann. Dies ist immer dann nützlich wenn mehrere Knoten im Netzwerk denselben Dienst anbieten und es der Anwendung egal ist welcher Knoten den Dienst erbringt. Dies gilt z.B. für eine Mirror-Site eines Web-Servers oder die Beantwortung einer DNS-Anfrage durch einen DNS-Server. In diesem Fall können wir die Auswahl des Knotens aus der Anycast-Gruppe getrost der Vermittlungsschicht überlassen die z.B. den am billigsten erreichbaren Knoten wählen könnte. Der 40 Byte lange IPv6 Header wurde so optimiert dass er effizienter verarbeitet werden kann. So wurde zum Beispiel das Feld Prüfsumme entfernt da die Transportprotokolle und viele Sicherungsprotokolle selbst Fehlererkennung und Fehlerkorrektur durchführen. Dies führt dazu dass in IPv6 das Berechnen und Testen der Prüfsumme in jedem Router entfallen kann. Weiterhin wurde auch das Fragmentation/Reassembly-Feld von IPv4 aufgegeben: ein Router der ein zu großes Datagramm empfängt verwirft dies einfach und sendet eine ICMP-Fehlermeldung an den Sender zurück. Dieser muss dann die Nachricht in kleinere Datagramme verpackt noch einmal schicken. Dies spart in den Routern den Aufwand für die Fragmentierung von Datagrammen ein. Der Übergang von IPv4 zu IPv6 kann auf verschiedene Weise erfolgen: An einem Stichtag könnte der Übergang aller Hosts und Router verpflichtend sein. Da das Internet aus Millionen von Geräten besteht erscheint dieser Ansatz nicht sehr realistisch. Dual-Stack Ansatz: Jeder IPv6 Knoten könnte parallel eine IPv4-Implementierung und eine IPv6-Implementierung enthalten. Solche Knoten müssten mit Hilfe von DNS feststellen welche IP-Version ein Empfängerknoten unterstützt und dann Pakete im entsprechenden Format senden. DNS kann einfach eine IPv4-Adresse für den Empfänger zurückgeben wenn entweder der Sender oder der Empfänger nur IPv4 unterstützt. Dieser Ansatz führt allerdings dazu dass Datagramme zwischen IPv6-Knoten wenn sie über einen Pfad mit mindestens einem IPv4-Knoten transportiert werden die Vorteile von IPv6 nicht ausnutzen können. Tunneling Ansatz: Hierbei können die IPv6-Knoten zwischen sich Tunnel aufbauen indem sie den IPv6-Verkehr bei Übertragung über IPv4-Knoten insgesamt in IPv4 Datagramme einpacken. Hierbei kann der IPv6-Empfänger dann das IPv6-Datagramm wieder auspacken und normal weiterverarbeiten. Es bleibt zum Schluss anzumerken dass der Austausch eines Protokolls der Vermittlungsschicht sehr viel schwieriger ist als ein neues Protokoll auf der Anwendungsschicht wie z.B. HTTP Chat oder Audio/Video Streaming einzuführen. Dies liegt daran dass die Vermittlungsschicht das Internet zusammenhält.  
2; mod_longpage;215; 4; 5; 4.5 Aufgaben der Sicherungsschicht Wie wir in den vorhergehenden Abschnitten gesehen haben bietet die Vermittlungsschicht einen Kommunikationsdienst der Datagramme von einem Host über eine Route zu einem anderen überträgt. Die Route besteht aus einer Reihe von direkten Verbindungsleitungen zwischen Netzknoten das können Hosts oder Router sein. In der Sicherungsschicht (data link layer) geht es nun darum wie Datagramme der Vermittlungsschicht von einem Knoten zu einem anderen direkt benachbarten Knoten über eine einzelne Verbindungsleitung übertragen werden. Aus Platzgründen können wir in diesem Kurs nur einen kurzen Überblick über die Sicherungsschicht geben. Im Kurs Kommunikations- und Rechnernetze wird die Sicherungsschicht detailliert behandelt. Die heute übliche Form der Vernetzung von mehr als zwei Rechnern über eine physische Leitung sind Busstrukturen und das gebräuchlichste Protokoll ist das Ethernet-Protokoll. Der Bus ist ein allen Knoten gemeinsam zugängliches Übertragungsmedium das einen passiven Nachrichtentransport unterstützt. Der Sender sendet die Bits in Form von Signalen und während ein Bit an einem Knoten vorbeifließt strömt ein Teil der Signalenergie in den Adapter (Busankopplung). Terminatoren an beiden Busenden schließen den Bus ab und absorbieren die Restenergie der Signale siehe Abbildung 4.14. So hören alle angeschlossenen Knoten die von einem beliebigen Knoten gesendeten Signale mit. Busnetze heißen daher auch Broadcast-Netze oder Carrier-Sense-Netze. Broadcast-Netze haben einen einzigen Übertragungskanal der von allen am Netz angeschlossenen Hosts gemeinsam genutzt wird. Pakete werden von einem Host versendet und von allen anderen empfangen. Zugangsprotokolle für Broadcast-Netze regeln den Zugriff von Knoten auf den Bus sowie das Verhalten im Falle von Kollisionen wenn verschiedene Knoten gleichzeitig versuchen den Bus zu nutzen. Die physische Ausprägung eines solchen Netzes kann verschiedene Formen annehmen indem Busnetze durch sogenannte Repeater miteinander verbunden werden siehe Abbildung 4.15. Ein Repeater ist ein reiner Signalverstärker der ankommende Signale aus einem angeschlossenen Segment unverändert an alle anderen angeschlossenen Segmente weiterleitet. Das klassische Ethernet z.B. unterstützte bis zu vier Repeater um die maximale Kabellänge von 500 m auf 2.500 m zu erweitern. Abbildung 4.14: Einfaches Busnetz. Abbildung 4.15: Durch Repeater \((R )\) erweiterte Busstruktur eines Busnetzes. In den 80ern und Anfang der 90er Jahre wurden verstärkt Ringnetze genutzt ein zweiter Typ von Broadcast-Netzen. Die Datenübertragung erfolgt hier über gerichtete Leitungen die jeweils zwei benachbarte Knoten verbinden siehe Abbildung 4.16 (a). Jeder Knoten besitzt eine Ringankopplung die aktiv am Datentransport beteiligt ist. Sie entscheidet ob eine Nachricht unverändert weiterzugeben (Broadcast) verändert weiterzuleiten oder vom Ring zu nehmen ist. Protokolle für Ringnetze sind Token-Ring (IEEE 802.5) und FDDI. Im Unterschied zu den Busankoppelungen in Busnetzen sind die Ringankoppelungen aktiv am Transport beteiligt. Dadurch fallen Verzögerungszeiten pro Knoten an. Ein weiteres Merkmal von Ringnetzen ist dass bei Ausfall eines Knotens keine geregelte Kommunikation mehr möglich ist. Dieses Problem tritt in einem sternförmigen Netz nicht mehr auf wo jeder Knoten direkt mit einem zentralen Gerät - dem Hub - verbunden wird siehe Abbildung 4.16 (b). So können ausgefallene Knoten die Kommunikation nicht mehr behindern. Auch das Einfügen neuer Knoten in ein Netz wird vereinfacht. Abbildung 4.16: (a) Ringnetz (b) sternförmiges Netz mit einem Hub in der Mitte. Protokolle der Sicherungsschicht werden also in der Regel für eine bestimmte Kommunikationsleitung implementiert. Sie sind in Adaptern als Netzschnittstellenkarte (Network Interface Card NIC) realisiert die heute meistens als Steckkarte (oder PCMCIA-Karte) angeboten wird. Ein Adapter verfügt über zwei Komponenten das Bus-Interface zu dem Rechner auch Host-Bus-Schnittstelle genannt und das Leitungs-Interface zu der Netzwerkleitung auch Leitungsschnittstelle genannt siehe Abbildung 4.17.9 Er verbindet also Host oder Router mit der physischen Netzwerkleitung. Abbildung 4.17: Aufbau eines Adapters. Adapter werden auch als halbautonome Einheiten bezeichnet: Alle Entscheidungen bezüglich des Sicherungsschichtprotokolls sind in der Leitungsschnittstelle des Adapters oft in Hardware implementiert.10 Der Adapter erhält zu versendende Datagramme des Knotens über den E/A-Bus des Knotens und sein Bus-Interface verpackt diese in seinem Leitungs-Interface in Rahmen der Sicherungsschicht und schickt den Rahmen mit Hilfe der unterliegenden Bitübertragungsschicht auf die Netzwerkleitung. Erhält der Adapter Daten von der Netzwerkleitung spricht er den Knoten nur über die Bus-Schnittstelle an wenn Datagramme an die Schicht 3 weitergegeben werden sollen. Der Adapter gehört aber zum gleichen physischen Gerät wie der Knoten und teilt sich mit ihm Stromversorgung und Busse und steht unter der Kontrolle des Knotens den man deshalb auch Elternknoten des Adapters nennt. Die Dienste eines Sicherungsschichtprotokolls umfassen folgende Aspekte: Zum Weiterleiten der Datagramme werden die Datagramme in Rahmen (frames) gekapselt. Diese Aufgabe heißt auch Framing. Die eigentliche Weiterleitung soll nach Möglichkeit zuverlässig sein. Hier kommen Techniken die wir schon von der Transportschicht kennen wie das Bestätigen und die Neuübertragung zum Einsatz. Der Dienst der Flusskontrolle soll wie auf der Transportschicht siehe Abschnitt 3.4.5.6 den sendenden Knoten daran hindern den empfangenden Knoten zu überschwemmen. Zudem kann ein Sicherungsprotokoll eine Vollduplex- oder Halbduplex-Dienst anbieten: im Halbduplex-Betrieb kann ein Knoten nicht gleichzeitig senden und empfangen. Bei Vollduplex können beide Knoten an den Enden einer Verbindungsleitung gleichzeitig Pakete übertragen. Auf diese Dienstaspekte werden wir nicht im Detail eingehen. Wir stellen drei typische Dienstaspekte von Protokollen der Sicherungsschicht vor. Die Unterschiede der von Schicht-2-Protokollen angebotenen Dienste resultieren im wesentlichen aus den Unterschieden der Leitungstypen über die die Protokolle operieren: Von ihren physikalischen Eigenschaften her können störungsanfällige Leitungen häufig übertragene Bits verfälschen. Also können Fehlererkennung oder sogar Fehlerkorrektur das Weiterleiten verfälschter Rahmen und damit das unnötige Belasten von Leitungen vermeiden. Für die Sicherungsschicht wurden deshalb ausgefeilte Techniken zur Fehlererkennung entwickelt die wir hier nicht weiter behandeln können. Liegt beispielsweise genau eine einzige Verbindung zwischen einem Sender und einem Empfänger vor ist der Leitungszugang trivial. Teilen sich jedoch mehrere Knoten eine Broadcast-Leitung tritt das Mehrfachzugriffsproblem auf und Kanalzugriffsprotokolle müssen den Zugang zu der Leitung regeln. Die gemeinsame Nutzung eines Broadcast-Kanals durch mehrere Knoten erfordert zudem die Einführung von Adressen auf der Sicherungsschicht. Diese müssen mit den Adressen der Vermittlungsschicht in Deckung gebracht werden. Das Address Resolution Protocol (ARP RFC 826 [16]) bildet im Internet die Adressen der Vermittlungsschicht auf Adressen der Sicherungsschicht ab.  
2; mod_longpage;215; 4; 6; 4.6 Aufgaben der Bitübertragungsschicht Die Bitübertragungsschicht stellt physische Übertragungskanäle für die Übertragung beliebiger Bitfolgen zur Verfügung. Wir werden auf die Bitübertragungsschicht nicht im Detail eingehen und nur einige Aspekte beleuchten. Die Qualität des Übertragungskanals den die Bitübertragungsschicht auf einem physischen Medium realisiert hat immer Konsequenzen für die Entwicklung von Protokollen. Wir haben in den vorhergehenden Abschnitten entsprechende Konsequenzen gesehen: Die Fehlerrate des physischen Mediums begründet das ganze Spektrum der Fehlerbehandlung in Protokollen. Kupferkabel ist zum Beispiel rauschbehaftet und störungsanfällig während Glasfaser relativ sicher ist. Auf Kupferkabel verwendet man elektrische Signale zur Übertragung auf Glasfaser optische Signale. Protokolle werden für diese unterschiedlichen Medien ausgelegt. Die maximal mögliche Übertragungsgeschwindigkeit des Mediums (gemessen in bit/s) bestimmt unter anderem die Leistungsfähigkeit der Datenübertragung. Die Übertragungsverzögerung führt dazu dass Kollisionen entstehen können obwohl die Sender die Leitung vor dem Senden abhören. Auch auf die Abschwächung von Signalen in Abhängigkeit des konkreten Mediums wurde bereits in den vorhergehenden Abschnitten eingegangen. Hierzu haben wir als Geräte der Bitübertragungsschicht Repeater kennen gelernt. Zur Bitübertragung muss die Bitübertragungsschicht die zu übertragende Information auf dem physischen Medium darstellen können und beim Empfang wieder erkennen und herauslesen können. Die Information muss auf Signale die mit dem physischen Medium übertragen werden können abgebildet werden. Prinzipiell hat man bei der Informationsübertragung auf physischen Medien folgende Ausgangssituation zu betrachten: Die Daten liegen entweder digital (Texte oder Bitmaps) oder analog (Audio oder Video) vor. Zur Datenübertragung kann man ebenfalls digitale oder analoge Signale verwenden. Möchte man digitale und analoge Daten über einen Übertragungskanal übertragen so muss man entweder digitale Daten auf analoge Signale abbilden oder analoge Daten auf digitale Signale. Das Trägerstrom-Übertragungsverfahren basiert darauf digitale Informationen auf analogen Signalen darzustellen. Hier werden drei Modellierungsarten unterschieden: Amplituden- Frequenz- und Phasenmodulationsverfahren siehe Abbildung 4.18. Modems (Modulator/Demodulator) wandeln ein digitales Signal in ein analoges Signal um und gewinnen aus dem analogen Signal die digitale Information. So wurden die Telefonnetze die ersten Datenübertragungskanäle für jedermann. Abbildung 4.18: Modellierung von Nullen und Einsen auf analogen Signalen. Während der Übertragung werden die Signale mehr oder weniger gedämpft oder gestört. Digitale Signale erlauben nicht nur die Verstärkung von Signalen um Dämpfungen auszugleichen sondern auch die Regeneration von Signalen d.h. das Eliminieren von Störungen während der Übertragung. Dies erklärt den Erfolg der digitalen Übertragungstechnik. Analoge Daten werden dazu digital codiert. Dies ist auch die Grundlage der heutigen CD- und DVD-Technik für Audio- und Video-Daten. Ein bekanntes Verfahren ist das PCM-Verfahren (Pulse Code Modulation) siehe Abbildung 4.19. Datenübertragungseinrichtungen die ein analoges Signal in ein digitales umsetzen beziehungsweise aus einem digitalen Signal ein analoges Signal wieder gewinnen heißen Codec (Codierer/Decodierer). Abbildung 4.19: PCM-Verfahren: Die reellen Werte des analogen Signals werden gerundet und diese gerundeten Werte digital als Zahlen übertragen. Um die Signale zu erkennen müssen sich Sender und Empfänger synchronisieren. Sender und Empfänger legen dazu die Schrittdauer und die Länge der Zeichen vorher fest. Beim asynchronen Start-/Stopbetrieb werden Beginn und Ende einer Teilübertragung dem Empfänger durch besondere Signalfolgen mitgeteilt. Im Falle der synchronen Datenübertragung wird ein Takt entweder zentral vom Netz geliefert oder aber der empfangene Rechner muss die Möglichkeit haben sich von Zeit zu Zeit anhand des empfangenen Signals mit dem sendenden Rechner zu synchronisieren. Dazu wird der kontinuierliche Datenstrom in Blöcke fester Länge eingeteilt. Jeder Block beginnt mit einem Synchronisationszeichen das ausschließlich zur Synchronisation beider Rechner dient. Ethernet führt ein solches Zeichen in Form der Präambel ein. Wegen der fehlenden Start/Stop-Schritte ist die Geschwindigkeit mit der ein Zeichen im Mittel übertragen werden kann bei synchroner Betriebsweise erheblich höher als bei asynchroner Betriebsweise.  
2; mod_longpage;215; 4; 7; 4.7 Vertiefungen In den Kurseinheiten 3 und 4 haben wir einen Überblick über den Aufbau und die Grundprinzipien von Rechnernetzen gewonnen. Kurseinheit 3 begann mit einem Überblick über Rechnernetze und das Internet-Schichtenmodell. Es behandelte dann die obersten zwei Schichten die Anwendungsschicht und die Transportschicht. Kurseinheit 4 behandelte die tieferen Schichten des Schichtenmodells und Aspekte des Netzwerkkerns. Die Prinzipien und Konzepte der Vermittlungsschicht bildeten den weitaus größten Teil dieser Kurseinheit und haben ihr den Titel Vermittlung in Rechnernetzen gegeben. Um die Behandlung der Schichten des Protokollstapels abzuschließen haben wir am Ende der Kurseinheit kurz die Sicherungsschicht und Bitübertragungsschicht besprochen. Natürlich kann das Thema Rechnernetze nicht in zwei Kurseinheiten erschöpfend behandelt werden. Es ist unser Ziel Ihnen in diesen beiden Kurseinheiten ein grundlegendes und praktisch orientiertes Verständnis des Aufbaus und der Funktionsweise von Rechnernetzen und der damit zusammenhängenden Probleme zu vermitteln. Außerdem wollen wir die Benutzung von Rechnernetzen zur Realisierung verteilter Anwendungen an einigen Beispielen illustrieren. Insbesondere haben wir in diesem Kurs eine ganze Reihe von Aspekten von Rechnernetzen nicht behandelt. Hierzu gehören z.B. die Multimedia-Vernetzung Sicherheit in Computernetzwerken und Netzwerkmanagement. Auf der Basis der in diesem Kurs erworbenen Kenntnisse sollten Sie in der Lage sein sich mit Hilfe der weiterführenden Literatur (z.B. [17][18]) in diese Themen einzuarbeiten. Wenn Sie dieses Thema im weiteren Verlauf Ihres Studiums vertiefen wollen dann können Sie eine Reihe weiterführender Kurse belegen. Der Kurs Betriebssysteme betrachtet den Aufbau und die Funktionsweise moderner Betriebssysteme. Im Kurs Verteilte Systeme wird genauer dargestellt wie verteilte Systeme auf der Basis moderner Betriebssysteme und Rechnernetze entworfen und realisiert werden können. Der Kurs Kommunikations- und Rechnernetze betrachtet mit mehr Detail wie Netzwerke entworfen werden und wie sie funktionieren. Schließlich betrachtet der Kurs Sicherheit im Internet mit welchen Mechanismen im Internet Sicherheit gegen Spione und Einbrecher gewährleistet werden kann.  
2; mod_assign; 122; 40; 0; Einsendeaufgabe 1.1 Der Hauptspeicher eines Rechners umfasst 230  Worte � 32 Bit.  a) Wie groß ist der Speicher in GByte?b) Wie breit muss der Adressbus des Rechners (mindestens) sein, wenn pro Zugriff je ein 32-Bit Wort geladen werden soll? 
2; mod_assign; 123; 41; 0; Einsendeaufgabe 1.2 Um den Speicherzugriff zu beschleunigen, setzen wir einen Cachespeicher ein und benutzen den  Cache-Algorithmus. Angenommen, ein Schreib-/Lesezugriff auf den Cache benötigt 2 Nanosekunden (1 ns = 10−9  s), und ein Schreib-/Lesezugriff auf den Hauptspeicher incl. der Suche und der Ablage im Cache braucht 10 ns. Welche Trefferquote ist nötig, um die durchschnittliche Zugriffszeit auf 4 ns zu reduzieren? 
2; mod_assign; 124; 42; 0; Einsendeaufgabe 1.3 Erläutern Sie mit eigenen Worten die Strategien FCFS, SSTF und SCAN zur Abarbeitung von Aufträgen auf der Festplatte.Welche Strategie hat das Problem der  Starvation  und warum? Wie kann man diese Strategie modifizieren, damit das Problem vermieden wird?Wir nehmen an, dass eine Festplatte 200 Spuren hat, die von 0 bis 199 nummeriert sind. Es sind Aufträge für Sektoren in den Spuren98,  183,  37,  122,  14,  124,  65,  67,  199in dieser Reihenfolge eingegangen. Welche Distanz muss der Kopf bei Anwendung der drei Strategien FCFS, SSTF und SCAN jeweils insgesamt zurücklegen, bis alle Aufträge erledigt sind?Hinweis: Der Kopf steht anfangs auf Spur 53 und bewegt sich im Falle der SCAN- Strategie gerade nach absteigender Spurnummern. 
2; mod_assign; 125; 43; 0; Einsendeaufgabe 1.4 Geben Sie jeweils zwei Beispiele für das Auslösen von Hardware- und Software-Unterbrechungen an. Welches Ziel verfolgt die einzelne Unterbrechung?Was ist der Hauptunterschied zwischen einer Hardware- und einer Software-Unterbrechung? 
2; mod_assign; 126; 44; 0; Einsendeaufgabe 1.5 Ein modernes Betriebssystem muss vor bösartigen oder fehlerhaften Anwendungsprogrammen geschützt werden. Um dieses Ziel zu erreichen, ist dafür eine Unterstützung durch Hardware unverzichtbar. Geben Sie drei Hardware-Unterstützungen an und beschreiben Sie kurz, wie Betriebssysteme dadurch geschützt werden. 
2; mod_assign; 127; 45; 0; Einsendeaufgabe 1.6 Was bedeutet Time-Sharing-Betrieb?Bei einem Einprozessorsystem läuft zu jedem Zeitpunkt genau ein Prozess. Warum laufen die Prozesse trotzdem scheinbar parallel?Was ist ein Scheduler?WelchederfolgendenScheduling-StrategienhabendasStarvation-Problemundwarum:First-Come, First-Served.Shortest Job First.Round Robin.Priorität-Strategie: Jedem Prozess wird eine Priorität zugewiesen. Der Prozess mit der höchsten Priorität bekommt als Nächster den Prozessor. 
2; mod_assign; 128; 46; 0; Einsendeaufgabe 1.7 Gegeben seien die Prozesse  P1, P2, P3, P4  und  P5. Sie treffen in dieser Reihenfolge zur selben Zeit im System ein. Die Prozesse haben folgende Bearbeitungszeiten (in Zeiteinheiten) und Prioritäten:ProzessP1P2P3P4P5Bearbeitungszeit     101312Priorität31342Geben Sie für die folgenden Scheduling Verfahren jeweils die Ausführungsreihenfolge der Prozesse und die Gesamtwartezeit (vgl. Übungsaufgabe 1.10 im Kurstext) an. Vernachlässigen Sie dabei den Overhead des Prozesswechsels.a)   First-Come, First-Served.b)   Shortest Job First (Bei der gleichen Bearbeitungszeit gilt First-Come, First-Served).c)   Round Robin (Dicke einer Zeitscheibe = eine Zeiteinheit).d)   Prioritätsscheduling (je kleiner die Zahl desto höher die Priorität. Bei der gleichen Priorität gilt First-Come,           First-Served). 
2; mod_assign; 129; 47; 0; Einsendeaufgabe 1.8 Wir betrachten ein virtuelles Gerät für eine Festplatte, siehe Abbildung 1.6 im Kurstext. In welcher der Schichten des virtuellen Geräts wird jeder der folgenden Punkte bearbeitet?Vorbereitung einer Ein-/Ausgabeoperation und Aktivierung des Geräts für den Datentransport.Optimierung der Armbewegungen bei vorliegenden Aufträgen.Berechnung der Spur und des Sektors beim Lesen von der Festplatte.Bewegung des Lesekopfs bei Anlegen einer Spannung an den Steuermotor.Durchführung der Fehlererkennung, nachdem Daten von der Festplatte gelesen wurden.Auslösen einer Unterbrechung.Reaktion auf Unterbrechungen. 
2; mod_assign; 156; 48; 0; Einsendeaufgabe 2.1 Ein Paging-System hat einen maximalen logischen Speicher der Größe 16 MByte. Die Seitengröße beträgt 1024 Byte und der maximale physische Speicher ist 2 MByte groß. Ein Abschnitt der Seitentabelle eines aktiven Prozesses hat die folgenden Einträge: Seitennummer als Index     Seitenrahmennummer 0 4 1 8 2 16 3 17 4 9 Wie viele Bit hat eine logische Adresse? Wie viele Einträge hat eine Seitentabelle maximal?Wie viele Bit hat ein Eintrag der Seitentabelle?Welche physische Adresse hat die logische Adresse 1524?Welche logische Adresse wird auf die physische Adresse 10020 abgebildet? 
2; mod_assign; 157; 49; 0; Einsendeaufgabe 2.2 Die Schwimmstaffel-Übung mit drei nummerierten Schwimmern findet in einer Schwimmbahn statt. Bei der Übung müssen die folgenden Regeln eingehalten werden:Der erste Schwimmer startet.Der zweite Schwimmer darf erst starten, wenn der erste fertig ist.  Der dritte Schwimmer darf erst starten, wenn der zweite fertig ist.  Der erste Schwimmer darf wieder starten, wenn der dritte fertig ist.Realisieren Sie die folgenden Prozesse mit Semaphoren.Erster Schwimmer:  Zweiter SchwimmerDritter Schwimmer 
2; mod_assign; 158; 50; 0; Einsendeaufgabe 2.3 Synchronisieren Sie die folgenden Prozesse A und B mit Semaphoren.Prozess AProzess B1. Anweisung a1 1. Anweisung b1 2. Anweisung a2 2. Anweisung b2 Bei der Ausführung der Programme müssen folgende Regeln eingehalten werden: Die Anweisung  a1  muss vor der Anweisung  b2  und die Anweisung  b1  muss vor der Anweisung  a2 ausgeführt werden. Die Ausführungsreihenfolge von  a1  und  b1  kann beliebig sein, ebenso die von  a2  und  b2.Deklarieren und initialisieren Sie also passende Semaphore und fügen Sie die notwendigen up- und  down-Anweisungen in den obigen Programmcode ein. 
2; mod_assign; 159; 51; 0; Einsendeaufgabe 2.4 Ein Prozess hat zwei Threads,  Erzeuger-Thread  und  Verbraucher-Thread. Die gemeinsam benutzten Variablen sind folgende: var buffer : array[1..10] of data sem : semaphor (* Semaphor zur Synchronisation des Zugriffs auf buffer *) sem : = 0 (* Initialisierung *)Der  Erzeuger-Thread  läuft im wesentlichen so ab: for i from 1 to 10 do schreibe Daten in buffer[i] up(sem)   end (* for *)Und der  Verbraucher-Thread  macht folgendes: for j from 1 to 10 do down(sem) bearbeite den Inhalt von buffer[j] end (* for *) Angenommen, der  Erzeuger-Thread  blockiert jedes Mal für eine Zeit  te, wenn er die Daten in den  buffer  schreibt. Der  Verbraucher-Thread  braucht  tv  Zeit, um den Inhalt eines Elements von  buffer  zu bearbeiten. Während der Bearbeitung des Pufferinhalts blockiert der  Verbraucher-Thread  nicht. Wir nehmen noch an, dass man die Zeit für die Ausführung des übrigen Programmcodes ignorieren kann. Wie lange dauert es mindestens, bis das ganze Programm ausgeführt ist, wenn die Threads alsBenutzer-Threads  Kernel-Threadsrealisiert werden? 
2; mod_assign; 160; 52; 0; Einsendeaufgabe 2.5 Man kann beobachten, dass durch Entfernen von Dateien Lücken zwischen den verbliebenen Dateien auf einer Platte entstehen. Eine Möglichkeit zum Aufräumen ist die Kompaktifizierung durch Zusammenschieben der vorhandenen Dateien. Nehmen wir an, dass jede Datei zusammenhängend gespeichert ist. Das Kopieren einer Datei benötigt zuerst Lesezugriffe und dann Schreibzugriffe, die die gleiche Zeit wie Lesen kosten sollen. Angenommen, dass die Suchzeit für eine Datei insgesamt 9 ms (1 s = 103  ms) beträgt, die Übertragungsrate 8 MByte/s beträgt und die durchschnittliche Dateigröße 8 KByte ist. Wie lange würde es dauern,eine Datei in den Speicher einzulesen und sie dann an einer anderen Position wieder auf die Platte zu schreiben?die Hälfte einer 32-GB-Festplatte aufzuräumen? 
2; mod_assign; 161; 53; 0; Einsendeaufgabe 2.6 In einem Verzeichnis liefert  ls -ls  folgendes:4 -rw-r--r-- 1 bean user 1209 Aug 11 12:25 Makefile2 -rwxr--r-- 1 bean user   929 Jul 25 17:05 geheimprog2 -rw-rw-rw- 1 bean user   945 Mai 24 13:29 spielprog8 -rw-rw-r-- 2 bean user 3176 Mai 24 13:29 spielprog.java Wie müssen die Rechte gesetzt sein, damit jeder Benutzer auf  spielprog  lesend und ausführend zugreifen darf, aber nur  bean  selbst lesen, schreiben und ausführen darf? Wie lautet der Befehl, um diese Rechte zu setzen?Was besagt die 2 in der 3. Spalte von  spielprog.java  und was passiert, wenn diese Datei mit  rm  gelöscht wird?bean  möchte verhindern, dass ein anderer Benutzer das Programm  geheimprog  ausführen darf. Ist dies mit den aktuellen Rechten gewährleistet? Begründen Sie Ihre Antwort! 
2; mod_assign; 162; 54; 0; Einsendeaufgabe 2.7 Bei den FAT16- bzw. FAT32-Dateisystemen werden Blockadresslängen von 16 bzw. 32 Bit verwendet.Wie viele Blöcke kann eine Partition bei FAT16 bzw. FAT32 haben und wie groß kann die FAT werden?Auf einer Festplatte gibt es eine kleine Partition der Größe 64 GByte mit einer Blockgröße von 4 KByte.          (a)   Wie lang (in Bit) ist eine Blockadresse mindestens?          (b)   Wie viel Platz belegt die File Allocation Table (FAT) für ein Dateisystem auf der Partition mindestens, wenn diese                         Adresslänge verwendet wird?          (c)   Jetzt soll die Partition mit einem Dateisystem mit inodes verwaltet werden, wobei wieder dieselbe minimale                             Adresslänge benutzt wird. Wird die dreifach-indirekte Adresse im inode für besonders große Dateien nun wirklich                  benötigt?          (d) Wie viele Indexblöcke werden bei Verwendung von inodes für die Speicherung einer 5 509 KByte großen Datei  benötigt? 
2; mod_assign; 163; 55; 0; Einsendeaufgabe 2.8 Ein bestimmter Diskettentyp (ED-Diskette) hat 2 Seiten mit je 80 Spuren mit je 18 Sektoren bei einer Sektorgröße (Blockgröße) von 1 kByte.Über wieviel Speicherplatz verfügt eine solche Diskette? Wieviel Bit benötigt man für eine Blockadresse?Die Diskette wird formatiert, d.h. ein Dateisystem mit inodes wird angelegt. Dabei wird ein Bootblock und ein Superblock erzeugt, der die grundlegenden Informationen zum Dateisystem selbst enthält, außerdem werden 184 inodes mit je 128 Byte fest reserviert. Wie groß darf eine Datei maximal sein, die auf diesem Dateisystem abgespeichert werden kann? 
2; mod_assign; 187; 56; 0; Einsendeaufgabe 3.1 Betrachten wir zwei Hosts  A  und  B, die über eine einzige Verbindungsleitung mit einer Übertragungsrate  R  (in bps) verbunden sind. Die beiden Hosts sind  d  Meter voneinander entfernt die Ausbreitungsgeschwindigkeit auf der Leitung beträgt  v  (in m/s). Host  A  sendet zum Zeitpunkt  t  = 0 ein Paket mit einer Größe von  L  Bit an Host  B. Ignorieren Sie die Verarbeitungs- und Warteschlangenverzögerungen.Geben Sie Formeln für die  Ausbreitungsverzögerung  tprop  und die  Übertragungsverzögerung  ttrans  in Abhängigkeit von den angegebenen Parametern an.Was ist hier dann die  Ende-zu-Ende-Verzögerung  des Pakets?Wo befindet sich das letzte Bit des Pakets zum Zeitpunkt  t  =  ttrans?Wo befindet sich das erste Bit des Pakets zum Zeitpunkt  t  =  ttrans?Angenommen,  v  = 2,8  ·  108  m/s,  L  = 100 und  R  = 28000 bps. Wie groß muss die Entfernung  d  sein, so dass  tprop  =  ttrans  gilt? 
2; mod_assign; 188; 57; 0; Einsendeaufgabe 3.2 In einem paketvermittelnden Netz sollen  x  Bit Nutzdaten über einen Pfad mit  k  Switches und  k+1 Verbindungsleitungen als Paketfolge übertragen werden. Jedes Paket enthält  p  Bit Daten und  h  Header-Bits, und es soll  x  ≫  p+h  gelten (≫  bedeutet  ”wesentlich größer“, d. h. die Nutzdaten sind wesentlich mehr als eine Paketgröße und müssen aufgeteilt werden). Die Übertragungsrate der Leitung liegt bei  b  bps, weitere Verzögerungen können vernachlässigt werden. Durch welchen Wert von  p  wird die Verzögerung minimiert?Hinweis:  Betrachten Sie die Verzögerung als Funktion von  p. Minima berechnet man über Nullstellen der ersten Ableitung. 
2; mod_assign; 189; 58; 0; Einsendeaufgabe 3.3 Eine Beispiel-Web-Seite besteht aus einer HTML-Basisdatei mit Verweisen auf zwei Bilder. Die Basisdatei und ein Bild liegen auf demselben Server, das zweite Bild liegt auf einem anderen Server. Ein Benutzer fordert die Web-Seite an.Wieviele  Anfragenachrichten  muss der Client senden und wieviele  Antwortnachrichten bekommt er?Wieviele  TCP-Verbindungen  müssen erzeugt werden bei Verwendung von  (a) HTTP 1.0(b) HTTP 1.1? 
2; mod_assign; 190; 59; 0; Einsendeaufgabe 3.4 Laut DNS-Spezifikation kann die  DNS-Anwendung über UDP oder TCP laufen, aber in der Praxis läuft DNS fast immer über UDP. Was, vermuten Sie, ist der Hauptgrund?Eine zuverlässige Datenübertragung wie TCP verlangt, dass die Daten vom Sender korrekt beim Empfänger ankommen müssen. Welche Mechanismen werden im Kurstext dafür genannt?Was könnte eine Anwendung tun, damit ein  zuverlässiger Datentransfer über UDP auf der  Anwendungsschicht  (also ohne TCP) realisiert wird? 
2; mod_assign; 191; 60; 0; Einsendeaufgabe 3.5 Zwei im Internet kommunizierende Prozesse auf unterschiedlichen Hosts werden durch ihre Portnummern identifiziert. Warum können die Prozesse sich nicht gegenseitig durch eine vom Betriebssystem zugewiesene Prozess-ID (PID) identifizieren? 
2; mod_assign; 192; 61; 0; Einsendeaufgabe 3.6 Bei der Programmierung von Internet-Anwendungen wird die Socket-Technik verwendet, siehe Abschnitt 3.3.3 im Kurstext. Ein Socket dient als Schnittstelle zwischen der Anwendungs- und Transportschicht.Betrachten wir das ConversionClient-Programm im Abschnitt 3.3.6. Der Client will eine Verbindung zum Server-Prozess mit der Portnummer 6789 auf dem Rechner mit der IP-Adresse des Servers  servername  mittels  clientSocket  aufbauen.Ist die Verwendung der Portnummer und der IP-Adresse hier im Programm problematisch, wenn Sie an das Prinzip des Schichtenmodells denken?Betrachten wir das ConversionServer-Programm. Der Server verwendet zwei Sockets, nämlich  welcomeSocket  und  connectionSocket. Zum Verbindungsaufbau wird der welcomeSocket  und zur Übertragung von Nachrichten der  connectionSocket  verwendet.Wie viele Sockets benötigt der Server, wenn er  n  Verbindungen gleichzeitig unterstützt und jede Verbindung einen eigenen Socket verwendet? 
2; mod_assign; 193; 62; 0; Einsendeaufgabe 3.7 Berechnen Sie die Prüfsumme der folgenden sechs 8-Bit-Wörter:11000111,  00111011,  10010101,  00111111,  11101111,  11000100 
2; mod_assign; 194; 63; 0; Einsendeaufgabe 3.8 Drei Hosts  A,  B  und  C  befinden sich in einem  Broadcast-Netz, d.h. alle Hosts sind durch einen einzigen Übertragungskanal verbunden. Wenn ein Host im Broadcast-Netz eine Nachricht versendet, empfangen sie alle anderen Hosts. Beim Empfang einer Nachricht prüft ein Host, ob sie für ihn bestimmt ist. Falls ja, verarbeitet sie der Host, sonst ignoriert er sie einfach. Die Pakete können unabhängig von einander verloren gehen oder beschädigt werden.Host  A  soll nun eine Nachricht (aus vielen Paketen) per Broadcast an  B  und  C  senden, also nicht an jeden Empfänger einzeln. Entwerfen Sie ein  Stop-and-Wait-Protokoll  für die zuverlässige Übertragung von Nachrichten von  A  an  B  und  C, wobei jedes Paket mit einer Sequenznummer versehen wird und  A  erst das nächste Paket an beide senden kann, wenn er weiß, dass beide Empfänger das letzte Paket korrekt empfangen haben. Das Stop-and-Wait-Protokoll soll mit  Zustandsautomaten  für die Sender- und Empfängerseite beschrieben werden. 
2; mod_assign; 217; 64; 0; Einsendeaufgabe 4.1 Betrachten wir eine TCP-Verbindung mit der Übertragungsrate 1 Gbit/s. Um den Überlauf des Empfangspuffers zu vermeiden, teilt der Empfänger  B  dem Sender  A mit, wie groß sein  Empfangsfenster  ist, siehe auch die Struktur eines Segments im Abschnitt 3.4.5.2. Wir nehmen an, dass die  Roundtrip-Zeit  (Round-Trip Time, RTT) zwischen  A  und  B  insgesamt 100 ms beträgt, wobei dies die Zeit ist, die benötigt wird, um eine Nachricht von  A  zu  B  bzw. von  B  zu  A  und die Bestätigung vom Empfänger zurück zum Sender zu übermitteln. Wie groß sollte das Empfangsfenster mindestens sein, damit ein Überlauf vermieden wird?Sequenznummern müssen in einer TCP-Verbindung dann wiederholt verwendet werden, wenn ein Überlauf eintritt, d.h. die größtmögliche Nummer wird überschritten, so dass man wieder bei 0 anfängt.Wie lange dauert es in einem Netzwerk mit der Übertragungsrate 100 Mbit/s, bis die Sequenznummer überläuft? 
2; mod_assign; 218; 65; 0; Einsendeaufgabe 4.2 Host  A  und  B  kommunizieren über eine TCP-Verbindung. Host  B  hat von  A  die ersten 126 Byte erhalten und bestätigt. Nun sendet  A  zwei weitere Segmente an  B, zuerst  s  mit 80 Byte und dann  t  mit 40 Byte. Wir nehmen an, dass die initiale Sequenznummer 0 ist.Welche Sequenznummern haben die Segmente  s  und  t?Falls Segment  s  vor  t  beim Empfänger ankommt, was ist dann die Bestätigungsnummer im Bestätigungssegment für das Segment  s?Falls Segment  t  vor  s  beim Empfänger ankommt, was ist nun die Bestätigungsnummer im Bestätigungssegment für das Segment  t? 
2; mod_assign; 219; 66; 0; Einsendeaufgabe 4.3 Sie verwalten IP-Nummern und vergeben IPv4-Adressbereiche, natürlich möglichst ohne Verschwendung von ungenutzten Bereichen. Ihre Adressen sollen bei 186.16.0.0 beginnen.Acht Organisationen,  A,  B,  C,  D,  E,  F,  G  und  H, fordern 2000, 8000, 8000, 2000, 2000, 2000, 4000 und 2000 Adressen in dieser zeitlichen Reihenfolge an, jede Anforderung muss sofort und ohne Rücksicht auf die späteren Anforderungen erfüllt werden.Jede Organisation soll ein  einfaches IP-Netzwerk  mit einer  Netzwerkmaske  bekommen. Geben Sie für jede Anforderung die erste und die letzte zugewiesene IP-Adresse sowie die  Netzwerkadresse  und  -maske  in der Notation  a.b.c.d/x  an. 
2; mod_assign; 220; 67; 0; Einsendeaufgabe 4.4 Betrachten Sie das Netzwerk in der nachstehenden Abbildung und verwenden Sie den  Algorithmus von Dijkstra, um den kürzesten Pfad von  F  zu allen anderen Knoten zu berechnen.Stellen Sie dazu die einzelnen Schritte in einer Tabelle dar und zeichnen Sie das Ergebnis als Baum in die Abbildung ein. 
2; mod_assign; 221; 68; 0; Einsendeaufgabe 4.5 Betrachten Sie das Netzwerk in der nachfolgenden Abbildung.Erstellen Sie Distanztabellen für alle Knoten gemäß des dezentralen  Distanzvektor-Algorithmus.Angenommen, dass die Verbindung zwischen  T2  und  T3  unterbrochen ist. Wie oft müssen die Routing-Tabellen von  T1  und  T5  bis zur Konvergenz insgesamt aktualisiert werden?Angenommen, dass  T1  , . . . ,  T5  Gateway-Router von autonomen Systemen sind und das Protokoll BGP4 für Inter-AS-Routing gebraucht wird. Kann das count-to-infinity- Problem mit BGP4 entstehen, wenn die Verbindung zwischen  T2  und  T3  unterbrochen ist? 
2; mod_assign; 222; 69; 0; Einsendeaufgabe 4.6 1. Ein Router hat in seiner  Routing-Tabelle  die folgenden CIDR-Einträge stehen:Ziel-Netzwerk   Interface135.46.56.0/22       0135.46.60.0/22       1192.53.40.0/23       2  Standard                 3Was macht der Router, wenn Pakete mit den folgenden Adressen ankommen? (mit Begründung)a. 135.46.63.10  b. 135.46.57.14  c. 135.46.52.2  d. 192.53.40.7  e. 192.53.56.72. Ein Router soll die neuen IP-Adressen 57.6.96.0/21, 57.6.104.0/21, 57.6.112.0/21 und 57.6.120.0/21 in seine Routing-Tabelle übernehmen. Pakete an alle diese Adressen sollen an dieselbe Router-Schnittstelle weitergegeben werden. Kann man die neuen Adressen in eine einzige Adresse in der Notation  a.b.c.d/x  zusammenfassen? 
2; mod_assign; 232; 70; 0; Praktische Aufgabe: Unix/Linux Installation Zusätzlich zu den Einsendeaufgaben stellen wir Ihnen noch praktische Aufgaben, die Sie anregen sollen, wirklich mit einem Betriebssystem zu arbeiten. Die praktischen Aufgaben sind freiwillig und in erster Linie für UNIX-Anfänger. Wenn Sie schon UNIX-Erfahrungen haben,dann können Sie sich gerne an den Übungen beteiligen, indem Sie im unten genannten Diskussionsforum auf die Fragen der Unerfahrenen antworten.Installieren Sie UNIX auf Ihrem Rechner bzw. besorgen Sie sich einen Rechner mit UNIX.Finden Sie heraus, wie Sie ein Terminal-Fenster öffnen, üben Sie ein wenig den Umgang mit dem Terminal und benutzen Sie die Kommandos  ls,  ls -l,  cd,  pwd,  mkdir,  man, date,  who,  echo.Wie bekommt man UNIX auf einen Rechner? Wenn Ihr Rechner unter Linux, MacOSX, Solaris, IRIX, HP/UX, AIX oder einem ähnlichen System läuft, dann haben Sie bereits alles, was Sie brauchen. Ansonsten empfehlen wir, Linux auf einem PC zu verwenden. Eine Linux-Distribution, die wir hier empfohlen möchten, ist Ubuntu es gibt aber noch eine ganze Reihe weiterer Distribution, die auch kostenlos zur Verfügung stehen.Um Ubuntu zu benutzen, benötigt man zunächst ein Start-Volume, z. B. eine CD/DVD oder einen USB-Stick. Eine solches Medium kann man gelegentlich als Beilage in Fachzeitschriften oder Büchern finden, man kann sich auch eine Image-Datei unterwww.ubuntu.comaus dem Netz holen und als CD/DVD selbst brennen oder das Image auf einem USB-Stick speichern.Eine Variante für leistungsschwächere Rechner, ideal für den ausgemusterten, noch funktionsfähigen PC ist Xubuntu, siehewww.xubuntu.orgUbuntu und Xubuntu können zum Ausprobieren als Live-System benutzt werden, d.h. das gebrannte oder gespeicherte Volume ist bootfähig und das Betriebssystem lässt sich ohne Installation direkt davon starten. Auf die Dauer bequemer und schneller ist aber die Installation auf der Festplatte. Entweder man installiert es als einziges Betriebssystem auf einem Rechner oder als zweites Betriebssystem neben MS-Windows. Bei der zweiten Möglichkeit verwendet man eine freie Partition der Festplatte oder eine zweite Festplatte, beim Systemstart kann man dann immer wählen, welches System gestartet wird. Vorsicht beim Partitionieren der Festplatte, es können alle Daten verloren gehen!Eine weitere Möglichkeit zur Installation ist die Benutzung einer virtuellen Maschine: Auf dem ursprünglichen Betriebssystem (Gastgebersystem), das z.B. MS-Windows sein kann, wird ein Programm installiert, das einen oder auch mehrere weitere Rechner (Gastsysteme) emuliert alles, was auf diesen virtuellen Maschinen passiert, kann der Benutzer in einem eigenen Fenster ablaufen lassen oder der virtuellen Maschine auch den gesamten Bildschirm zur Verfügung stellen. Ausreichend Hauptspeicher und Festplattenplatz sollte vorhanden sein, als Gastbetriebssystem kommt hier vor allem das weniger anspruchsvolle Xubuntu in Frage.Eine kostenlose virtuelle Maschine ist VirtualBox, die man sich vonwww.virtualbox.orgherunterladen kann. Für die Installation von z. B. Xubuntu kann man sich sogar das Brennen einer Start-CD sparen man legt einfach eine neue Maschine an und bindet zur Installation das Xbuntu-Image als CD-Laufwerk ein. Jetzt können Sie in dem Fenster der virtuellen Box Ihren neuen Linux-Rechner benutzen: starten, herunterfahren, Software installieren, das System aktualisieren, Webbrowser benutzen, die Shell im Terminal ausprobieren usw.Für Fragen, Kommentare, Diskussionen usw. benutzen Sie bitte das News- und Diskussionsforumfeu.informatik.kurs.1801auf dem Newsserver feunews.fernuni-hagen.de der FernUniversität. 
2; mod_assign; 233; 71; 0; Praktische Aufgabe: Benutzung der Shell Mit welchem Werkzeug können Sie die Partitionierung Ihrer Platte anschauen bzw. ändern? Welche Partitionen gibt es auf Ihrer Festplatte? Welche Dateisysteme werden verwendet?Hinweis: Sie sollten möglichst nichts an den Partitionen und Dateisystemen ändern, jedenfalls so lange, bis Sie ganz sicher sind, was Sie tun, sonst können Sie sehr leicht das gesamte System unbrauchbar machen.Finden Sie heraus, z.B. mit Hilfe des  man-Kommandos und durch praktisches Ausprobieren, was man mit den Befehlen  ps,  pstree,  top,  kill,  mount,  umount,  newfs, file,  ls -i,  ls -n,  echo  machen kann. Welche Prozesse dürfen Sie  kill en?Navigieren Sie mit  pwd,  cd,  ls  im Dateisystem. Mit  ls -l  können Sie die Zugriffsrechte, den Besitzer, die Größe, das letzten Änderungsdatum von Dateien anzeigen lassen.Was passiert im Terminal, wenn Sie ein Programm wie  nedit  oder  xedit  starten, das ein eigenes Fenster öffnet und dort Benutzereingaben erwartet? Versuchen Sie im Terminal jetzt  CTRL-Z, nimmt die Shell dann wieder Befehle an? Was ist jetzt mit dem Editorfenster? Geben Sie nun das Kommando  bg  im Terminal ein, jetzt geht plötzlich sowohl die Shell als auch der Editor. Versuchen Sie, einiges über Vordergrund- bzw. Hintergrund-Prozesse im Terminal herauszufinden, wie startet man Hintergrundprozesse direkt? Tipp: das  & -Zeichen verwenden. Kann man das Hintergrundkommando auch wieder in den Vordergrund holen? Tipp: das Gegenteil von  bg  ist  fg.Erzeugen Sie eine ausführbare neue Datei  hallo, indem Sie die folgenden Kommandos ausführen.$  echo echo Hallo > hallo  # Testprogramm  hallo  erzeugen$  chmod +x hallo  # das Programm  hallo  wird ausführbar$  cat > > hallo  Stellen Sie mit  ls  fest, dass eine neue Datei  hallo  erzeugt wurde. Führen Sie das Kommando  hallo  aus.Probieren Sie das  cat-Kommando aus und führen Sie die folgenden Kommandos aus:$ cat > > hallo   # neue Zeilen werden an das Ende der Datei hallo gehängt. echo Guten Tag!  echo Guten Abend! ^D # Ende der Datei mit  Control-DFühren Sie das Kommando  hallo  aus. Was sehen Sie? 
2; mod_assign; 234; 72; 0; Praktische Aufgabe: Traceroute und ping Probieren diese Aufgabe bitte unbedingt aus, sie ist sehr wichtig für die Prüfung!Verfolgen Sie den Weg von Ihrem Computer zu folgenden Servern der Universitäten in der Welt.www.berkeley.edu  (Kalifornien),www.mit.edu  (Massachusetts),www.vu.nl  (Amsterdam),www.ox.ac.uk  (Oxford),www.uab.es  (Barcelona),www.tau.ac.il  (Tel Aviv),www.usyd.edu.au  (Sydney),www.aut.ac.nz  (Auckland),www.u-tokyo.ac.jp  (Tokyo) undwww.uct.ac.za  (Cape Town).1. Verwenden Sie das  traceroute- (UNIX) oder  tracert- (Windows) Kommando, um die IP-Adressen der Server zu finden. Geben Sie auch jeweils den letzten namentlich genannten Router an.2. Finden Sie mit  ping-Kommando heraus,•  wie viele ICMP-Pakete gesendet wurden,•  wie groß ein ICMP-Paket ist,•  wie viele Hops die Pakete durchgelaufen sind,•  wie viele Zeit eine Paket hin und zurück benötigt hat. 
2; mod_safran; 165; 2; 0; Timerzugriff privilegiert Muss der Zugriff auf das Register des Zeitgebers privilegiert sein? Tipp: Was könnte ein Benutzerprozess machen, wenn er den Zugriff auf die Hardware Timer hätte. Der Zugriff auf das Register, das die Anzahl der verbleibenden Zeiteinheiten der aktuellen Zeitscheibe enthält, muss privilegiert sein! Denn sonst könnte ein Prozess sich selbst zusätzliche Rechenzeit verschaffen. ❌ Diese Antwort ist leider nicht richtig. Die Konsequenzen der Möglichkeit zur Veränderung der Zeitscheibenlänge durch den Prozess selbst wird in Ihrer Lösung nicht deutlich: wenn ein Prozess im Benutzermodus selbst seine Zeitscheibenlänge verändern könnte, dann könnte ein Prozess sich selbst zusätzliche Rechenzeit verschaffen, und damit die Scheduling-Strategie aushebeln. In ihrer Lösung sagen Sie, dass der Zugriff auf das Register, das die Anzahl der verbleibenden Zeiteinheiten der aktuellen Zeitscheibe enthält, privilegiert sein muss. 
2; mod_safran; 196; 3; 0; Wartezeit SJF Gegeben sei eine feste Menge von endlich vielen Prozessen mit bekannten Rechenzeiten. Beweisen Sie, dass SJF die Gesamtwartezeit minimiert, also die Summe aller Wartezeiten der einzelnen Prozesse. Dabei ist die Wartezeit eines Prozesses diejenige Zeit, in der sich der Prozess bis zur Beendigung im Zustand „bereit“ befindet. Tipps: 1). Wir setzen voraus, dass es mindestens zwei Prozesse gibt, die unterschiedliche Bearbeitungszeiten haben. 2). Betrachten Sie eine Ausführungsreihenfolge der Prozesse, die nicht nach SJF sortiert sind. Vertauschen Sie zwei Prozesse in der Reihenfolge $L$ und $K$, wobei $L$ direkt vor $K$ steht und die Bearbeitungszeit von $L$ größer als $K$ hat. So ein Paar von Prozessen muss es in der Ausführungsreihenfolge geben. Bestimmen Sie die Differenz der Gesamtwartezeiten der beiden Ausführungsreihenfolgen. Betrachten wir eine Ausführungsreihenfolge, bei der ein langer Prozess L vor einem kurzen Prozess K an die Reihe kommt. Wenn wir die beiden miteinander vertauschen, brauchen alle dazwischen liegenden Prozesse nicht mehr so lange zu warten. Für L verlängert sich bei diesem Tausch zwar die Wartezeit, aber die Verkürzung der Wartezeit von K ist größer! Alle übrigen Prozesse sind nicht betroffen. Also verkürzt solch ein Tausch die Gesamtwartezeit. Jede von SJF verschiedene Ausführungsreihenfolge lässt sich also noch verbessern folglich ist SJF optimal. ❌ Diese Antwort ist leider nicht richtig. Für den Beweis fehlt der Abschluss. In ihrer Lösung argumentieren Sie, dass die Vertauschung von L und K dazu führt, dass alle dazwischenliegenden Prozesse nicht mehr so lange zu warten brauchen. 
2; mod_safran; 224; 4; 0; Wartezeit SJF Gegeben sei eine feste Menge von endlich vielen Prozessen mit bekannten Rechenzeiten. Beweisen Sie, dass SJF die Gesamtwartezeit minimiert, also die Summe aller Wartezeiten der einzelnen Prozesse. Dabei ist die Wartezeit eines Prozesses diejenige Zeit, in der sich der Prozess bis zur Beendigung im Zustand „bereit“ befindet. Tipps: 1). Wir setzen voraus, dass es mindestens zwei Prozesse gibt, die unterschiedliche Bearbeitungszeiten haben. 2). Betrachten Sie eine Ausführungsreihenfolge der Prozesse, die nicht nach SJF sortiert sind. Vertauschen Sie zwei Prozesse in der Reihenfolge $L$ und $K$, wobei $L$ direkt vor $K$ steht und die Bearbeitungszeit von $L$ größer als $K$ hat. So ein Paar von Prozessen muss es in der Ausführungsreihenfolge geben. Bestimmen Sie die Differenz der Gesamtwartezeiten der beiden Ausführungsreihenfolgen. Betrachten wir eine Ausführungsreihenfolge, bei der ein langer Prozess L vor einem kurzen Prozess K an die Reihe kommt. Wenn wir die beiden miteinander vertauschen, brauchen alle dazwischen liegenden Prozesse nicht mehr so lange zu warten. Für L verlängert sich bei diesem Tausch zwar die Wartezeit, aber die Verkürzung der Wartezeit von K ist größer! Alle übrigen Prozesse sind nicht betroffen. Also verkürzt solch ein Tausch die Gesamtwartezeit. Jede von SJF verschiedene Ausführungsreihenfolge lässt sich also noch verbessern folglich ist SJF optimal. ❌ Diese Antwort ist leider nicht richtig. Für den Beweis fehlt der Abschluss. In ihrer Lösung argumentieren Sie, dass die Vertauschung von L und K dazu führt, dass alle dazwischenliegenden Prozesse nicht mehr so lange zu warten brauchen. 
2; mod_safran; 412; 5; 0; Adressbusbreite Beim Prozessor 68000 von Motorola beträgt die Wortlänge 2 Byte. Wie breit muss der Adressbus sein – das heißt: aus wie vielen Bits bestehen die Adressen – bei einer Hauptspeicherkapazität von 16 MByte? Tipp: Berechnen Sie zuerst die Anzahl der Worte im Hauptspeicher und beantworten Sie dann, wie viele Bit Sie benötigen, um diese Zahl binär darzustellen. Um die Anzahl der Worte der Länge von 2 Byte, die in den gegebenen Hauptspeicher passen, zu ermitteln, muss die gegebene Hauptspeichergröße durch die Wortlänge aufgeteilt werden. Teilt man 16 MByte in Worte der Länge 2 Byte auf, so entstehen $(16 • 2^{20}) / 2 = 8 • 2^{20} = 2^{23}$ Worte. 23 Bit reichen aus, um die $2^{23}$ Worte zu adressieren. Die Adressbusbreite muss so groß sein, dass man die Bitfolge jeder Adresse, die zum Zugriff auf ein Wort im Speicher nötig ist, auf die Busleitungen abbilden kann. Bei 23 Bit langen Adressen sind deshalb 23 Leitungen nötig. ❌ Diese Antwort ist leider nicht richtig. Die Adressbusbreite beträgt 23. Teilt man 16 MByte in Worte der Länge 2 Byte auf, so entstehen $(16 • 2^{20}) / 2 = 8 • 2^{20} = 2^3• 2^{20} = 2^{23}$ Worte. 
